<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Tutorial • flaiR</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/Source_Sans_Pro-0.4.9/font.css" rel="stylesheet">
<link href="../deps/Source_Code_Pro-0.4.9/font.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Tutorial">
<!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=G-3ZG40PPG98"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-3ZG40PPG98');
</script><script defer data-domain="{YOUR DOMAIN},all.tidyverse.org" src="https://plausible.io/js/plausible.js"></script>
</head>
<body>
    <a href="#container" class="visually-hidden-focusable">Skip to content</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-none" data-bs-theme="inverse" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">flaiR</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Released version">0.1.7</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../index.html"><span class="fa fa-home"></span></a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-quick-start" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true"><span class="fa fa-rocket"></span> Quick Start</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-quick-start">
<li><a class="dropdown-item" href="../articles/quickstart.html#flair-installation">flaiR Installation</a></li>
    <li><a class="dropdown-item" href="../articles/quickstart.html#nlp-tasks">NLP Tasks</a></li>
    <li><a class="dropdown-item" href="../articles/quickstart.html#training-and-fine-tuning">Training and Finetuning</a></li>
  </ul>
</li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-tutorial" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true"><span class="fa fa-project-diagram"></span> Tutorial</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-tutorial">
<li><a class="dropdown-item" href="../articles/tutorial.html#introduction">Introduction</a></li>
    <li><a class="dropdown-item" href="../articles/tutorial.html#sentence-and-token">Sentence and Token</a></li>
    <li><a class="dropdown-item" href="../articles/tutorial.html#sequence-taggings">Sequence Taggings</a></li>
    <li><a class="dropdown-item" href="../articles/tutorial.html#performing-ner-tasks">Performing NER Tasks</a></li>
    <li><a class="dropdown-item" href="../articles/tutorial.html#the-overview-of-embeddings">The Overview of Embedding</a></li>
    <li><a class="dropdown-item" href="../articles/tutorial.html#embedding">Embeddings Example</a></li>
    <li><a class="dropdown-item" href="../articles/tutorial.html#training-a-binary-classifier">Training a Binary Classifier</a></li>
    <li><a class="dropdown-item" href="../articles/tutorial.html#training-rnns">Training RNNs</a></li>
    <li><a class="dropdown-item" href="../articles/tutorial.html#finetune-transformers">Finetune Transformers </a></li>
    <li><a class="dropdown-item" href="../articles/tutorial.html#extending-contexts-embedding-regression">Extending conText</a></li>
  </ul>
</li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-reference" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true"><span class="fa fa-file-code-o"></span> Reference</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-reference">
<li><a class="dropdown-item" href="../reference/index.html">All Function Reference</a></li>
  </ul>
</li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-news" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true"><span class="fa fa-newspaper-o"></span> News</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-news">
<li><a class="dropdown-item" href="../news/index.html#flair-007-2025-01-06">0.1.7</a></li>
    <li><a class="dropdown-item" href="../news/index.html#flair-007-2024-12-26">0.0.7</a></li>
    <li><a class="dropdown-item" href="../news/index.html#flair-006-2023-10-29">0.0.6</a></li>
    <li><a class="dropdown-item" href="../news/index.html#flair-005-2023-10-01">0.0.5</a></li>
    <li><a class="dropdown-item" href="../news/index.html#flair-003-2023-09-10">0.0.3</a></li>
    <li><a class="dropdown-item" href="../news/index.html#flair-001-development-version">0.0.1</a></li>
  </ul>
</li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/davidycliao/flaiR"><span class="fa fa-github fa-lg"></span> GitHub</a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article" id="container">



<link href="tutorial_files/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet">
<script src="tutorial_files/htmlwidgets-1.6.4/htmlwidgets.js"></script><script src="tutorial_files/plotly-binding-4.10.4/plotly.js"></script><script src="tutorial_files/typedarray-0.1/typedarray.min.js"></script><link href="tutorial_files/crosstalk-1.2.1/css/crosstalk.min.css" rel="stylesheet">
<script src="tutorial_files/crosstalk-1.2.1/js/crosstalk.min.js"></script><link href="tutorial_files/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet">
<script src="tutorial_files/plotly-main-2.11.1/plotly-latest.min.js"></script><div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>Tutorial</h1>
                        <h4 data-toc-skip class="author">Yen-Chieh Liao
| Sohini Timbadia | Stefan Müller</h4>
            <address class="author_afil">
      University of Birmingham &amp; University College
Dublin<br><small class="dont-index">Source: <a href="https://github.com/davidycliao/flaiR/blob/HEAD/vignettes/tutorial.Rmd" class="external-link"><code>vignettes/tutorial.Rmd</code></a></small>
      <div class="d-none name"><code>tutorial.Rmd</code></div>
    </address>
</div>

    
    
<div class="section level2">
<h2 id="flair-nlp-and-flair-for-social-science">Flair NLP and flaiR for Social Science<a class="anchor" aria-label="anchor" href="#flair-nlp-and-flair-for-social-science"></a>
</h2>
<div style="text-align: justify;">
<p>Flair NLP is an open-source Natural Language Processing (NLP) library
developed by <a href="https://github.com/zalandoresearch/" class="external-link">Zalando
Research</a>. Known for its state-of-the-art solutions, it excels in
contextual string embeddings, Named Entity Recognition (NER), and
Part-of-Speech tagging (POS). Flair offers robust text analysis tools
through multiple embedding approaches, including Flair contextual string
embeddings, transformer-based embeddings from Hugging Face, and
traditional models like GloVe and fasttext. Additionally, it provides
pre-trained models for various languages and seamless integration with
fine-tuned transformers hosted on Hugging Face.</p>
<p>flaiR bridges these powerful NLP features from Python to the R
environment, making advanced text analysis accessible for social science
researcher by combining Flair’s ease of use with R’s familiar interface
for integration with popular R packages such as <a href="https://quanteda.io" class="external-link">quanteda</a> and more.</p>
</div>
</div>
<div class="section level2">
<h2 id="the-overview">The Overview<a class="anchor" aria-label="anchor" href="#the-overview"></a>
</h2>
<ul>
<li><p><a href="#tutorial.html#sentence-and-token"><strong>Sentence and
Token Object in FlaiR</strong></a></p></li>
<li><p><a href="#tutorial.html#sequence-taggings"><strong>Sequence
Taggings</strong></a></p></li>
<li><p><a href="#tutorial.html#the-overview-of-embedding"><strong>The
Overview of Embedding</strong></a></p></li>
<li><p><a href="#tutorial.html#embedding-examples"><strong>Embedding
Examples</strong></a></p></li>
<li><p><a href="#tutorial.html#performing-ner-tasks"><strong>Performing
NER Tasks</strong></a></p></li>
<li><p><a href="#tutorial.html#training-a-binary-classifier-in-flair"><strong>Training
a Binary Classifier in flaiR</strong></a></p></li>
<li><p><a href="#tutorial.html#training-rnns"><strong>Training
RNNa</strong></a></p></li>
<li><p><a href="#tutorial.html#finetune-transformers"><strong>Finetune
BERT</strong></a></p></li>
<li><p><a href="#tutorial.html#extending-contexts-embedding-regression"><strong>Extending
conText’s Embedding Regression</strong></a></p></li>
</ul>
<p> </p>
<hr>
</div>
<div class="section level2">
<h2 id="sentence-and-token">Sentence and Token<a class="anchor" aria-label="anchor" href="#sentence-and-token"></a>
</h2>
<p>Sentence and Token are fundamental classes.</p>
<div class="section level3">
<h3 id="sentence">Sentence<a class="anchor" aria-label="anchor" href="#sentence"></a>
</h3>
<div style="text-align: justify;">
<p>A Sentence in Flair is an object that contains a sequence of Token
objects, and it can be annotated with labels, such as named entities,
part-of-speech tags, and more. It also can store embeddings for the
sentence as a whole and different kinds of linguistic annotations.</p>
<p>Here’s a simple example of how you create a Sentence:</p>
</div>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Creating a Sentence object</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://davidycliao.github.io/flaiR">flaiR</a></span><span class="op">)</span></span>
<span><span class="va">string</span> <span class="op">&lt;-</span> <span class="st">"What I see in UCD today, what I have seen of UCD in its impact on my own life and the life of Ireland."</span></span>
<span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span>
<span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="va">string</span><span class="op">)</span></span></code></pre></div>
<p><code>Sentence[26]</code> means that there are a total of 26 tokens
in the sentence.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="co">#&gt; Sentence[26]: "What I see in UCD today, what I have seen of UCD in its impact on my own life and the life of Ireland."</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="token">Token<a class="anchor" aria-label="anchor" href="#token"></a>
</h3>
<div style="text-align: justify;">
<p>When you use Flair to handle text data,<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content="&lt;p&gt;Flair is built on PyTorch, which is a library in
Python.&lt;/p&gt;"><sup>1</sup></a> <code>Sentence</code>
and <code>Token</code> objects often play central roles in many use
cases. When you create a Sentence object, it automatically tokenizes the
text, removing the need to create the Token object manually.</p>
<p>Unlike R, which indexes from 1, Python indexes from 0. Therefore,
when using a for loop, I use <code>seq_along(sentence) - 1</code>. The
output should be something like:</p>
</div>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># The Sentence object has automatically created and contains multiple Token objects</span></span>
<span><span class="co"># We can iterate through the Sentence object to view each Token</span></span>
<span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq_along</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="co">#&gt; Token[0]: "What"</span></span>
<span><span class="co">#&gt; Token[1]: "I"</span></span>
<span><span class="co">#&gt; Token[2]: "see"</span></span>
<span><span class="co">#&gt; Token[3]: "in"</span></span>
<span><span class="co">#&gt; Token[4]: "UCD"</span></span>
<span><span class="co">#&gt; Token[5]: "today"</span></span>
<span><span class="co">#&gt; Token[6]: ","</span></span>
<span><span class="co">#&gt; Token[7]: "what"</span></span>
<span><span class="co">#&gt; Token[8]: "I"</span></span>
<span><span class="co">#&gt; Token[9]: "have"</span></span>
<span><span class="co">#&gt; Token[10]: "seen"</span></span>
<span><span class="co">#&gt; Token[11]: "of"</span></span>
<span><span class="co">#&gt; Token[12]: "UCD"</span></span>
<span><span class="co">#&gt; Token[13]: "in"</span></span>
<span><span class="co">#&gt; Token[14]: "its"</span></span>
<span><span class="co">#&gt; Token[15]: "impact"</span></span>
<span><span class="co">#&gt; Token[16]: "on"</span></span>
<span><span class="co">#&gt; Token[17]: "my"</span></span>
<span><span class="co">#&gt; Token[18]: "own"</span></span>
<span><span class="co">#&gt; Token[19]: "life"</span></span>
<span><span class="co">#&gt; Token[20]: "and"</span></span>
<span><span class="co">#&gt; Token[21]: "the"</span></span>
<span><span class="co">#&gt; Token[22]: "life"</span></span>
<span><span class="co">#&gt; Token[23]: "of"</span></span>
<span><span class="co">#&gt; Token[24]: "Ireland"</span></span>
<span><span class="co">#&gt; Token[25]: "."</span></span></code></pre></div>
<p>Or you can directly use <code>$tokens</code> method to print all
tokens.</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="va">tokens</span><span class="op">)</span></span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Token[0]: "What"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[2]]</span></span>
<span><span class="co">#&gt; Token[1]: "I"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[3]]</span></span>
<span><span class="co">#&gt; Token[2]: "see"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[4]]</span></span>
<span><span class="co">#&gt; Token[3]: "in"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[5]]</span></span>
<span><span class="co">#&gt; Token[4]: "UCD"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[6]]</span></span>
<span><span class="co">#&gt; Token[5]: "today"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[7]]</span></span>
<span><span class="co">#&gt; Token[6]: ","</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[8]]</span></span>
<span><span class="co">#&gt; Token[7]: "what"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[9]]</span></span>
<span><span class="co">#&gt; Token[8]: "I"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[10]]</span></span>
<span><span class="co">#&gt; Token[9]: "have"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[11]]</span></span>
<span><span class="co">#&gt; Token[10]: "seen"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[12]]</span></span>
<span><span class="co">#&gt; Token[11]: "of"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[13]]</span></span>
<span><span class="co">#&gt; Token[12]: "UCD"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[14]]</span></span>
<span><span class="co">#&gt; Token[13]: "in"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[15]]</span></span>
<span><span class="co">#&gt; Token[14]: "its"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[16]]</span></span>
<span><span class="co">#&gt; Token[15]: "impact"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[17]]</span></span>
<span><span class="co">#&gt; Token[16]: "on"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[18]]</span></span>
<span><span class="co">#&gt; Token[17]: "my"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[19]]</span></span>
<span><span class="co">#&gt; Token[18]: "own"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[20]]</span></span>
<span><span class="co">#&gt; Token[19]: "life"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[21]]</span></span>
<span><span class="co">#&gt; Token[20]: "and"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[22]]</span></span>
<span><span class="co">#&gt; Token[21]: "the"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[23]]</span></span>
<span><span class="co">#&gt; Token[22]: "life"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[24]]</span></span>
<span><span class="co">#&gt; Token[23]: "of"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[25]]</span></span>
<span><span class="co">#&gt; Token[24]: "Ireland"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[26]]</span></span>
<span><span class="co">#&gt; Token[25]: "."</span></span></code></pre></div>
<p><strong>Retrieve the Token</strong></p>
<div style="text-align: justify;">
<p>To comprehend the string representation format of the Sentence
object, tagging at least one token is adequate. Python’s
<code>get_token(n)</code> method allows us to retrieve the Token object
for a particular token. Additionally, we can use
<strong><code>[]</code></strong> to index a specific token.</p>
</div>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># method in Python</span></span>
<span><span class="va">sentence</span><span class="op">$</span><span class="fu">get_token</span><span class="op">(</span><span class="fl">5</span><span class="op">)</span></span>
<span><span class="co">#&gt; Token[4]: "UCD"</span></span></code></pre></div>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># indexing in R </span></span>
<span><span class="va">sentence</span><span class="op">[</span><span class="fl">6</span><span class="op">]</span></span>
<span><span class="co">#&gt; Token[6]: ","</span></span></code></pre></div>
<div style="text-align: justify;">
<p>Each word (and punctuation) in the text is treated as an individual
Token object. These Token objects store text information and other
possible linguistic information (such as part-of-speech tags or named
entity tags) and embedding (if you used a model to generate them).</p>
<p>While you do not need to create Token objects manually, understanding
how to manage them is useful in situations where you might want to
fine-tune the tokenization process. For example, you can control the
exactness of tokenization by manually creating Token objects from a
Sentence object.</p>
<p>This makes Flair very flexible when handling text data since the
automatic tokenization feature can be used for rapid development, while
also allowing users to fine-tune their tokenization.</p>
</div>
<p><strong>Annotate POS tag and NER tag</strong></p>
<div style="text-align: justify;">
<p>The <code>add_label(label_type, value)</code> method can be employed
to assign a label to the token. In Universal POS tags, if
<code>sentence[10]</code> is ‘see’, ‘seen’ might be tagged as
<code>VERB</code>, indicating it is a past participle form of a
verb.</p>
</div>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">sentence</span><span class="op">[</span><span class="fl">10</span><span class="op">]</span><span class="op">$</span><span class="fu">add_label</span><span class="op">(</span><span class="st">'manual-pos'</span>, <span class="st">'VERB'</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">[</span><span class="fl">10</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="co">#&gt; Token[10]: "seen" → VERB (1.0000)</span></span></code></pre></div>
<div style="text-align: justify;">
<p>We can also add a NER (Named Entity Recognition) tag to
<code>sentence[4]</code>, “UCD”, identifying it as a university in
Dublin.</p>
</div>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">sentence</span><span class="op">[</span><span class="fl">4</span><span class="op">]</span><span class="op">$</span><span class="fu">add_label</span><span class="op">(</span><span class="st">'ner'</span>, <span class="st">'ORG'</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">[</span><span class="fl">4</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="co">#&gt; Token[4]: "UCD" → ORG (1.0000)</span></span></code></pre></div>
<div style="text-align: justify;">
<p>If we print the sentence object, <code>Sentence[50]</code> provides
information for 50 tokens → [‘in’/ORG, ‘seen’/VERB], thus displaying two
tagging pieces of information.</p>
</div>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="co">#&gt; Sentence[26]: "What I see in UCD today, what I have seen of UCD in its impact on my own life and the life of Ireland." → ["UCD"/ORG, "seen"/VERB]</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="corpus">Corpus<a class="anchor" aria-label="anchor" href="#corpus"></a>
</h3>
<p>The Corpus object in Flair is a fundamental data structure that
represents a dataset containing text samples, usually comprising of a
training set, a development set (or validation set), and a test set.
It’s designed to work smoothly with Flair’s models for tasks like named
entity recognition, text classification, and more.</p>
<p><strong>Attributes:</strong></p>
<ul>
<li>
<code>train</code>: A list of sentences (List<a href="#sentence">Sentence</a>) that form the training dataset.</li>
<li>
<code>dev</code> (or development): A list of sentences (List<a href="#sentence">Sentence</a>) that form the development (or validation)
dataset.</li>
<li>
<code>test</code>: A list of sentences (List<a href="#sentence">Sentence</a>) that form the test dataset.</li>
</ul>
<p><strong>Important Methods:</strong></p>
<ul>
<li>
<code>downsample</code>: This method allows you to downsample
(reduce) the number of sentences in the train, dev, and test
splits.</li>
<li>
<code>obtain_statistics</code>: This method gives a quick overview
of the statistics of the corpus, including the number of sentences and
the distribution of labels.</li>
<li>
<code>make_vocab_dictionary</code>: Used to create a vocabulary
dictionary from the corpus.</li>
</ul>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://davidycliao.github.io/flaiR">flaiR</a></span><span class="op">)</span></span>
<span><span class="va">Corpus</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Corpus</span></span>
<span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span></code></pre></div>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Create some example sentences</span></span>
<span><span class="va">train</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="fu">Sentence</span><span class="op">(</span><span class="st">'This is a training example.'</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">dev</span> <span class="op">&lt;-</span>  <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="fu">Sentence</span><span class="op">(</span><span class="st">'This is a validation example.'</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">test</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="fu">Sentence</span><span class="op">(</span><span class="st">'This is a test example.'</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Create a corpus using the custom data splits</span></span>
<span><span class="va">corp</span> <span class="op">&lt;-</span>  <span class="fu">Corpus</span><span class="op">(</span>train <span class="op">=</span> <span class="va">train</span>, dev <span class="op">=</span> <span class="va">dev</span>, test <span class="op">=</span> <span class="va">test</span><span class="op">)</span></span></code></pre></div>
<div style="text-align: justify;">
<p><code>$obtain_statistics()</code> method of the Corpus object in the
Flair library provides an overview of the dataset statistics. The method
returns a <a href="https://www.w3schools.com/python/python_dictionaries.asp" class="external-link">Python
dictionary</a> with details about the training, validation
(development), and test datasets that make up the corpus. In R, you can
use the jsonlite package to format JSON.</p>
</div>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://jeroen.r-universe.dev/jsonlite" class="external-link">jsonlite</a></span><span class="op">)</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://jeroen.r-universe.dev/jsonlite/reference/fromJSON.html" class="external-link">fromJSON</a></span><span class="op">(</span><span class="va">corp</span><span class="op">$</span><span class="fu">obtain_statistics</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">formatted_str</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://jeroen.r-universe.dev/jsonlite/reference/fromJSON.html" class="external-link">toJSON</a></span><span class="op">(</span><span class="va">data</span>, pretty<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">formatted_str</span><span class="op">)</span></span>
<span><span class="co">#&gt; {</span></span>
<span><span class="co">#&gt;   "TRAIN": {</span></span>
<span><span class="co">#&gt;     "dataset": ["TRAIN"],</span></span>
<span><span class="co">#&gt;     "total_number_of_documents": [1],</span></span>
<span><span class="co">#&gt;     "number_of_documents_per_class": {},</span></span>
<span><span class="co">#&gt;     "number_of_tokens_per_tag": {},</span></span>
<span><span class="co">#&gt;     "number_of_tokens": {</span></span>
<span><span class="co">#&gt;       "total": [6],</span></span>
<span><span class="co">#&gt;       "min": [6],</span></span>
<span><span class="co">#&gt;       "max": [6],</span></span>
<span><span class="co">#&gt;       "avg": [6]</span></span>
<span><span class="co">#&gt;     }</span></span>
<span><span class="co">#&gt;   },</span></span>
<span><span class="co">#&gt;   "TEST": {</span></span>
<span><span class="co">#&gt;     "dataset": ["TEST"],</span></span>
<span><span class="co">#&gt;     "total_number_of_documents": [1],</span></span>
<span><span class="co">#&gt;     "number_of_documents_per_class": {},</span></span>
<span><span class="co">#&gt;     "number_of_tokens_per_tag": {},</span></span>
<span><span class="co">#&gt;     "number_of_tokens": {</span></span>
<span><span class="co">#&gt;       "total": [6],</span></span>
<span><span class="co">#&gt;       "min": [6],</span></span>
<span><span class="co">#&gt;       "max": [6],</span></span>
<span><span class="co">#&gt;       "avg": [6]</span></span>
<span><span class="co">#&gt;     }</span></span>
<span><span class="co">#&gt;   },</span></span>
<span><span class="co">#&gt;   "DEV": {</span></span>
<span><span class="co">#&gt;     "dataset": ["DEV"],</span></span>
<span><span class="co">#&gt;     "total_number_of_documents": [1],</span></span>
<span><span class="co">#&gt;     "number_of_documents_per_class": {},</span></span>
<span><span class="co">#&gt;     "number_of_tokens_per_tag": {},</span></span>
<span><span class="co">#&gt;     "number_of_tokens": {</span></span>
<span><span class="co">#&gt;       "total": [6],</span></span>
<span><span class="co">#&gt;       "min": [6],</span></span>
<span><span class="co">#&gt;       "max": [6],</span></span>
<span><span class="co">#&gt;       "avg": [6]</span></span>
<span><span class="co">#&gt;     }</span></span>
<span><span class="co">#&gt;   }</span></span>
<span><span class="co">#&gt; }</span></span></code></pre></div>
<p><strong>In R</strong></p>
<div style="text-align: justify;">
<p>Below, we use data from the article <a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;ved=2ahUKEwjz_bS3p5KCAxWEWEEAHcuVAi4QFnoECA8QAQ&amp;url=https%3A%2F%2Fwww.journals.uchicago.edu%2Fdoi%2Ffull%2F10.1086%2F715165&amp;usg=AOvVaw3f_J3sXTrym2ZR64pF3ZtN&amp;opi=89978449" class="external-link"><em>The
Temporal Focus of Campaign Communication</em></a> by <a href="https://muellerstefan.net" class="external-link">Stefan Muller</a>, published in the
<em>Journal of Politics</em> in 2020, as an example.</p>
<p>First, we vectorize the <code>cc_muller$text</code> using the
Sentence function to transform it into a list object. Then, we reformat
<code>cc_muller$class_pro_retro</code> as a factor. It’s essential to
note that R handles numerical values differently than Python. In R,
numerical values are represented with a floating point, so it’s
advisable to convert them into factors or strings. Lastly, we employ the
map function from the purrr package to assign labels to each sentence
corpus using the <code>$add_label</code> method.</p>
</div>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://purrr.tidyverse.org/" class="external-link">purrr</a></span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Attaching package: 'purrr'</span></span>
<span><span class="co">#&gt; The following object is masked from 'package:jsonlite':</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     flatten</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="va">cc_muller</span><span class="op">)</span></span>
<span><span class="co"># The `Sentence` object tokenizes text </span></span>
<span><span class="va">text</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html" class="external-link">lapply</a></span><span class="op">(</span> <span class="va">cc_muller</span><span class="op">$</span><span class="va">text</span>, <span class="va">Sentence</span><span class="op">)</span></span>
<span><span class="co"># split sentence object to train and test. </span></span>
<span><span class="va">labels</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html" class="external-link">as.factor</a></span><span class="op">(</span><span class="va">cc_muller</span><span class="op">$</span><span class="va">class_pro_retro</span><span class="op">)</span></span>
<span><span class="co"># `$add_label` method assigns the corresponding coded type to each Sentence corpus.</span></span>
<span><span class="va">text</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://purrr.tidyverse.org/reference/map2.html" class="external-link">map2</a></span><span class="op">(</span><span class="va">text</span>, <span class="va">labels</span>, <span class="op">~</span> <span class="va">.x</span><span class="op">$</span><span class="fu">add_label</span><span class="op">(</span><span class="st">"classification"</span>, <span class="va">.y</span><span class="op">)</span>, .progress <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<p>To perform a train-test split using base R, we can follow these
steps:</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">2046</span><span class="op">)</span></span>
<span><span class="va">sample</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html" class="external-link">sample</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="cn">TRUE</span>, <span class="cn">FALSE</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">text</span><span class="op">)</span>, replace<span class="op">=</span><span class="cn">TRUE</span>, prob<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0.8</span>, <span class="fl">0.2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">train</span>  <span class="op">&lt;-</span> <span class="va">text</span><span class="op">[</span><span class="va">sample</span><span class="op">]</span></span>
<span><span class="va">test</span>   <span class="op">&lt;-</span> <span class="va">text</span><span class="op">[</span><span class="op">!</span><span class="va">sample</span><span class="op">]</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html" class="external-link">sprintf</a></span><span class="op">(</span><span class="st">"Corpus object sizes - Train: %d |  Test: %d"</span>, <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">train</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">test</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "Corpus object sizes - Train: 4710 |  Test: 1148"</span></span></code></pre></div>
<div style="text-align: justify;">
<p>If you don’t provide a dev set, flaiR will not force you to carve out
a portion of your test set to serve as a dev set. However, in some cases
when only the train and test sets are provided without a dev set, flaiR
might automatically take a fraction of the train set (e.g., 10%) to use
as a dev set (<a href="https://github.com/flairNLP/flair/issues/2259#issuecomment-830040253" class="external-link">#2259</a>).
This is to offer a mechanism for model selection and to prevent the
model from overfitting on the train set.</p>
<p>In the “Corpus” function, there is a random selection of the
<code>"dev"</code> dataset. To ensure reproducibility, we need to set
the seed in the flaiR framework. We can accomplish this by calling the
top-level module “flair” from flaiR and using
<code>$set_seed(1964L)</code> to set the seed.</p>
</div>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">flair</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/import_flair.html">import_flair</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">flair</span><span class="op">$</span><span class="fu">set_seed</span><span class="op">(</span><span class="fl">1964L</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">corp</span> <span class="op">&lt;-</span> <span class="fu">Corpus</span><span class="op">(</span>train<span class="op">=</span><span class="va">train</span>, </span>
<span>                 <span class="co"># dev=test,</span></span>
<span>                 test<span class="op">=</span><span class="va">test</span><span class="op">)</span></span>
<span><span class="co">#&gt; 2025-01-19 02:01:07,747 No dev split found. Using 10% (i.e. 471 samples) of the train split as dev data</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html" class="external-link">sprintf</a></span><span class="op">(</span><span class="st">"Corpus object sizes - Train: %d | Test: %d | Dev: %d"</span>, </span>
<span>        <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">corp</span><span class="op">$</span><span class="va">train</span><span class="op">)</span>, </span>
<span>        <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">corp</span><span class="op">$</span><span class="va">test</span><span class="op">)</span>,</span>
<span>        <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">corp</span><span class="op">$</span><span class="va">dev</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "Corpus object sizes - Train: 4239 | Test: 1148 | Dev: 471"</span></span></code></pre></div>
<div style="text-align: justify;">
<p>In the later sections, there will be more similar processing using
the <code>Corpus</code>. Following that, we will focus on advanced NLP
applications.</p>
</div>
<p> </p>
<hr>
</div>
</div>
<div class="section level2">
<h2 id="sequence-taggings">Sequence Taggings<a class="anchor" aria-label="anchor" href="#sequence-taggings"></a>
</h2>
<div class="section level3">
<h3 id="tag-entities-in-text">Tag Entities in Text<a class="anchor" aria-label="anchor" href="#tag-entities-in-text"></a>
</h3>
<div style="text-align: justify;">
<p>Let’s run named entity recognition over the following example
sentence: “I love Berlin and New York”. To do this, all you need to do
is make a Sentence object for this text, load a pre-trained model and
use it to predict tags for the object.</p>
</div>
<p><strong>NER Models</strong></p>
<table class="table">
<colgroup>
<col width="16%">
<col width="15%">
<col width="14%">
<col width="23%">
<col width="11%">
<col width="19%">
</colgroup>
<thead><tr class="header">
<th>ID</th>
<th>Task</th>
<th>Language</th>
<th>Training Dataset</th>
<th>Accuracy</th>
<th>Contributor / Notes</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>‘ner’</td>
<td>NER (4-class)</td>
<td>English</td>
<td>Conll-03</td>
<td>93.03 (F1)</td>
<td></td>
</tr>
<tr class="even">
<td>‘ner-fast’</td>
<td>NER (4-class)</td>
<td>English</td>
<td>Conll-03</td>
<td>92.75 (F1)</td>
<td>(fast model)</td>
</tr>
<tr class="odd">
<td>‘ner-large’</td>
<td>NER (4-class)</td>
<td>English / Multilingual</td>
<td>Conll-03</td>
<td>94.09 (F1)</td>
<td>(large model)</td>
</tr>
<tr class="even">
<td>‘ner-pooled’</td>
<td>NER (4-class)</td>
<td>English</td>
<td>Conll-03</td>
<td>93.24 (F1)</td>
<td>(memory inefficient)</td>
</tr>
<tr class="odd">
<td>‘ner-ontonotes’</td>
<td>NER (18-class)</td>
<td>English</td>
<td>Ontonotes</td>
<td>89.06 (F1)</td>
<td></td>
</tr>
<tr class="even">
<td>‘ner-ontonotes-fast’</td>
<td>NER (18-class)</td>
<td>English</td>
<td>Ontonotes</td>
<td>89.27 (F1)</td>
<td>(fast model)</td>
</tr>
<tr class="odd">
<td>‘ner-ontonotes-large’</td>
<td>NER (18-class)</td>
<td>English / Multilingual</td>
<td>Ontonotes</td>
<td>90.93 (F1)</td>
<td>(large model)</td>
</tr>
<tr class="even">
<td>‘ar-ner’</td>
<td>NER (4-class)</td>
<td>Arabic</td>
<td>AQMAR &amp; ANERcorp (curated)</td>
<td>86.66 (F1)</td>
<td></td>
</tr>
<tr class="odd">
<td>‘da-ner’</td>
<td>NER (4-class)</td>
<td>Danish</td>
<td>Danish NER dataset</td>
<td></td>
<td>AmaliePauli</td>
</tr>
<tr class="even">
<td>‘de-ner’</td>
<td>NER (4-class)</td>
<td>German</td>
<td>Conll-03</td>
<td>87.94 (F1)</td>
<td></td>
</tr>
<tr class="odd">
<td>‘de-ner-large’</td>
<td>NER (4-class)</td>
<td>German / Multilingual</td>
<td>Conll-03</td>
<td>92.31 (F1)</td>
<td></td>
</tr>
<tr class="even">
<td>‘de-ner-germeval’</td>
<td>NER (4-class)</td>
<td>German</td>
<td>Germeval</td>
<td>84.90 (F1)</td>
<td></td>
</tr>
<tr class="odd">
<td>‘de-ner-legal’</td>
<td>NER (legal text)</td>
<td>German</td>
<td>LER dataset</td>
<td>96.35 (F1)</td>
<td></td>
</tr>
<tr class="even">
<td>‘fr-ner’</td>
<td>NER (4-class)</td>
<td>French</td>
<td>WikiNER (aij-wikiner-fr-wp3)</td>
<td>95.57 (F1)</td>
<td>mhham</td>
</tr>
<tr class="odd">
<td>‘es-ner-large’</td>
<td>NER (4-class)</td>
<td>Spanish</td>
<td>CoNLL-03</td>
<td>90.54 (F1)</td>
<td>mhham</td>
</tr>
<tr class="even">
<td>‘nl-ner’</td>
<td>NER (4-class)</td>
<td>Dutch</td>
<td>CoNLL 2002</td>
<td>92.58 (F1)</td>
<td></td>
</tr>
<tr class="odd">
<td>‘nl-ner-large’</td>
<td>NER (4-class)</td>
<td>Dutch</td>
<td>Conll-03</td>
<td>95.25 (F1)</td>
<td></td>
</tr>
<tr class="even">
<td>‘nl-ner-rnn’</td>
<td>NER (4-class)</td>
<td>Dutch</td>
<td>CoNLL 2002</td>
<td>90.79 (F1)</td>
<td></td>
</tr>
<tr class="odd">
<td>‘ner-ukrainian’</td>
<td>NER (4-class)</td>
<td>Ukrainian</td>
<td>NER-UK dataset</td>
<td>86.05 (F1)</td>
<td>dchaplinsky</td>
</tr>
</tbody>
</table>
<p>Source: <a href="https://flairnlp.github.io/docs/tutorial-basics/tagging-entities" class="external-link uri">https://flairnlp.github.io/docs/tutorial-basics/tagging-entities</a></p>
<p><strong>POS Models</strong></p>
<table class="table">
<colgroup>
<col width="10%">
<col width="16%">
<col width="13%">
<col width="22%">
<col width="13%">
<col width="23%">
</colgroup>
<thead><tr class="header">
<th>ID</th>
<th>Task</th>
<th>Language</th>
<th>Training Dataset</th>
<th>Accuracy</th>
<th>Contributor / Notes</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>‘pos’</td>
<td>POS-tagging</td>
<td>English</td>
<td>Ontonotes</td>
<td>98.19 (Accuracy)</td>
<td></td>
</tr>
<tr class="even">
<td>‘pos-fast’</td>
<td>POS-tagging</td>
<td>English</td>
<td>Ontonotes</td>
<td>98.1 (Accuracy)</td>
<td>(fast model)</td>
</tr>
<tr class="odd">
<td>‘upos’</td>
<td>POS-tagging (universal)</td>
<td>English</td>
<td>Ontonotes</td>
<td>98.6 (Accuracy)</td>
<td></td>
</tr>
<tr class="even">
<td>‘upos-fast’</td>
<td>POS-tagging (universal)</td>
<td>English</td>
<td>Ontonotes</td>
<td>98.47 (Accuracy)</td>
<td>(fast model)</td>
</tr>
<tr class="odd">
<td>‘pos-multi’</td>
<td>POS-tagging</td>
<td>Multilingual</td>
<td>UD Treebanks</td>
<td>96.41 (average acc.)</td>
<td>(12 languages)</td>
</tr>
<tr class="even">
<td>‘pos-multi-fast’</td>
<td>POS-tagging</td>
<td>Multilingual</td>
<td>UD Treebanks</td>
<td>92.88 (average acc.)</td>
<td>(12 languages)</td>
</tr>
<tr class="odd">
<td>‘ar-pos’</td>
<td>POS-tagging</td>
<td>Arabic (+dialects)</td>
<td>combination of corpora</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>‘de-pos’</td>
<td>POS-tagging</td>
<td>German</td>
<td>UD German - HDT</td>
<td>98.50 (Accuracy)</td>
<td></td>
</tr>
<tr class="odd">
<td>‘de-pos-tweets’</td>
<td>POS-tagging</td>
<td>German</td>
<td>German Tweets</td>
<td>93.06 (Accuracy)</td>
<td>stefan-it</td>
</tr>
<tr class="even">
<td>‘da-pos’</td>
<td>POS-tagging</td>
<td>Danish</td>
<td>Danish Dependency Treebank</td>
<td></td>
<td>AmaliePauli</td>
</tr>
<tr class="odd">
<td>‘ml-pos’</td>
<td>POS-tagging</td>
<td>Malayalam</td>
<td>30000 Malayalam sentences</td>
<td>83</td>
<td>sabiqueqb</td>
</tr>
<tr class="even">
<td>‘ml-upos’</td>
<td>POS-tagging</td>
<td>Malayalam</td>
<td>30000 Malayalam sentences</td>
<td>87</td>
<td>sabiqueqb</td>
</tr>
<tr class="odd">
<td>‘pt-pos-clinical’</td>
<td>POS-tagging</td>
<td>Portuguese</td>
<td>PUCPR</td>
<td>92.39</td>
<td>LucasFerroHAILab for clinical texts</td>
</tr>
<tr class="even">
<td>‘pos-ukrainian’</td>
<td>POS-tagging</td>
<td>Ukrainian</td>
<td>Ukrainian UD</td>
<td>97.93 (F1)</td>
<td>dchaplinsky</td>
</tr>
</tbody>
</table>
<p>Source: <a href="https://flairnlp.github.io/docs/tutorial-basics/part-of-speech-tagging" class="external-link uri">https://flairnlp.github.io/docs/tutorial-basics/part-of-speech-tagging</a></p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># attach flaiR in R</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://davidycliao.github.io/flaiR">flaiR</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># make a sentence</span></span>
<span></span>
<span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span>
<span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">'I love Berlin and New York.'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># load the NER tagger</span></span>
<span><span class="va">SequenceTagger</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_models.html">flair_models</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">SequenceTagger</span></span>
<span><span class="va">tagger</span> <span class="op">&lt;-</span> <span class="va">SequenceTagger</span><span class="op">$</span><span class="fu">load</span><span class="op">(</span><span class="st">'flair/ner-english'</span><span class="op">)</span>  </span>
<span><span class="co">#&gt; 2025-01-19 02:01:09,156 SequenceTagger predicts: Dictionary with 20 tags: &lt;unk&gt;, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, &lt;START&gt;, &lt;STOP&gt;</span></span>
<span></span>
<span><span class="co"># run NER over sentence</span></span>
<span><span class="va">tagger</span><span class="op">$</span><span class="fu">predict</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span></code></pre></div>
<p>To print all annotations:</p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># print the sentence with all annotations</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="co">#&gt; Sentence[7]: "I love Berlin and New York." → ["Berlin"/LOC, "New York"/LOC]</span></span></code></pre></div>
<div style="text-align: justify;">
<p>Use a for loop to print out each POS tag. It’s important to note that
Python is indexed from 0. Therefore, in an R environment, we must use
<code>seq_along(sentence$get_labels()) - 1</code>.</p>
</div>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq_along</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="fu">get_labels</span><span class="op">(</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="fu">get_labels</span><span class="op">(</span><span class="op">)</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span><span class="co">#&gt; 'Span[2:3]: "Berlin"'/'LOC' (0.9812)</span></span>
<span><span class="co">#&gt; 'Span[4:6]: "New York"'/'LOC' (0.9957)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="tag-part-of-speech">Tag Part-of-Speech<a class="anchor" aria-label="anchor" href="#tag-part-of-speech"></a>
</h3>
<div style="text-align: justify;">
<p>We use <code>flaiR/POS-english</code> for POS tagging in the standard
models on Hugging Face.</p>
</div>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># attach flaiR in R</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://davidycliao.github.io/flaiR">flaiR</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># make a sentence</span></span>
<span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span>
<span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">'I love Berlin and New York.'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># load the NER tagger</span></span>
<span><span class="va">Classifier</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_nn.html">flair_nn</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Classifier</span></span>
<span><span class="va">tagger</span> <span class="op">&lt;-</span> <span class="va">Classifier</span><span class="op">$</span><span class="fu">load</span><span class="op">(</span><span class="st">'pos'</span><span class="op">)</span></span>
<span><span class="co">#&gt; 2025-01-19 02:01:10,276 SequenceTagger predicts: Dictionary with 53 tags: &lt;unk&gt;, O, UH, ,, VBD, PRP, VB, PRP$, NN, RB, ., DT, JJ, VBP, VBG, IN, CD, NNS, NNP, WRB, VBZ, WDT, CC, TO, MD, VBN, WP, :, RP, EX, JJR, FW, XX, HYPH, POS, RBR, JJS, PDT, NNPS, RBS, AFX, WP$, -LRB-, -RRB-, ``, '', LS, $, SYM, ADD</span></span></code></pre></div>
<p><em>Penn Treebank POS Tags Reference</em></p>
<table class="table">
<thead><tr class="header">
<th>Tag</th>
<th>Description</th>
<th>Example</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>DT</td>
<td>Determiner</td>
<td>the, a, these</td>
</tr>
<tr class="even">
<td>NN</td>
<td>Noun, singular</td>
<td>cat, tree</td>
</tr>
<tr class="odd">
<td>NNS</td>
<td>Noun, plural</td>
<td>cats, trees</td>
</tr>
<tr class="even">
<td>NNP</td>
<td>Proper noun, singular</td>
<td>John, London</td>
</tr>
<tr class="odd">
<td>NNPS</td>
<td>Proper noun, plural</td>
<td>Americans</td>
</tr>
<tr class="even">
<td>VB</td>
<td>Verb, base form</td>
<td>take</td>
</tr>
<tr class="odd">
<td>VBD</td>
<td>Verb, past tense</td>
<td>took</td>
</tr>
<tr class="even">
<td>VBG</td>
<td>Verb, gerund/present participle</td>
<td>taking</td>
</tr>
<tr class="odd">
<td>VBN</td>
<td>Verb, past participle</td>
<td>taken</td>
</tr>
<tr class="even">
<td>VBP</td>
<td>Verb, non-3rd person singular present</td>
<td>take</td>
</tr>
<tr class="odd">
<td>VBZ</td>
<td>Verb, 3rd person singular present</td>
<td>takes</td>
</tr>
<tr class="even">
<td>JJ</td>
<td>Adjective</td>
<td>big</td>
</tr>
<tr class="odd">
<td>RB</td>
<td>Adverb</td>
<td>quickly</td>
</tr>
<tr class="even">
<td>O</td>
<td>Other</td>
<td>-</td>
</tr>
<tr class="odd">
<td>,</td>
<td>Comma</td>
<td>,</td>
</tr>
<tr class="even">
<td>.</td>
<td>Period</td>
<td>.</td>
</tr>
<tr class="odd">
<td>:</td>
<td>Colon</td>
<td>:</td>
</tr>
<tr class="even">
<td>-LRB-</td>
<td>Left bracket</td>
<td>(</td>
</tr>
<tr class="odd">
<td>-RRB-</td>
<td>Right bracket</td>
<td>)</td>
</tr>
<tr class="even">
<td>``</td>
<td>Opening quotation</td>
<td>”</td>
</tr>
<tr class="odd">
<td>’’</td>
<td>Closing quotation</td>
<td>”</td>
</tr>
<tr class="even">
<td>HYPH</td>
<td>Hyphen</td>
<td>-</td>
</tr>
<tr class="odd">
<td>CD</td>
<td>Cardinal number</td>
<td>1, 2, 3</td>
</tr>
<tr class="even">
<td>IN</td>
<td>Preposition</td>
<td>in, on, at</td>
</tr>
<tr class="odd">
<td>PRP</td>
<td>Personal pronoun</td>
<td>I, you, he</td>
</tr>
<tr class="even">
<td>PRP$</td>
<td>Possessive pronoun</td>
<td>my, your</td>
</tr>
<tr class="odd">
<td>UH</td>
<td>Interjection</td>
<td>oh, wow</td>
</tr>
<tr class="even">
<td>FW</td>
<td>Foreign word</td>
<td>café</td>
</tr>
<tr class="odd">
<td>SYM</td>
<td>Symbol</td>
<td>+, %</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># run NER over sentence</span></span>
<span><span class="va">tagger</span><span class="op">$</span><span class="fu">predict</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># print the sentence with all annotations</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="co">#&gt; Sentence[7]: "I love Berlin and New York." → ["I"/PRP, "love"/VBP, "Berlin"/NNP, "and"/CC, "New"/NNP, "York"/NNP, "."/.]</span></span></code></pre></div>
<p>Use a for loop to print out each POS tag.</p>
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq_along</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="fu">get_labels</span><span class="op">(</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="fu">get_labels</span><span class="op">(</span><span class="op">)</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span><span class="co">#&gt; 'Token[0]: "I"'/'PRP' (1.0)</span></span>
<span><span class="co">#&gt; 'Token[1]: "love"'/'VBP' (1.0)</span></span>
<span><span class="co">#&gt; 'Token[2]: "Berlin"'/'NNP' (0.9999)</span></span>
<span><span class="co">#&gt; 'Token[3]: "and"'/'CC' (1.0)</span></span>
<span><span class="co">#&gt; 'Token[4]: "New"'/'NNP' (1.0)</span></span>
<span><span class="co">#&gt; 'Token[5]: "York"'/'NNP' (1.0)</span></span>
<span><span class="co">#&gt; 'Token[6]: "."'/'.' (1.0)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="detect-sentiment">Detect Sentiment<a class="anchor" aria-label="anchor" href="#detect-sentiment"></a>
</h3>
<div style="text-align: justify;">
<p>Let’s run sentiment analysis over the same sentence to determine
whether it is POSITIVE or NEGATIVE.</p>
<p>You can do this with essentially the same code as above. Instead of
loading the ‘ner’ model, you now load the <code>'sentiment'</code>
model:</p>
</div>
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># attach flaiR in R</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://davidycliao.github.io/flaiR">flaiR</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># make a sentence</span></span>
<span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span>
<span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">'I love Berlin and New York.'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># load the Classifier tagger from flair.nn module</span></span>
<span><span class="va">Classifier</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_nn.html">flair_nn</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Classifier</span></span>
<span><span class="va">tagger</span> <span class="op">&lt;-</span> <span class="va">Classifier</span><span class="op">$</span><span class="fu">load</span><span class="op">(</span><span class="st">'sentiment'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># run sentiment analysis over sentence</span></span>
<span><span class="va">tagger</span><span class="op">$</span><span class="fu">predict</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># print the sentence with all annotations</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="co">#&gt; Sentence[7]: "I love Berlin and New York." → POSITIVE (0.9982)</span></span></code></pre></div>
<hr>
</div>
<div class="section level3">
<h3 id="dealing-with-dataframe">Dealing with Dataframe<a class="anchor" aria-label="anchor" href="#dealing-with-dataframe"></a>
</h3>
<div class="section level4">
<h4 id="parts-of-speech-tagging-across-full-dataframe">Parts-of-Speech Tagging Across Full DataFrame<a class="anchor" aria-label="anchor" href="#parts-of-speech-tagging-across-full-dataframe"></a>
</h4>
<div style="text-align: justify;">
<p>You can apply Part-of-Speech (POS) tagging across an entire DataFrame
using Flair’s pre-trained models. Let’s walk through an example using
the pos-fast model. You can apply Part-of-Speech (POS) tagging across an
entire DataFrame using Flair’s pre-trained models. Let’s walk through an
example using the pos-fast model. First, let’s load our required
packages and sample data:</p>
</div>
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://davidycliao.github.io/flaiR">flaiR</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="va">uk_immigration</span><span class="op">)</span></span>
<span><span class="va">uk_immigration</span> <span class="op">&lt;-</span> <span class="va">uk_immigration</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">2</span>,<span class="op">]</span></span></code></pre></div>
<div style="text-align: justify;">
<p>For POS tagging, we’ll use Flair’s pre-trained model. The pos-fast
model offers a good balance between speed and accuracy. For more
pre-trained models, check out Flair’s documentation at Flair POS Tagging
Documentation. There are two ways to load the POS tagger:</p>
</div>
<ul>
<li>Load with tag dictionary display (default):</li>
</ul>
<div class="sourceCode" id="cb29"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">tagger_pos</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/load_tagger_pos.html">load_tagger_pos</a></span><span class="op">(</span><span class="st">"pos-fast"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Loading POS tagger model: pos-fast</span></span>
<span><span class="co">#&gt; 2025-01-19 02:01:13,909 SequenceTagger predicts: Dictionary with 53 tags: &lt;unk&gt;, O, UH, ,, VBD, PRP, VB, PRP$, NN, RB, ., DT, JJ, VBP, VBG, IN, CD, NNS, NNP, WRB, VBZ, WDT, CC, TO, MD, VBN, WP, :, RP, EX, JJR, FW, XX, HYPH, POS, RBR, JJS, PDT, NNPS, RBS, AFX, WP$, -LRB-, -RRB-, ``, '', LS, $, SYM, ADD</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; POS Tagger Dictionary:</span></span>
<span><span class="co">#&gt; ========================================</span></span>
<span><span class="co">#&gt; Total tags: 53</span></span>
<span><span class="co">#&gt; ----------------------------------------</span></span>
<span><span class="co">#&gt; Special:       &lt;unk&gt;, O, &lt;START&gt;, &lt;STOP&gt; </span></span>
<span><span class="co">#&gt; Nouns:         PRP, PRP$, NN, NNS, NNP, WP, EX, NNPS, WP$ </span></span>
<span><span class="co">#&gt; Verbs:         VBD, VB, VBP, VBG, VBZ, MD, VBN </span></span>
<span><span class="co">#&gt; Adjectives:    JJ, JJR, POS, JJS </span></span>
<span><span class="co">#&gt; Adverbs:       RB, WRB, RBR, RBS </span></span>
<span><span class="co">#&gt; Determiners:   DT, WDT, PDT </span></span>
<span><span class="co">#&gt; Prepositions:  IN, TO </span></span>
<span><span class="co">#&gt; Conjunctions:  CC </span></span>
<span><span class="co">#&gt; Numbers:       CD </span></span>
<span><span class="co">#&gt; Punctuation:   &lt;unk&gt;, ,, ., :, HYPH, -LRB-, -RRB-, ``, '', $, NFP, &lt;START&gt;, &lt;STOP&gt; </span></span>
<span><span class="co">#&gt; Others:        UH, FW, XX, LS, $, SYM, ADD </span></span>
<span><span class="co">#&gt; ----------------------------------------</span></span>
<span><span class="co">#&gt; Common POS Tag Meanings:</span></span>
<span><span class="co">#&gt; NN*: Nouns (NNP: Proper, NNS: Plural)</span></span>
<span><span class="co">#&gt; VB*: Verbs (VBD: Past, VBG: Gerund)</span></span>
<span><span class="co">#&gt; JJ*: Adjectives (JJR: Comparative)</span></span>
<span><span class="co">#&gt; RB*: Adverbs</span></span>
<span><span class="co">#&gt; PRP: Pronouns, DT: Determiners</span></span>
<span><span class="co">#&gt; IN: Prepositions, CC: Conjunctions</span></span>
<span><span class="co">#&gt; ========================================</span></span></code></pre></div>
<p>This will show you all available POS tags grouped by categories
(nouns, verbs, adjectives, etc.).</p>
<ul>
<li>Load without tag display for a cleaner output:</li>
</ul>
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">pos_tagger</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/load_tagger_pos.html">load_tagger_pos</a></span><span class="op">(</span><span class="st">"pos-fast"</span>, show_tags <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="co">#&gt; Loading POS tagger model: pos-fast</span></span>
<span><span class="co">#&gt; 2025-01-19 02:01:14,279 SequenceTagger predicts: Dictionary with 53 tags: &lt;unk&gt;, O, UH, ,, VBD, PRP, VB, PRP$, NN, RB, ., DT, JJ, VBP, VBG, IN, CD, NNS, NNP, WRB, VBZ, WDT, CC, TO, MD, VBN, WP, :, RP, EX, JJR, FW, XX, HYPH, POS, RBR, JJS, PDT, NNPS, RBS, AFX, WP$, -LRB-, -RRB-, ``, '', LS, $, SYM, ADD</span></span></code></pre></div>
<p>Now we can process our texts:</p>
<div class="sourceCode" id="cb31"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">results</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/get_pos.html">get_pos</a></span><span class="op">(</span>texts <span class="op">=</span> <span class="va">uk_immigration</span><span class="op">$</span><span class="va">text</span>,</span>
<span>                   doc_ids <span class="op">=</span> <span class="va">uk_immigration</span><span class="op">$</span><span class="va">speaker</span>,</span>
<span>                   show.text_id <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                   tagger <span class="op">=</span> <span class="va">pos_tagger</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">results</span>, n <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span>
<span><span class="co">#&gt;               doc_id token_id</span></span>
<span><span class="co">#&gt;               &lt;char&gt;    &lt;num&gt;</span></span>
<span><span class="co">#&gt;  1: Philip Hollobone        0</span></span>
<span><span class="co">#&gt;  2: Philip Hollobone        1</span></span>
<span><span class="co">#&gt;  3: Philip Hollobone        2</span></span>
<span><span class="co">#&gt;  4: Philip Hollobone        3</span></span>
<span><span class="co">#&gt;  5: Philip Hollobone        4</span></span>
<span><span class="co">#&gt;  6: Philip Hollobone        5</span></span>
<span><span class="co">#&gt;  7: Philip Hollobone        6</span></span>
<span><span class="co">#&gt;  8: Philip Hollobone        7</span></span>
<span><span class="co">#&gt;  9: Philip Hollobone        8</span></span>
<span><span class="co">#&gt; 10: Philip Hollobone        9</span></span>
<span><span class="co">#&gt;                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   text_id</span></span>
<span><span class="co">#&gt;                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    &lt;char&gt;</span></span>
<span><span class="co">#&gt;  1: I thank Mr. Speaker for giving me permission to hold this debate today. I welcome the Minister-I very much appreciate the contact from his office prior to today-and the Conservative and Liberal Democrat Front Benchers to the debate. I also welcome my hon. Friends on the Back Benches. Immigration is the most important issue for my constituents. I get more complaints, comments and suggestions about immigration than about anything else. In the Kettering constituency, the number of immigrants is actually very low. There is a well-settled Sikh community in the middle of Kettering town itself, which has been in Kettering for some 40 or 50 years and is very much part of the local community and of the fabric of local life. There are other very small migrant groups in my constituency, but it is predominantly made up of indigenous British people. However, there is huge concern among my constituents about the level of immigration into our country. I believe that I am right in saying that, in recent years, net immigration into the United Kingdom is the largest wave of immigration that our country has ever known and, proportionately, is probably the biggest wave of immigration since the Norman conquest. My contention is that our country simply cannot cope with immigration on that scale-to coin a phrase, we simply cannot go on like this. It is about time that mainstream politicians started airing the views of their constituents, because for too long people have muttered under their breath that they are concerned about immigration. They have been frightened to speak out about it because they are frightened of being accused of being racist. My contention is that immigration is not a racist issue; it is a question of numbers. I personally could not care tuppence about the ethnicity of the immigrants concerned, the colour of their skin or the language that they speak. What I am concerned about is the very large numbers of new arrivals to our country. My contention is that the United Kingdom simply cannot cope with them.</span></span>
<span><span class="co">#&gt;  2: I thank Mr. Speaker for giving me permission to hold this debate today. I welcome the Minister-I very much appreciate the contact from his office prior to today-and the Conservative and Liberal Democrat Front Benchers to the debate. I also welcome my hon. Friends on the Back Benches. Immigration is the most important issue for my constituents. I get more complaints, comments and suggestions about immigration than about anything else. In the Kettering constituency, the number of immigrants is actually very low. There is a well-settled Sikh community in the middle of Kettering town itself, which has been in Kettering for some 40 or 50 years and is very much part of the local community and of the fabric of local life. There are other very small migrant groups in my constituency, but it is predominantly made up of indigenous British people. However, there is huge concern among my constituents about the level of immigration into our country. I believe that I am right in saying that, in recent years, net immigration into the United Kingdom is the largest wave of immigration that our country has ever known and, proportionately, is probably the biggest wave of immigration since the Norman conquest. My contention is that our country simply cannot cope with immigration on that scale-to coin a phrase, we simply cannot go on like this. It is about time that mainstream politicians started airing the views of their constituents, because for too long people have muttered under their breath that they are concerned about immigration. They have been frightened to speak out about it because they are frightened of being accused of being racist. My contention is that immigration is not a racist issue; it is a question of numbers. I personally could not care tuppence about the ethnicity of the immigrants concerned, the colour of their skin or the language that they speak. What I am concerned about is the very large numbers of new arrivals to our country. My contention is that the United Kingdom simply cannot cope with them.</span></span>
<span><span class="co">#&gt;  3: I thank Mr. Speaker for giving me permission to hold this debate today. I welcome the Minister-I very much appreciate the contact from his office prior to today-and the Conservative and Liberal Democrat Front Benchers to the debate. I also welcome my hon. Friends on the Back Benches. Immigration is the most important issue for my constituents. I get more complaints, comments and suggestions about immigration than about anything else. In the Kettering constituency, the number of immigrants is actually very low. There is a well-settled Sikh community in the middle of Kettering town itself, which has been in Kettering for some 40 or 50 years and is very much part of the local community and of the fabric of local life. There are other very small migrant groups in my constituency, but it is predominantly made up of indigenous British people. However, there is huge concern among my constituents about the level of immigration into our country. I believe that I am right in saying that, in recent years, net immigration into the United Kingdom is the largest wave of immigration that our country has ever known and, proportionately, is probably the biggest wave of immigration since the Norman conquest. My contention is that our country simply cannot cope with immigration on that scale-to coin a phrase, we simply cannot go on like this. It is about time that mainstream politicians started airing the views of their constituents, because for too long people have muttered under their breath that they are concerned about immigration. They have been frightened to speak out about it because they are frightened of being accused of being racist. My contention is that immigration is not a racist issue; it is a question of numbers. I personally could not care tuppence about the ethnicity of the immigrants concerned, the colour of their skin or the language that they speak. What I am concerned about is the very large numbers of new arrivals to our country. My contention is that the United Kingdom simply cannot cope with them.</span></span>
<span><span class="co">#&gt;  4: I thank Mr. Speaker for giving me permission to hold this debate today. I welcome the Minister-I very much appreciate the contact from his office prior to today-and the Conservative and Liberal Democrat Front Benchers to the debate. I also welcome my hon. Friends on the Back Benches. Immigration is the most important issue for my constituents. I get more complaints, comments and suggestions about immigration than about anything else. In the Kettering constituency, the number of immigrants is actually very low. There is a well-settled Sikh community in the middle of Kettering town itself, which has been in Kettering for some 40 or 50 years and is very much part of the local community and of the fabric of local life. There are other very small migrant groups in my constituency, but it is predominantly made up of indigenous British people. However, there is huge concern among my constituents about the level of immigration into our country. I believe that I am right in saying that, in recent years, net immigration into the United Kingdom is the largest wave of immigration that our country has ever known and, proportionately, is probably the biggest wave of immigration since the Norman conquest. My contention is that our country simply cannot cope with immigration on that scale-to coin a phrase, we simply cannot go on like this. It is about time that mainstream politicians started airing the views of their constituents, because for too long people have muttered under their breath that they are concerned about immigration. They have been frightened to speak out about it because they are frightened of being accused of being racist. My contention is that immigration is not a racist issue; it is a question of numbers. I personally could not care tuppence about the ethnicity of the immigrants concerned, the colour of their skin or the language that they speak. What I am concerned about is the very large numbers of new arrivals to our country. My contention is that the United Kingdom simply cannot cope with them.</span></span>
<span><span class="co">#&gt;  5: I thank Mr. Speaker for giving me permission to hold this debate today. I welcome the Minister-I very much appreciate the contact from his office prior to today-and the Conservative and Liberal Democrat Front Benchers to the debate. I also welcome my hon. Friends on the Back Benches. Immigration is the most important issue for my constituents. I get more complaints, comments and suggestions about immigration than about anything else. In the Kettering constituency, the number of immigrants is actually very low. There is a well-settled Sikh community in the middle of Kettering town itself, which has been in Kettering for some 40 or 50 years and is very much part of the local community and of the fabric of local life. There are other very small migrant groups in my constituency, but it is predominantly made up of indigenous British people. However, there is huge concern among my constituents about the level of immigration into our country. I believe that I am right in saying that, in recent years, net immigration into the United Kingdom is the largest wave of immigration that our country has ever known and, proportionately, is probably the biggest wave of immigration since the Norman conquest. My contention is that our country simply cannot cope with immigration on that scale-to coin a phrase, we simply cannot go on like this. It is about time that mainstream politicians started airing the views of their constituents, because for too long people have muttered under their breath that they are concerned about immigration. They have been frightened to speak out about it because they are frightened of being accused of being racist. My contention is that immigration is not a racist issue; it is a question of numbers. I personally could not care tuppence about the ethnicity of the immigrants concerned, the colour of their skin or the language that they speak. What I am concerned about is the very large numbers of new arrivals to our country. My contention is that the United Kingdom simply cannot cope with them.</span></span>
<span><span class="co">#&gt;  6: I thank Mr. Speaker for giving me permission to hold this debate today. I welcome the Minister-I very much appreciate the contact from his office prior to today-and the Conservative and Liberal Democrat Front Benchers to the debate. I also welcome my hon. Friends on the Back Benches. Immigration is the most important issue for my constituents. I get more complaints, comments and suggestions about immigration than about anything else. In the Kettering constituency, the number of immigrants is actually very low. There is a well-settled Sikh community in the middle of Kettering town itself, which has been in Kettering for some 40 or 50 years and is very much part of the local community and of the fabric of local life. There are other very small migrant groups in my constituency, but it is predominantly made up of indigenous British people. However, there is huge concern among my constituents about the level of immigration into our country. I believe that I am right in saying that, in recent years, net immigration into the United Kingdom is the largest wave of immigration that our country has ever known and, proportionately, is probably the biggest wave of immigration since the Norman conquest. My contention is that our country simply cannot cope with immigration on that scale-to coin a phrase, we simply cannot go on like this. It is about time that mainstream politicians started airing the views of their constituents, because for too long people have muttered under their breath that they are concerned about immigration. They have been frightened to speak out about it because they are frightened of being accused of being racist. My contention is that immigration is not a racist issue; it is a question of numbers. I personally could not care tuppence about the ethnicity of the immigrants concerned, the colour of their skin or the language that they speak. What I am concerned about is the very large numbers of new arrivals to our country. My contention is that the United Kingdom simply cannot cope with them.</span></span>
<span><span class="co">#&gt;  7: I thank Mr. Speaker for giving me permission to hold this debate today. I welcome the Minister-I very much appreciate the contact from his office prior to today-and the Conservative and Liberal Democrat Front Benchers to the debate. I also welcome my hon. Friends on the Back Benches. Immigration is the most important issue for my constituents. I get more complaints, comments and suggestions about immigration than about anything else. In the Kettering constituency, the number of immigrants is actually very low. There is a well-settled Sikh community in the middle of Kettering town itself, which has been in Kettering for some 40 or 50 years and is very much part of the local community and of the fabric of local life. There are other very small migrant groups in my constituency, but it is predominantly made up of indigenous British people. However, there is huge concern among my constituents about the level of immigration into our country. I believe that I am right in saying that, in recent years, net immigration into the United Kingdom is the largest wave of immigration that our country has ever known and, proportionately, is probably the biggest wave of immigration since the Norman conquest. My contention is that our country simply cannot cope with immigration on that scale-to coin a phrase, we simply cannot go on like this. It is about time that mainstream politicians started airing the views of their constituents, because for too long people have muttered under their breath that they are concerned about immigration. They have been frightened to speak out about it because they are frightened of being accused of being racist. My contention is that immigration is not a racist issue; it is a question of numbers. I personally could not care tuppence about the ethnicity of the immigrants concerned, the colour of their skin or the language that they speak. What I am concerned about is the very large numbers of new arrivals to our country. My contention is that the United Kingdom simply cannot cope with them.</span></span>
<span><span class="co">#&gt;  8: I thank Mr. Speaker for giving me permission to hold this debate today. I welcome the Minister-I very much appreciate the contact from his office prior to today-and the Conservative and Liberal Democrat Front Benchers to the debate. I also welcome my hon. Friends on the Back Benches. Immigration is the most important issue for my constituents. I get more complaints, comments and suggestions about immigration than about anything else. In the Kettering constituency, the number of immigrants is actually very low. There is a well-settled Sikh community in the middle of Kettering town itself, which has been in Kettering for some 40 or 50 years and is very much part of the local community and of the fabric of local life. There are other very small migrant groups in my constituency, but it is predominantly made up of indigenous British people. However, there is huge concern among my constituents about the level of immigration into our country. I believe that I am right in saying that, in recent years, net immigration into the United Kingdom is the largest wave of immigration that our country has ever known and, proportionately, is probably the biggest wave of immigration since the Norman conquest. My contention is that our country simply cannot cope with immigration on that scale-to coin a phrase, we simply cannot go on like this. It is about time that mainstream politicians started airing the views of their constituents, because for too long people have muttered under their breath that they are concerned about immigration. They have been frightened to speak out about it because they are frightened of being accused of being racist. My contention is that immigration is not a racist issue; it is a question of numbers. I personally could not care tuppence about the ethnicity of the immigrants concerned, the colour of their skin or the language that they speak. What I am concerned about is the very large numbers of new arrivals to our country. My contention is that the United Kingdom simply cannot cope with them.</span></span>
<span><span class="co">#&gt;  9: I thank Mr. Speaker for giving me permission to hold this debate today. I welcome the Minister-I very much appreciate the contact from his office prior to today-and the Conservative and Liberal Democrat Front Benchers to the debate. I also welcome my hon. Friends on the Back Benches. Immigration is the most important issue for my constituents. I get more complaints, comments and suggestions about immigration than about anything else. In the Kettering constituency, the number of immigrants is actually very low. There is a well-settled Sikh community in the middle of Kettering town itself, which has been in Kettering for some 40 or 50 years and is very much part of the local community and of the fabric of local life. There are other very small migrant groups in my constituency, but it is predominantly made up of indigenous British people. However, there is huge concern among my constituents about the level of immigration into our country. I believe that I am right in saying that, in recent years, net immigration into the United Kingdom is the largest wave of immigration that our country has ever known and, proportionately, is probably the biggest wave of immigration since the Norman conquest. My contention is that our country simply cannot cope with immigration on that scale-to coin a phrase, we simply cannot go on like this. It is about time that mainstream politicians started airing the views of their constituents, because for too long people have muttered under their breath that they are concerned about immigration. They have been frightened to speak out about it because they are frightened of being accused of being racist. My contention is that immigration is not a racist issue; it is a question of numbers. I personally could not care tuppence about the ethnicity of the immigrants concerned, the colour of their skin or the language that they speak. What I am concerned about is the very large numbers of new arrivals to our country. My contention is that the United Kingdom simply cannot cope with them.</span></span>
<span><span class="co">#&gt; 10: I thank Mr. Speaker for giving me permission to hold this debate today. I welcome the Minister-I very much appreciate the contact from his office prior to today-and the Conservative and Liberal Democrat Front Benchers to the debate. I also welcome my hon. Friends on the Back Benches. Immigration is the most important issue for my constituents. I get more complaints, comments and suggestions about immigration than about anything else. In the Kettering constituency, the number of immigrants is actually very low. There is a well-settled Sikh community in the middle of Kettering town itself, which has been in Kettering for some 40 or 50 years and is very much part of the local community and of the fabric of local life. There are other very small migrant groups in my constituency, but it is predominantly made up of indigenous British people. However, there is huge concern among my constituents about the level of immigration into our country. I believe that I am right in saying that, in recent years, net immigration into the United Kingdom is the largest wave of immigration that our country has ever known and, proportionately, is probably the biggest wave of immigration since the Norman conquest. My contention is that our country simply cannot cope with immigration on that scale-to coin a phrase, we simply cannot go on like this. It is about time that mainstream politicians started airing the views of their constituents, because for too long people have muttered under their breath that they are concerned about immigration. They have been frightened to speak out about it because they are frightened of being accused of being racist. My contention is that immigration is not a racist issue; it is a question of numbers. I personally could not care tuppence about the ethnicity of the immigrants concerned, the colour of their skin or the language that they speak. What I am concerned about is the very large numbers of new arrivals to our country. My contention is that the United Kingdom simply cannot cope with them.</span></span>
<span><span class="co">#&gt;          token    tag  score</span></span>
<span><span class="co">#&gt;         &lt;char&gt; &lt;char&gt;  &lt;num&gt;</span></span>
<span><span class="co">#&gt;  1:          I    PRP 1.0000</span></span>
<span><span class="co">#&gt;  2:      thank    VBP 0.9992</span></span>
<span><span class="co">#&gt;  3:        Mr.    NNP 1.0000</span></span>
<span><span class="co">#&gt;  4:    Speaker    NNP 1.0000</span></span>
<span><span class="co">#&gt;  5:        for     IN 1.0000</span></span>
<span><span class="co">#&gt;  6:     giving    VBG 1.0000</span></span>
<span><span class="co">#&gt;  7:         me    PRP 1.0000</span></span>
<span><span class="co">#&gt;  8: permission     NN 0.9999</span></span>
<span><span class="co">#&gt;  9:         to     TO 0.9999</span></span>
<span><span class="co">#&gt; 10:       hold     VB 1.0000</span></span></code></pre></div>
</div>
<div class="section level4">
<h4 id="tagging-entities-across-full-dataframe">Tagging Entities Across Full DataFrame<a class="anchor" aria-label="anchor" href="#tagging-entities-across-full-dataframe"></a>
</h4>
<div style="text-align: justify;">
<p>This section focuses on performing Named Entity Recognition (NER) on
data stored in a dataframe format. My goal is to identify and tag named
entities within text that is organized in a structured dataframe.</p>
<p>I load the flaiR package and use the built-in uk_immigration dataset.
For demonstration purposes, I’m only taking the first two rows. This
dataset contains discussions about immigration in the UK.</p>
<p>Load the pre-trained model <code>ner</code>. For more pre-trained
models, see <a href="https://flairnlp.github.io/docs/tutorial-basics/tagging-entities" class="external-link uri">https://flairnlp.github.io/docs/tutorial-basics/tagging-entities</a>.</p>
</div>
<div class="sourceCode" id="cb32"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://davidycliao.github.io/flaiR">flaiR</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="va">uk_immigration</span><span class="op">)</span></span>
<span><span class="va">uk_immigration</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">uk_immigration</span>, n <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span></code></pre></div>
<div style="text-align: justify;">
<p>Next, I load the latest model hosted and maintained on Hugging Face
by the Flair NLP team. For more Flair NER models, you can visit the
official Flair NLP page on Hugging Face (<a href="https://huggingface.co/flair" class="external-link uri">https://huggingface.co/flair</a>).</p>
</div>
<div class="sourceCode" id="cb33"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load model without displaying tags</span></span>
<span><span class="co"># tagger &lt;- load_tagger_ner("flair/ner-english-large", show_tags = FALSE)</span></span>
<span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://davidycliao.github.io/flaiR">flaiR</a></span><span class="op">)</span></span>
<span><span class="va">tagger_ner</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/load_tagger_ner.html">load_tagger_ner</a></span><span class="op">(</span><span class="st">"flair/ner-english-ontonotes"</span><span class="op">)</span></span>
<span><span class="co">#&gt; 2025-01-19 02:01:20,307 SequenceTagger predicts: Dictionary with 75 tags: O, S-PERSON, B-PERSON, E-PERSON, I-PERSON, S-GPE, B-GPE, E-GPE, I-GPE, S-ORG, B-ORG, E-ORG, I-ORG, S-DATE, B-DATE, E-DATE, I-DATE, S-CARDINAL, B-CARDINAL, E-CARDINAL, I-CARDINAL, S-NORP, B-NORP, E-NORP, I-NORP, S-MONEY, B-MONEY, E-MONEY, I-MONEY, S-PERCENT, B-PERCENT, E-PERCENT, I-PERCENT, S-ORDINAL, B-ORDINAL, E-ORDINAL, I-ORDINAL, S-LOC, B-LOC, E-LOC, I-LOC, S-TIME, B-TIME, E-TIME, I-TIME, S-WORK_OF_ART, B-WORK_OF_ART, E-WORK_OF_ART, I-WORK_OF_ART, S-FAC</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; NER Tagger Dictionary:</span></span>
<span><span class="co">#&gt; ========================================</span></span>
<span><span class="co">#&gt; Total tags: 75</span></span>
<span><span class="co">#&gt; Model: flair/ner-english-ontonotes</span></span>
<span><span class="co">#&gt; ----------------------------------------</span></span>
<span><span class="co">#&gt; Special        : O, &lt;START&gt;, &lt;STOP&gt;</span></span>
<span><span class="co">#&gt; Person         : S-PERSON, B-PERSON, E-PERSON, I-PERSON</span></span>
<span><span class="co">#&gt; Organization   : S-ORG, B-ORG, E-ORG, I-ORG</span></span>
<span><span class="co">#&gt; Location       : S-GPE, B-GPE, E-GPE, I-GPE, S-LOC, B-LOC, E-LOC, I-LOC</span></span>
<span><span class="co">#&gt; Time           : S-DATE, B-DATE, E-DATE, I-DATE, S-TIME, B-TIME, E-TIME, I-TIME</span></span>
<span><span class="co">#&gt; Numbers        : S-CARDINAL, B-CARDINAL, E-CARDINAL, I-CARDINAL, S-MONEY, B-MONEY, E-MONEY, I-MONEY, S-PERCENT, B-PERCENT, E-PERCENT, I-PERCENT, S-ORDINAL, B-ORDINAL, E-ORDINAL, I-ORDINAL</span></span>
<span><span class="co">#&gt; Groups         : S-NORP, B-NORP, E-NORP, I-NORP</span></span>
<span><span class="co">#&gt; Facilities     : S-FAC, B-FAC, E-FAC, I-FAC</span></span>
<span><span class="co">#&gt; Products       : S-PRODUCT, B-PRODUCT, E-PRODUCT, I-PRODUCT</span></span>
<span><span class="co">#&gt; Events         : S-EVENT, B-EVENT, E-EVENT, I-EVENT</span></span>
<span><span class="co">#&gt; Art            : S-WORK_OF_ART, B-WORK_OF_ART, E-WORK_OF_ART, I-WORK_OF_ART</span></span>
<span><span class="co">#&gt; Languages      : S-LANGUAGE, B-LANGUAGE, E-LANGUAGE, I-LANGUAGE</span></span>
<span><span class="co">#&gt; Laws           : S-LAW, B-LAW, E-LAW, I-LAW</span></span>
<span><span class="co">#&gt; ----------------------------------------</span></span>
<span><span class="co">#&gt; Tag scheme: BIOES</span></span>
<span><span class="co">#&gt; B-: Beginning of multi-token entity</span></span>
<span><span class="co">#&gt; I-: Inside of multi-token entity</span></span>
<span><span class="co">#&gt; O: Outside (not part of any entity)</span></span>
<span><span class="co">#&gt; E-: End of multi-token entity</span></span>
<span><span class="co">#&gt; S-: Single token entity</span></span>
<span><span class="co">#&gt; ========================================</span></span></code></pre></div>
<div style="text-align: justify;">
<p>I load a pre-trained NER model. Since I’m using a Mac M1/M2, I set
the model to run on the MPS device for faster processing. If I want to
use other pre-trained models, I can check the Flair documentation
website for available options.</p>
</div>
<p>Now I’m ready to process the text:</p>
<div class="sourceCode" id="cb34"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">results</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/get_entities.html">get_entities</a></span><span class="op">(</span>texts <span class="op">=</span> <span class="va">uk_immigration</span><span class="op">$</span><span class="va">text</span>,</span>
<span>                        doc_ids <span class="op">=</span> <span class="va">uk_immigration</span><span class="op">$</span><span class="va">speaker</span>,</span>
<span>                        tagger <span class="op">=</span> <span class="va">tagger_ner</span>,</span>
<span>                        batch_size <span class="op">=</span> <span class="fl">2</span>,</span>
<span>                        verbose <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="co">#&gt; CPU is used.</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">results</span>, n <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span>
<span><span class="co">#&gt;               doc_id                          entity    tag     score</span></span>
<span><span class="co">#&gt;               &lt;char&gt;                          &lt;char&gt; &lt;char&gt;     &lt;num&gt;</span></span>
<span><span class="co">#&gt;  1: Philip Hollobone                           today   DATE 0.9843613</span></span>
<span><span class="co">#&gt;  2: Philip Hollobone                    Conservative   NORP 0.9976857</span></span>
<span><span class="co">#&gt;  3: Philip Hollobone Liberal Democrat Front Benchers    ORG 0.7668477</span></span>
<span><span class="co">#&gt;  4: Philip Hollobone                       Kettering    GPE 0.9885774</span></span>
<span><span class="co">#&gt;  5: Philip Hollobone                            Sikh   NORP 0.9939976</span></span>
<span><span class="co">#&gt;  6: Philip Hollobone                       Kettering    GPE 0.9955219</span></span>
<span><span class="co">#&gt;  7: Philip Hollobone                       Kettering    GPE 0.9948049</span></span>
<span><span class="co">#&gt;  8: Philip Hollobone             some 40 or 50 years   DATE 0.8059650</span></span>
<span><span class="co">#&gt;  9: Philip Hollobone                         British   NORP 0.9986913</span></span>
<span><span class="co">#&gt; 10: Philip Hollobone                    recent years   DATE 0.8596769</span></span></code></pre></div>
<p> </p>
<hr>
</div>
</div>
<div class="section level3">
<h3 id="highlight-entities-with-colors">Highlight Entities with Colors<a class="anchor" aria-label="anchor" href="#highlight-entities-with-colors"></a>
</h3>
<div style="text-align: justify;">
<p>This tutorial demonstrates how to use the flaiR package to identify
and highlight named entities (such as names, locations, organizations)
in text.</p>
<p><u><strong>Step 1</strong></u> Create Text with Named Entities</p>
<p>First, we load the flaiR package and work with a sample text:</p>
</div>
<div class="sourceCode" id="cb35"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://davidycliao.github.io/flaiR">flaiR</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="st">"uk_immigration"</span><span class="op">)</span></span>
<span><span class="va">uk_immigration</span> <span class="op">&lt;-</span> <span class="va">uk_immigration</span><span class="op">[</span><span class="fl">30</span>,<span class="op">]</span></span>
<span><span class="va">tagger_ner</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/load_tagger_ner.html">load_tagger_ner</a></span><span class="op">(</span><span class="st">"flair/ner-english-fast"</span><span class="op">)</span></span>
<span><span class="co">#&gt; 2025-01-19 02:01:28,453 SequenceTagger predicts: Dictionary with 20 tags: &lt;unk&gt;, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, &lt;START&gt;, &lt;STOP&gt;</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; NER Tagger Dictionary:</span></span>
<span><span class="co">#&gt; ========================================</span></span>
<span><span class="co">#&gt; Total tags: 20</span></span>
<span><span class="co">#&gt; Model: flair/ner-english-fast</span></span>
<span><span class="co">#&gt; ----------------------------------------</span></span>
<span><span class="co">#&gt; Special        : &lt;unk&gt;, O, &lt;START&gt;, &lt;STOP&gt;</span></span>
<span><span class="co">#&gt; Organization   : S-ORG, B-ORG, E-ORG, I-ORG</span></span>
<span><span class="co">#&gt; Location       : S-LOC, B-LOC, E-LOC, I-LOC</span></span>
<span><span class="co">#&gt; Misc           : S-MISC, B-MISC, I-MISC, E-MISC</span></span>
<span><span class="co">#&gt; ----------------------------------------</span></span>
<span><span class="co">#&gt; Tag scheme: BIOES</span></span>
<span><span class="co">#&gt; B-: Beginning of multi-token entity</span></span>
<span><span class="co">#&gt; I-: Inside of multi-token entity</span></span>
<span><span class="co">#&gt; O: Outside (not part of any entity)</span></span>
<span><span class="co">#&gt; E-: End of multi-token entity</span></span>
<span><span class="co">#&gt; S-: Single token entity</span></span>
<span><span class="co">#&gt; ========================================</span></span>
<span></span>
<span><span class="va">result</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/get_entities.html">get_entities</a></span><span class="op">(</span><span class="va">uk_immigration</span><span class="op">$</span><span class="va">text</span>,</span>
<span>                       tagger <span class="op">=</span> <span class="va">tagger_ner</span>,</span>
<span>                       show.text_id <span class="op">=</span> <span class="cn">FALSE</span></span>
<span>                       <span class="op">)</span></span>
<span><span class="co">#&gt; CPU is used.</span></span>
<span><span class="co">#&gt; Warning in check_texts_and_ids(texts, doc_ids): doc_ids is NULL.</span></span>
<span><span class="co">#&gt; Auto-assigning doc_ids.</span></span></code></pre></div>
<p><u><strong>Step 2</strong></u> Highlight the Named Entities</p>
<div style="text-align: justify;">
<p>Use the <code>highlight_text</code> function to color-code the
identified entities:</p>
</div>
<div class="sourceCode" id="cb36"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">highlighted_text</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/highlight_text.html">highlight_text</a></span><span class="op">(</span>text <span class="op">=</span> <span class="va">uk_immigration</span><span class="op">$</span><span class="va">text</span>, </span>
<span>                                   entities_mapping <span class="op">=</span> <span class="fu"><a href="../reference/map_entities.html">map_entities</a></span><span class="op">(</span><span class="va">result</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">highlighted_text</span></span></code></pre></div>
<div style="text-align: justify; font-family: Arial">I am grateful to the Minister for that most helpful intervention. Perhaps I should bring us back on track. Although asylum is an important issue for all hon. Members and our constituents, the number of asylum claims is small compared with immigration as a whole. I believe that asylum claims are now running at a rate of about 30,000 a year, which is only 10 per cent. of net foreign migration. The big problem in this country is legal immigration, which brings us back to the population projections of 70 million. I understand that a migrant now arrives on our shores every minute. We must build a new home every six minutes for new migrants. Immigration will add 7 million to the population of <span style="background-color: lightblue; color: black; font-family: Arial">England</span> <span style="color: blue; font-family: Arial">(LOC)</span> in the next 15 to 20 years, which is seven times the population of <span style="background-color: lightblue; color: black; font-family: Arial">Birmingham</span> <span style="color: blue; font-family: Arial">(LOC)</span>. Immigration directly added a million people to the <span style="background-color: lightblue; color: black; font-family: Arial">UK</span> <span style="color: blue; font-family: Arial">(LOC)</span>'s population in the years 2003 to 2007. There was a net inflow of 2.3 million people to the <span style="background-color: lightblue; color: black; font-family: Arial">UK</span> <span style="color: blue; font-family: Arial">(LOC)</span> between 1991 and 2006, and 8 per cent. came from the new east <span style="background-color: yellow; color: black; font-family: Arial">European</span> <span style="color: orange; font-family: Arial">(MISC)</span> members of the <span style="background-color: pink; color: black; font-family: Arial">EU</span> <span style="color: pink; font-family: Arial">(ORG)</span>. I differ from my party in not agreeing with the free movement of people across <span style="background-color: pink; color: black; font-family: Arial">EU</span> <span style="color: pink; font-family: Arial">(ORG)</span> borders. Effectively, we have, by agreement of our Government, uncontrolled immigration within the <span style="background-color: pink; color: black; font-family: Arial">EU</span> <span style="color: pink; font-family: Arial">(ORG)</span>. The Government told us that there would be 13,000 new arrivals from the new entrant eastern <span style="background-color: yellow; color: black; font-family: Arial">European</span> <span style="color: orange; font-family: Arial">(MISC)</span> <span style="background-color: pink; color: black; font-family: Arial">EU</span> <span style="color: pink; font-family: Arial">(ORG)</span> countries, but the figure was approaching 1 million at its peak-perhaps the Minister will confirm what the figure was-which was a world apart from the 13,000 that we were told about, and it has placed huge strain on local infrastructure.</div>
<p><strong>Explanation:</strong></p>
<ul>
<li>
<code>load_tagger_ner("ner")</code> loads the pre-trained NER
model</li>
<li>
<code><a href="../reference/get_entities.html">get_entities()</a></code> identifies named entities in the
text</li>
<li>
<code><a href="../reference/map_entities.html">map_entities()</a></code> maps the identified entities to
colors</li>
<li>
<code><a href="../reference/highlight_text.html">highlight_text()</a></code> marks the original text using these
colors</li>
</ul>
<div style="text-align: justify;">
<p>Each type of entity (such as person names, locations, organization
names) will be displayed in a different color, making the named entities
in the text immediately visible.</p>
</div>
<p> </p>
<hr>
</div>
</div>
<div class="section level2">
<h2 id="the-overview-of-embedding">The Overview of Embedding<a class="anchor" aria-label="anchor" href="#the-overview-of-embedding"></a>
</h2>
<div style="text-align: justify">
<p>All word embedding classes inherit from the
<code>TokenEmbeddings</code> class and call the <code><a href="https://rdrr.io/r/stats/embed.html" class="external-link">embed()</a></code>
method to embed the text. In most cases when using Flair, various and
complex embedding processes are hidden behind the interface. Users
simply need to instantiate the necessary embedding class and call
<code><a href="https://rdrr.io/r/stats/embed.html" class="external-link">embed()</a></code> to embed text.</p>
<p>Here are the types of embeddings currently supported in FlairNLP:</p>
<table class="table">
<colgroup>
<col width="33%">
<col width="33%">
<col width="33%">
</colgroup>
<thead><tr class="header">
<th>Class</th>
<th>Type</th>
<th>Paper</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><a href="articles/transformer_wordembeddings.html#bytepairembeddings"><code>BytePairEmbeddings</code></a></td>
<td>Subword-level word embeddings</td>
<td><a href="https://www.aclweb.org/anthology/L18-1473" class="external-link">Heinzerling and
Strube (2018)</a></td>
</tr>
<tr class="even">
<td><a href=""><code>CharacterEmbeddings</code></a></td>
<td>Task-trained character-level embeddings of words</td>
<td><a href="https://www.aclweb.org/anthology/N16-1030" class="external-link">Lample et
al. (2016)</a></td>
</tr>
<tr class="odd">
<td><a href=""><code>ELMoEmbeddings</code></a></td>
<td>Contextualized word-level embeddings</td>
<td><a href="https://aclweb.org/anthology/N18-1202" class="external-link">Peters et
al. (2018)</a></td>
</tr>
<tr class="even">
<td><a href=""><code>FastTextEmbeddings</code></a></td>
<td>Word embeddings with subword features</td>
<td><a href="https://aclweb.org/anthology/Q17-1010" class="external-link">Bojanowski et
al. (2017)</a></td>
</tr>
<tr class="odd">
<td><a href="articles/transformer_wordembeddings.html#flair-embeddings"><code>FlairEmbeddings</code></a></td>
<td>Contextualized character-level embeddings</td>
<td><a href="https://www.aclweb.org/anthology/C18-1139/" class="external-link">Akbik et
al. (2018)</a></td>
</tr>
<tr class="even">
<td><a href=""><code>OneHotEmbeddings</code></a></td>
<td>Standard one-hot embeddings of text or tags</td>
<td>-</td>
</tr>
<tr class="odd">
<td><a href="articles/transformer_wordembeddings.html#pooled-flair-embeddings"><code>PooledFlairEmbeddings</code></a></td>
<td>Pooled variant of <code>FlairEmbeddings</code>
</td>
<td><a href="https://www.aclweb.org/anthology/N19-1078/" class="external-link">Akbik et
al. (2019)</a></td>
</tr>
<tr class="even">
<td>
<a href="articles/transformer_wordembeddings.html#transformer-embeddings"><code>TransformerWordEmbeddings</code></a>)</td>
<td>Embeddings from pretrained <a href="https://huggingface.co/transformers/pretrained_models.html" class="external-link">transformers</a>
(BERT, XLM, GPT, RoBERTa, XLNet, DistilBERT etc.)</td>
<td>
<a href="https://www.aclweb.org/anthology/N19-1423/" class="external-link">Devlin et
al. (2018)</a> <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" class="external-link">Radford
et al. (2018)</a> <a href="https://arxiv.org/abs/1907.11692" class="external-link">Liu et
al. (2019)</a> <a href="https://arxiv.org/abs/1901.02860" class="external-link">Dai et
al. (2019)</a> <a href="https://arxiv.org/abs/1906.08237" class="external-link">Yang et
al. (2019)</a> <a href="https://arxiv.org/abs/1901.07291" class="external-link">Lample and
Conneau (2019)</a>
</td>
</tr>
<tr class="odd">
<td><a href="https://github.com/flairNLP/flair/tree/master/resources/docs/embeddings/CLASSIC_WORD_EMBEDDINGS.md" class="external-link"><code>WordEmbeddings</code></a></td>
<td>Classic word embeddings</td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p> </p>
<hr>
<div class="section level3">
<h3 id="byte-pair-embeddings">Byte Pair Embeddings<a class="anchor" aria-label="anchor" href="#byte-pair-embeddings"></a>
</h3>
<div style="text-align: justify">
<p><em>Please note that ihis document for R is a conversion of the <a href="https://github.com/flairNLP/flair/blob/master/resources/docs/embeddings/BYTE_PAIR_EMBEDDINGS.md" class="external-link">Flair
NLP</a> document implemented in Python.</em></p>
<p><code>BytePairEmbeddings</code> are word embeddings that operate at
the subword level. They can embed any word by breaking it down into
subwords and looking up their corresponding embeddings. This technique
was introduced by <a href="https://www.aclweb.org/anthology/L18-1473" class="external-link">Heinzerling and Strube
(2018)</a> , who demonstrated that BytePairEmbeddings achieve comparable
accuracy to traditional word embeddings while requiring only a fraction
of the model size. This makes them an excellent choice for training
compact models.</p>
<p>To initialize BytePairEmbeddings, you need to specify:</p>
<ul>
<li>A language code (275 languages supported)</li>
<li>Number of syllables</li>
<li>Number of dimensions (options: 50, 100, 200, or 300)</li>
</ul>
<div class="sourceCode" id="cb37"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://davidycliao.github.io/flaiR">flaiR</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Initialize embedding</span></span>
<span><span class="va">BytePairEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">BytePairEmbeddings</span></span>
<span></span>
<span><span class="co"># Create BytePairEmbeddings with specified parameters</span></span>
<span><span class="va">embedding</span> <span class="op">&lt;-</span> <span class="fu">BytePairEmbeddings</span><span class="op">(</span></span>
<span>    language <span class="op">=</span> <span class="st">"en"</span>,        <span class="co"># Language code (e.g., "en" for English)</span></span>
<span>    dim <span class="op">=</span> <span class="fl">50L</span>,              <span class="co"># Embedding dimensions: options are 50L, 100L, 200L, or 300L</span></span>
<span>    syllables <span class="op">=</span> <span class="fl">100000L</span>     <span class="co"># Subword vocabulary size</span></span>
<span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Create a sample sentence</span></span>
<span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span>
<span><span class="va">sentence</span> <span class="op">=</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">'The grass is green .'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Embed words in the sentence</span></span>
<span><span class="va">embedding</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Sentence[5]: "The grass is green ."</span></span>
<span></span>
<span><span class="co"># Print embeddings </span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="va">tokens</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">token</span> <span class="op">&lt;-</span> <span class="va">sentence</span><span class="op">$</span><span class="va">tokens</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"\nWord:"</span>, <span class="va">token</span><span class="op">$</span><span class="va">text</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span>    </span>
<span>    <span class="co"># Convert embedding to R vector and print</span></span>
<span>    <span class="co"># Python index starts from 0, so use i-1</span></span>
<span>    <span class="va">embedding_vector</span> <span class="op">&lt;-</span> <span class="va">sentence</span><span class="op">[</span><span class="va">i</span><span class="op">-</span><span class="fl">1</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">numpy</span><span class="op">(</span><span class="op">)</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"Embedding shape:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">embedding_vector</span><span class="op">)</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"First 5 values:"</span>, <span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">embedding_vector</span>, <span class="fl">5</span><span class="op">)</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"-------------------\n"</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Word: The </span></span>
<span><span class="co">#&gt; Embedding shape: 100 </span></span>
<span><span class="co">#&gt; First 5 values: -0.585645 0.55233 -0.335385 -0.117119 -0.3433 </span></span>
<span><span class="co">#&gt; -------------------</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Word: grass </span></span>
<span><span class="co">#&gt; Embedding shape: 100 </span></span>
<span><span class="co">#&gt; First 5 values: 0.370427 -0.717806 -0.489089 0.384228 0.68443 </span></span>
<span><span class="co">#&gt; -------------------</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Word: is </span></span>
<span><span class="co">#&gt; Embedding shape: 100 </span></span>
<span><span class="co">#&gt; First 5 values: -0.186592 0.52804 -1.011618 0.416936 -0.166446 </span></span>
<span><span class="co">#&gt; -------------------</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Word: green </span></span>
<span><span class="co">#&gt; Embedding shape: 100 </span></span>
<span><span class="co">#&gt; First 5 values: -0.075467 -0.874228 0.20425 1.061623 -0.246111 </span></span>
<span><span class="co">#&gt; -------------------</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Word: . </span></span>
<span><span class="co">#&gt; Embedding shape: 100 </span></span>
<span><span class="co">#&gt; First 5 values: -0.214652 0.212236 -0.607079 0.512853 -0.325556 </span></span>
<span><span class="co">#&gt; -------------------</span></span></code></pre></div>
<p>More information can be found on the <a href="https://nlp.h-its.org/bpemb/" class="external-link">byte pair embeddings</a> web page.
<code>BytePairEmbeddings</code> also have a multilingual model capable
of embedding any word in any language. You can instantiate it with:</p>
<div class="sourceCode" id="cb38"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">embedding</span> <span class="op">&lt;-</span> <span class="fu">BytePairEmbeddings</span><span class="op">(</span><span class="st">'multi'</span><span class="op">)</span></span></code></pre></div>
<p>You can also load custom <code>BytePairEmbeddings</code> by
specifying a path to model_file_path and embedding_file_path arguments.
They correspond respectively to a <code>SentencePiece</code> model file
and to an embedding file (Word2Vec plain text or GenSim binary).</p>
</div>
</div>
<div class="section level3">
<h3 id="flair-embeddings">Flair Embeddings<a class="anchor" aria-label="anchor" href="#flair-embeddings"></a>
</h3>
<div style="text-align: justify">
<p><strong>The following example manual is translated into R from Flair
NLP by <a href="https://engineering.zalando.com/posts/2018/11/zalando-research-releases-flair.html" class="external-link">Zalando
Research</a>.</strong> In Flair, the use of embedding is very quite
straightforward. Here’s an example code snippet of how to use Flair’s
contextual string embeddings:</p>
<div class="sourceCode" id="cb39"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://davidycliao.github.io/flaiR">flaiR</a></span><span class="op">)</span></span>
<span><span class="va">FlairEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">FlairEmbeddings</span></span>
<span><span class="co"># init embedding</span></span>
<span><span class="va">flair_embedding_forward</span> <span class="op">&lt;-</span> <span class="fu">FlairEmbeddings</span><span class="op">(</span><span class="st">'news-forward'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># create a sentence</span></span>
<span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span>
<span><span class="va">sentence</span> <span class="op">=</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">'The grass is green .'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># embed words in sentence</span></span>
<span><span class="va">flair_embedding_forward</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Sentence[5]: "The grass is green ."</span></span></code></pre></div>
<table class="table">
<colgroup>
<col width="33%">
<col width="33%">
<col width="33%">
</colgroup>
<thead><tr class="header">
<th>ID</th>
<th>Language</th>
<th>Embedding</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>‘multi-X’</td>
<td>300+</td>
<td>
<a href="http://opus.nlpl.eu/JW300.php" class="external-link">JW300 corpus</a>, as
proposed by <a href="https://www.aclweb.org/anthology/P19-1310/" class="external-link">Agić
and Vulić (2019)</a>. The corpus is licensed under CC-BY-NC-SA</td>
</tr>
<tr class="even">
<td>‘multi-X-fast’</td>
<td>English, German, French, Italian, Dutch, Polish</td>
<td>Mix of corpora (Web, Wikipedia, Subtitles, News), CPU-friendly</td>
</tr>
<tr class="odd">
<td>‘news-X’</td>
<td>English</td>
<td>Trained with 1 billion word corpus</td>
</tr>
<tr class="even">
<td>‘news-X-fast’</td>
<td>English</td>
<td>Trained with 1 billion word corpus, CPU-friendly</td>
</tr>
<tr class="odd">
<td>‘mix-X’</td>
<td>English</td>
<td>Trained with mixed corpus (Web, Wikipedia, Subtitles)</td>
</tr>
<tr class="even">
<td>‘ar-X’</td>
<td>Arabic</td>
<td>Added by <a href="https://github.com/zalandoresearch/flair/issues/614" class="external-link"><span class="citation">@stefan-it</span></a>: Trained with Wikipedia/OPUS</td>
</tr>
<tr class="odd">
<td>‘bg-X’</td>
<td>Bulgarian</td>
<td>Added by <a href="https://github.com/zalandoresearch/flair/issues/614" class="external-link"><span class="citation">@stefan-it</span></a>: Trained with Wikipedia/OPUS</td>
</tr>
<tr class="even">
<td>‘bg-X-fast’</td>
<td>Bulgarian</td>
<td>Added by <a href="https://github.com/stefan-it/flair-lms" class="external-link"><span class="citation">@stefan-it</span></a>: Trained with various sources
(Europarl, Wikipedia or SETimes)</td>
</tr>
<tr class="odd">
<td>‘cs-X’</td>
<td>Czech</td>
<td>Added by <a href="https://github.com/zalandoresearch/flair/issues/614" class="external-link"><span class="citation">@stefan-it</span></a>: Trained with Wikipedia/OPUS</td>
</tr>
<tr class="even">
<td>‘cs-v0-X’</td>
<td>Czech</td>
<td>Added by <a href="https://github.com/stefan-it/flair-lms" class="external-link"><span class="citation">@stefan-it</span></a>: LM embeddings (earlier
version)</td>
</tr>
<tr class="odd">
<td>‘de-X’</td>
<td>German</td>
<td>Trained with mixed corpus (Web, Wikipedia, Subtitles)</td>
</tr>
<tr class="even">
<td>‘de-historic-ha-X’</td>
<td>German (historical)</td>
<td>Added by <a href="https://github.com/stefan-it/flair-lms" class="external-link"><span class="citation">@stefan-it</span></a>: Historical German trained over
<em>Hamburger Anzeiger</em>
</td>
</tr>
<tr class="odd">
<td>‘de-historic-wz-X’</td>
<td>German (historical)</td>
<td>Added by <a href="https://github.com/stefan-it/flair-lms" class="external-link"><span class="citation">@stefan-it</span></a>: Historical German trained over
<em>Wiener Zeitung</em>
</td>
</tr>
<tr class="even">
<td>‘de-historic-rw-X’</td>
<td>German (historical)</td>
<td>Added by <a href="https://github.com/redewiedergabe" class="external-link"><span class="citation">@redewiedergabe</span></a>: Historical German trained
over 100 million tokens</td>
</tr>
<tr class="odd">
<td>‘es-X’</td>
<td>Spanish</td>
<td>Added by <a href="https://github.com/zalandoresearch/flair/issues/80" class="external-link"><span class="citation">@iamyihwa</span></a>: Trained with Wikipedia</td>
</tr>
<tr class="even">
<td>‘es-X-fast’</td>
<td>Spanish</td>
<td>Added by <a href="https://github.com/zalandoresearch/flair/issues/80" class="external-link"><span class="citation">@iamyihwa</span></a>: Trained with Wikipedia,
CPU-friendly</td>
</tr>
<tr class="odd">
<td>‘es-clinical-’</td>
<td>Spanish (clinical)</td>
<td>Added by <a href="https://github.com/flairNLP/flair/issues/2292" class="external-link"><span class="citation">@matirojasg</span></a>: Trained with Wikipedia</td>
</tr>
<tr class="even">
<td>‘eu-X’</td>
<td>Basque</td>
<td>Added by <a href="https://github.com/zalandoresearch/flair/issues/614" class="external-link"><span class="citation">@stefan-it</span></a>: Trained with Wikipedia/OPUS</td>
</tr>
<tr class="odd">
<td>‘eu-v0-X’</td>
<td>Basque</td>
<td>Added by <a href="https://github.com/stefan-it/flair-lms" class="external-link"><span class="citation">@stefan-it</span></a>: LM embeddings (earlier
version)</td>
</tr>
<tr class="even">
<td>‘fa-X’</td>
<td>Persian</td>
<td>Added by <a href="https://github.com/zalandoresearch/flair/issues/614" class="external-link"><span class="citation">@stefan-it</span></a>: Trained with Wikipedia/OPUS</td>
</tr>
<tr class="odd">
<td>‘fi-X’</td>
<td>Finnish</td>
<td>Added by <a href="https://github.com/zalandoresearch/flair/issues/614" class="external-link"><span class="citation">@stefan-it</span></a>: Trained with Wikipedia/OPUS</td>
</tr>
<tr class="even">
<td>‘fr-X’</td>
<td>French</td>
<td>Added by <a href="https://github.com/mhham" class="external-link"><span class="citation">@mhham</span></a>: Trained with French Wikipedia</td>
</tr>
<tr class="odd">
<td>‘he-X’</td>
<td>Hebrew</td>
<td>Added by <a href="https://github.com/zalandoresearch/flair/issues/614" class="external-link"><span class="citation">@stefan-it</span></a>: Trained with Wikipedia/OPUS</td>
</tr>
<tr class="even">
<td>‘hi-X’</td>
<td>Hindi</td>
<td>Added by <a href="https://github.com/zalandoresearch/flair/issues/614" class="external-link"><span class="citation">@stefan-it</span></a>: Trained with Wikipedia/OPUS</td>
</tr>
<tr class="odd">
<td>‘hr-X’</td>
<td>Croatian</td>
<td>Added by <a href="https://github.com/zalandoresearch/flair/issues/614" class="external-link"><span class="citation">@stefan-it</span></a>: Trained with Wikipedia/OPUS</td>
</tr>
<tr class="even">
<td>‘id-X’</td>
<td>Indonesian</td>
<td>Added by <a href="https://github.com/zalandoresearch/flair/issues/614" class="external-link"><span class="citation">@stefan-it</span></a>: Trained with Wikipedia/OPUS</td>
</tr>
<tr class="odd">
<td>‘it-X’</td>
<td>Italian</td>
<td>Added by <a href="https://github.com/zalandoresearch/flair/issues/614" class="external-link"><span class="citation">@stefan-it</span></a>: Trained with Wikipedia/OPUS</td>
</tr>
<tr class="even">
<td>‘ja-X’</td>
<td>Japanese</td>
<td>Added by <a href="https://github.com/zalandoresearch/flair/issues/527" class="external-link"><span class="citation">@frtacoa</span></a>: Trained with 439M words of
Japanese Web crawls (2048 hidden states, 2 layers)</td>
</tr>
<tr class="odd">
<td>‘nl-X’</td>
<td>Dutch</td>
<td>Added by <a href="https://github.com/zalandoresearch/flair/issues/614" class="external-link"><span class="citation">@stefan-it</span></a>: Trained with Wikipedia/OPUS</td>
</tr>
<tr class="even">
<td>‘nl-v0-X’</td>
<td>Dutch</td>
<td>Added by <a href="https://github.com/stefan-it/flair-lms" class="external-link"><span class="citation">@stefan-it</span></a>: LM embeddings (earlier
version)</td>
</tr>
<tr class="odd">
<td>‘no-X’</td>
<td>Norwegian</td>
<td>Added by <a href="https://github.com/zalandoresearch/flair/issues/614" class="external-link"><span class="citation">@stefan-it</span></a>: Trained with Wikipedia/OPUS</td>
</tr>
<tr class="even">
<td>‘pl-X’</td>
<td>Polish</td>
<td>Added by <a href="https://github.com/applicaai/poleval-2018" class="external-link"><span class="citation">@borchmann</span></a>: Trained with web crawls (Polish
part of CommonCrawl)</td>
</tr>
<tr class="odd">
<td>‘pl-opus-X’</td>
<td>Polish</td>
<td>Added by <a href="https://github.com/zalandoresearch/flair/issues/614" class="external-link"><span class="citation">@stefan-it</span></a>: Trained with Wikipedia/OPUS</td>
</tr>
<tr class="even">
<td>‘pt-X’</td>
<td>Portuguese</td>
<td>Added by <a href="https://github.com/ericlief/language_models" class="external-link"><span class="citation">@ericlief</span></a>: LM embeddings</td>
</tr>
<tr class="odd">
<td>‘sl-X’</td>
<td>Slovenian</td>
<td>Added by <a href="https://github.com/zalandoresearch/flair/issues/614" class="external-link"><span class="citation">@stefan-it</span></a>: Trained with Wikipedia/OPUS</td>
</tr>
<tr class="even">
<td>‘sl-v0-X’</td>
<td>Slovenian</td>
<td>Added by <a href="https://github.com/stefan-it/flair-lms" class="external-link"><span class="citation">@stefan-it</span></a>: Trained with various sources
(Europarl, Wikipedia and OpenSubtitles2018)</td>
</tr>
<tr class="odd">
<td>‘sv-X’</td>
<td>Swedish</td>
<td>Added by <a href="https://github.com/zalandoresearch/flair/issues/614" class="external-link"><span class="citation">@stefan-it</span></a>: Trained with Wikipedia/OPUS</td>
</tr>
<tr class="even">
<td>‘sv-v0-X’</td>
<td>Swedish</td>
<td>Added by <a href="https://github.com/stefan-it/flair-lms" class="external-link"><span class="citation">@stefan-it</span></a>: Trained with various sources
(Europarl, Wikipedia or OpenSubtitles2018)</td>
</tr>
<tr class="odd">
<td>‘ta-X’</td>
<td>Tamil</td>
<td>Added by <a href="https://github.com/stefan-it/plur" class="external-link"><span class="citation">@stefan-it</span></a>
</td>
</tr>
<tr class="even">
<td>‘pubmed-X’</td>
<td>English</td>
<td>Added by <a href="https://github.com/zalandoresearch/flair/pull/519" class="external-link"><span class="citation">@jessepeng</span></a>: Trained with 5% of PubMed
abstracts until 2015 (1150 hidden states, 3 layers)</td>
</tr>
<tr class="odd">
<td>‘de-impresso-hipe-v1-X’</td>
<td>German (historical)</td>
<td>In-domain data (Swiss and Luxembourgish newspapers) for <a href="https://impresso.github.io/CLEF-HIPE-2020" class="external-link">CLEF HIPE Shared
task</a>. More information on the shared task can be found in <a href="https://zenodo.org/record/3752679#.XqgzxXUzZzU" class="external-link">this
paper</a>
</td>
</tr>
<tr class="even">
<td>‘en-impresso-hipe-v1-X’</td>
<td>English (historical)</td>
<td>In-domain data (Chronicling America material) for <a href="https://impresso.github.io/CLEF-HIPE-2020" class="external-link">CLEF HIPE Shared
task</a>. More information on the shared task can be found in <a href="https://zenodo.org/record/3752679#.XqgzxXUzZzU" class="external-link">this
paper</a>
</td>
</tr>
<tr class="odd">
<td>‘fr-impresso-hipe-v1-X’</td>
<td>French (historical)</td>
<td>In-domain data (Swiss and Luxembourgish newspapers) for <a href="https://impresso.github.io/CLEF-HIPE-2020" class="external-link">CLEF HIPE Shared
task</a>. More information on the shared task can be found in <a href="https://zenodo.org/record/3752679#.XqgzxXUzZzU" class="external-link">this
paper</a>
</td>
</tr>
<tr class="even">
<td>‘am-X’</td>
<td>Amharic</td>
<td>Based on 6.5m Amharic text corpus crawled from different sources.
See <a href="https://www.mdpi.com/1999-5903/13/11/275" class="external-link">this paper</a>
and the official <a href="https://github.com/uhh-lt/amharicmodels" class="external-link">GitHub Repository</a> for
more information.</td>
</tr>
<tr class="odd">
<td>‘uk-X’</td>
<td>Ukrainian</td>
<td>Added by <a href="https://github.com/dchaplinsky" class="external-link"><span class="citation">@dchaplinsky</span></a>: Trained with <a href="https://lang.org.ua/en/corpora/" class="external-link">UberText</a> corpus.</td>
</tr>
</tbody>
</table>
<p><strong>Source</strong>: <a href="https://github.com/flairNLP/flair/blob/master/resources/docs/embeddings/FLAIR_EMBEDDINGS.md#flair-embeddings" class="external-link">https://github.com/flairNLP/flair/blob/master/resources/docs/embeddings/FLAIR_EMBEDDINGS.md#flair-embeddings</a></p>
</div>
<div style="text-align: justify">
<p>So, if you want to load embeddings from the German forward LM model,
instantiate the method as follows:</p>
<div class="sourceCode" id="cb40"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">flair_de_forward</span> <span class="op">&lt;-</span> <span class="fu">FlairEmbeddings</span><span class="op">(</span><span class="st">'de-forward'</span><span class="op">)</span></span></code></pre></div>
<p>And if you want to load embeddings from the Bulgarian backward LM
model, instantiate the method as follows:</p>
<div class="sourceCode" id="cb41"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">flair_bg_backward</span> <span class="op">&lt;-</span> <span class="fu">FlairEmbeddings</span><span class="op">(</span><span class="st">'bg-backward'</span><span class="op">)</span></span></code></pre></div>
</div>
<p> </p>
<hr>
</div>
<div class="section level3">
<h3 id="recommended-flair-usage-in-flair-in-r">Recommended Flair Usage in flaiR in R<a class="anchor" aria-label="anchor" href="#recommended-flair-usage-in-flair-in-r"></a>
</h3>
<div style="text-align: justify">
<p>We recommend combining both forward and backward Flair embeddings.
Depending on the task, we also recommend adding standard word embeddings
into the mix. So, our recommended <code>StackedEmbedding</code> for most
English tasks is:</p>
<div class="sourceCode" id="cb42"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">FlairEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">FlairEmbeddings</span></span>
<span><span class="va">WordEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">WordEmbeddings</span></span>
<span><span class="va">StackedEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">StackedEmbeddings</span></span>
<span></span>
<span><span class="co"># create a StackedEmbedding object that combines glove and forward/backward flair embeddings</span></span>
<span><span class="va">stacked_embeddings</span> <span class="op">&lt;-</span> <span class="fu">StackedEmbeddings</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="fu">WordEmbeddings</span><span class="op">(</span><span class="st">"glove"</span><span class="op">)</span>,</span>
<span>                                             <span class="fu">FlairEmbeddings</span><span class="op">(</span><span class="st">"news-forward"</span><span class="op">)</span>,</span>
<span>                                             <span class="fu">FlairEmbeddings</span><span class="op">(</span><span class="st">"news-backward"</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>That’s it! Now just use this embedding like all the other embeddings,
i.e. call the <code><a href="https://rdrr.io/r/stats/embed.html" class="external-link">embed()</a></code> method over your sentences.</p>
<div class="sourceCode" id="cb43"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># create a sentence</span></span>
<span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span>
<span><span class="va">sentence</span> <span class="op">=</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">'The grass is green .'</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb44"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># just embed a sentence using the StackedEmbedding as you would with any single embedding.</span></span>
<span><span class="va">stacked_embeddings</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb45"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># now check out the embedded tokens.</span></span>
<span><span class="co"># Note that Python is indexing from 0. In an R for loop, using seq_along(sentence) - 1 achieves the same effect.</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span>  <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq_along</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">[</span><span class="va">i</span><span class="op">]</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">[</span><span class="va">i</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="co">#&gt; Token[0]: "The"</span></span>
<span><span class="co">#&gt; tensor([-0.0382, -0.2449,  0.7281,  ..., -0.0065, -0.0053,  0.0090])</span></span>
<span><span class="co">#&gt; Token[1]: "grass"</span></span>
<span><span class="co">#&gt; tensor([-0.8135,  0.9404, -0.2405,  ...,  0.0354, -0.0255, -0.0143])</span></span>
<span><span class="co">#&gt; Token[2]: "is"</span></span>
<span><span class="co">#&gt; tensor([-5.4264e-01,  4.1476e-01,  1.0322e+00,  ..., -5.3691e-04,</span></span>
<span><span class="co">#&gt;         -9.6750e-03, -2.7541e-02])</span></span>
<span><span class="co">#&gt; Token[3]: "green"</span></span>
<span><span class="co">#&gt; tensor([-0.6791,  0.3491, -0.2398,  ..., -0.0007, -0.1333,  0.0161])</span></span>
<span><span class="co">#&gt; Token[4]: "."</span></span>
<span><span class="co">#&gt; tensor([-0.3398,  0.2094,  0.4635,  ...,  0.0005, -0.0177,  0.0032])</span></span></code></pre></div>
<p>Words are now embedded using a concatenation of three different
embeddings. This combination often gives state-of-the-art accuracy.</p>
</div>
<p> </p>
<hr>
</div>
<div class="section level3">
<h3 id="pooled-flair-embeddings">Pooled Flair Embeddings<a class="anchor" aria-label="anchor" href="#pooled-flair-embeddings"></a>
</h3>
<div style="text-align: justify">
<p>We also developed a pooled variant of the
<code>FlairEmbeddings</code>. These embeddings differ in that they
<em>constantly evolve over time</em>, even at prediction time
(i.e. after training is complete). This means that the same words in the
same sentence at two different points in time may have different
embeddings.</p>
<p><code>PooledFlairEmbeddings</code> manage a ‘global’ representation
of each distinct word by using a pooling operation of all past
occurences. More details on how this works may be found in <a href="https://www.aclweb.org/anthology/N19-1078/" class="external-link">Akbik et
al. (2019)</a>.</p>
<p>You can instantiate and use <code>PooledFlairEmbeddings</code> like
any other embedding:</p>
<div class="sourceCode" id="cb46"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># initiate embedding from Flair NLP</span></span>
<span><span class="va">PooledFlairEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">PooledFlairEmbeddings</span></span>
<span><span class="va">flair_embedding_forward</span> <span class="op">&lt;-</span> <span class="fu">PooledFlairEmbeddings</span><span class="op">(</span><span class="st">'news-forward'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># create a sentence object</span></span>
<span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">'The grass is green .'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># embed words in sentence</span></span>
<span><span class="va">flair_embedding_forward</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Sentence[5]: "The grass is green ."</span></span></code></pre></div>
<p>Note that while we get some of our best results with
<code>PooledFlairEmbeddings</code> they are very ineffective memory-wise
since they keep past embeddings of all words in memory. In many cases,
regular <code>FlairEmbeddings</code> will be nearly as good but with
much lower memory requirements.</p>
</div>
<p> </p>
<hr>
</div>
<div class="section level3">
<h3 id="transformer-embeddings">Transformer Embeddings<a class="anchor" aria-label="anchor" href="#transformer-embeddings"></a>
</h3>
<div style="text-align: justify">
<p>Please note that content and examples in this section have been
extensively revised from the <a href="https://github.com/flairNLP/flair/tree/master/resources/docs/embeddings/TRANSFORMER_EMBEDDINGS.md" class="external-link"><code>TransformerWordEmbeddings</code></a>
official documentation. Flair supports various Transformer-based
architectures like BERT or XLNet from <a href="https://github.com/huggingface" class="external-link">HuggingFace</a>, with two classes
<a href="#flair.embeddings.token.TransformerWordEmbeddings"><code>TransformerWordEmbeddings</code></a>
(to embed words or tokens) and <a href="#flair.embeddings.document.TransformerDocumentEmbeddings"><code>TransformerDocumentEmbeddings</code></a>
(to embed documents).</p>
</div>
<p> </p>
</div>
<div class="section level3">
<h3 id="embeddings-words-with-transformers">Embeddings Words with Transformers<a class="anchor" aria-label="anchor" href="#embeddings-words-with-transformers"></a>
</h3>
<div style="text-align: justify">
<p>For instance, to load a standard BERT transformer model, do:</p>
<div class="sourceCode" id="cb47"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://davidycliao.github.io/flaiR">flaiR</a></span><span class="op">)</span></span>
<span><span class="co"># initiate embedding and load BERT model from HugginFaces</span></span>
<span><span class="va">TransformerWordEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">TransformerWordEmbeddings</span></span>
<span><span class="va">embedding</span> <span class="op">&lt;-</span> <span class="fu">TransformerWordEmbeddings</span><span class="op">(</span><span class="st">'bert-base-uncased'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># create a sentence</span></span>
<span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span>
<span><span class="va">sentence</span> <span class="op">=</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">'The grass is green .'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># embed words in sentence</span></span>
<span><span class="va">embedding</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Sentence[5]: "The grass is green ."</span></span></code></pre></div>
<p>If instead you want to use RoBERTa, do:</p>
<div class="sourceCode" id="cb48"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">TransformerWordEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">TransformerWordEmbeddings</span></span>
<span><span class="va">embedding</span> <span class="op">&lt;-</span> <span class="fu">TransformerWordEmbeddings</span><span class="op">(</span><span class="st">'roberta-base'</span><span class="op">)</span></span>
<span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">'The grass is green .'</span><span class="op">)</span></span>
<span><span class="va">embedding</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Sentence[5]: "The grass is green ."</span></span></code></pre></div>
<p>{<code>flaiR</code>} interacts with Flair NLP (<a href="https://github.com/zalandoresearch/" class="external-link">Zalando Research</a>),
allowing you to use pre-trained models from <a href="https://https://huggingface.co/models" class="external-link">HuggingFace</a> , where you
can search for models to use.</p>
<div class="section level3">
<h3 id="embedding-documents-with-transformers">Embedding Documents with Transformers<a class="anchor" aria-label="anchor" href="#embedding-documents-with-transformers"></a>
</h3>
<p>To embed a whole sentence as one (instead of each word in the
sentence), simply use the <a href="#flair.embeddings.document.TransformerDocumentEmbeddings"><code>TransformerDocumentEmbeddings</code></a>
instead:</p>
<div class="sourceCode" id="cb49"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">TransformerDocumentEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">TransformerDocumentEmbeddings</span></span>
<span><span class="va">embedding</span> <span class="op">&lt;-</span> <span class="fu">TransformerDocumentEmbeddings</span><span class="op">(</span><span class="st">'roberta-base'</span><span class="op">)</span></span>
<span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">'The grass is green .'</span><span class="op">)</span></span>
<span><span class="va">embedding</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Sentence[5]: "The grass is green ."</span></span></code></pre></div>
</div>
</div>
<div class="section level4">
<h4 id="arguments">Arguments<a class="anchor" aria-label="anchor" href="#arguments"></a>
</h4>
<div style="text-align: justify">
<p>There are several options that you can set when you init the <a href="#flair.embeddings.token.TransformerWordEmbeddings"><code>TransformerWordEmbeddings</code></a>
and <a href="#flair.embeddings.document.TransformerDocumentEmbeddings"><code>TransformerDocumentEmbeddings</code></a>
classes:</p>
<table class="table">
<colgroup>
<col width="19%">
<col width="17%">
<col width="62%">
</colgroup>
<thead><tr class="header">
<th>Argument</th>
<th>Default</th>
<th>Description</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><code>model</code></td>
<td><code>bert-base-uncased</code></td>
<td>The string identifier of the transformer model you want to use (see
above)</td>
</tr>
<tr class="even">
<td><code>layers</code></td>
<td><code>all</code></td>
<td>Defines the layers of the Transformer-based model that produce the
embedding</td>
</tr>
<tr class="odd">
<td><code>subtoken_pooling</code></td>
<td><code>first</code></td>
<td>See <a href="#Pooling-operation">Pooling operation section</a>.</td>
</tr>
<tr class="even">
<td><code>layer_mean</code></td>
<td><code>True</code></td>
<td>See <a href="#Layer-mean">Layer mean section</a>.</td>
</tr>
<tr class="odd">
<td><code>fine_tune</code></td>
<td><code>False</code></td>
<td>Whether or not embeddings are fine-tuneable.</td>
</tr>
<tr class="even">
<td><code>allow_long_sentences</code></td>
<td><code>True</code></td>
<td>Whether or not texts longer than maximal sequence length are
supported.</td>
</tr>
<tr class="odd">
<td><code>use_context</code></td>
<td><code>False</code></td>
<td>Set to True to include context outside of sentences. This can
greatly increase accuracy on some tasks, but slows down embedding
generation.</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section level4">
<h4 id="layers">Layers<a class="anchor" aria-label="anchor" href="#layers"></a>
</h4>
<div style="text-align: justify">
<p>The <code>layers</code> argument controls which transformer layers
are used for the embedding. If you set this value to ‘-1,-2,-3,-4’, the
top 4 layers are used to make an embedding. If you set it to ‘-1’, only
the last layer is used. If you set it to “all”, then all layers are
used. This affects the length of an embedding, since layers are just
concatenated.</p>
<div class="sourceCode" id="cb50"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span>
<span><span class="va">TransformerWordEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">TransformerWordEmbeddings</span></span>
<span><span class="va">sentence</span> <span class="op">=</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">'The grass is green.'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># use only last layers</span></span>
<span><span class="va">embeddings</span> <span class="op">&lt;-</span> <span class="fu">TransformerWordEmbeddings</span><span class="op">(</span><span class="st">'bert-base-uncased'</span>, layers<span class="op">=</span><span class="st">'-1'</span>, layer_mean <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="va">embeddings</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Sentence[5]: "The grass is green."</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">size</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; torch.Size([768])</span></span>
<span></span>
<span><span class="va">sentence</span><span class="op">$</span><span class="fu">clear_embeddings</span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb51"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">'The grass is green.'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># use only last layers</span></span>
<span><span class="va">embeddings</span> <span class="op">&lt;-</span> <span class="fu">TransformerWordEmbeddings</span><span class="op">(</span><span class="st">'bert-base-uncased'</span>, layers <span class="op">=</span> <span class="st">"-1"</span>, layer_mean <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="va">embeddings</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Sentence[5]: "The grass is green."</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">size</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; torch.Size([768])</span></span>
<span></span>
<span><span class="va">sentence</span><span class="op">$</span><span class="fu">clear_embeddings</span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb52"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># use last two layers</span></span>
<span><span class="va">embeddings</span> <span class="op">&lt;-</span> <span class="fu">TransformerWordEmbeddings</span><span class="op">(</span><span class="st">'bert-base-uncased'</span>, layers<span class="op">=</span><span class="st">'-1,-2'</span>, layer_mean <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="va">embeddings</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Sentence[5]: "The grass is green."</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">size</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; torch.Size([1536])</span></span>
<span></span>
<span><span class="va">sentence</span><span class="op">$</span><span class="fu">clear_embeddings</span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb53"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># use ALL layers</span></span>
<span><span class="va">embeddings</span> <span class="op">=</span> <span class="fu">TransformerWordEmbeddings</span><span class="op">(</span><span class="st">'bert-base-uncased'</span>, layers<span class="op">=</span><span class="st">'all'</span>, layer_mean<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="va">embeddings</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Sentence[5]: "The grass is green."</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">size</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; torch.Size([9984])</span></span></code></pre></div>
<p>Here’s an example of how it might be done:</p>
<p>You can directly import torch from reticulate since it has already
been installed through the flair dependency when you installed flair in
Python.</p>
<div class="sourceCode" id="cb54"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># You can directly import torch from reticulate since it has already been installed through the flair dependency when you installed flair in Python.</span></span>
<span><span class="va">torch</span> <span class="op">&lt;-</span> <span class="fu">reticulate</span><span class="fu">::</span><span class="fu"><a href="https://rstudio.github.io/reticulate/reference/import.html" class="external-link">import</a></span><span class="op">(</span><span class="st">'torch'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Attempting to create a tensor with integer dimensions</span></span>
<span><span class="va">torch</span><span class="op">$</span><span class="fu">Size</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="fl">768L</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; torch.Size([768])</span></span>
<span><span class="va">torch</span><span class="op">$</span><span class="fu">Size</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="fl">1536L</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; torch.Size([1536])</span></span>
<span><span class="va">torch</span><span class="op">$</span><span class="fu">Size</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="fl">9984L</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; torch.Size([9984])</span></span></code></pre></div>
<p>Notice the L after the numbers in the list? This ensures that R
treats the numbers as integers. If you’re generating these numbers
dynamically (e.g., through computation), you might want to ensure they
are integers before attempting to create the tensor. I.e. the size of
the embedding increases the mode layers we use (but ONLY if layer_mean
is set to False, otherwise the length is always the same).</p>
</div>
</div>
<div class="section level4">
<h4 id="pooling-operation">Pooling Operation<a class="anchor" aria-label="anchor" href="#pooling-operation"></a>
</h4>
<div style="text-align: justify">
<p>Most of the Transformer-based models use subword tokenization. E.g.
the following token <code>puppeteer</code> could be tokenized into the
subwords: <code>pupp</code>, <code>##ete</code> and
<code>##er</code>.</p>
<p>We implement different pooling operations for these subwords to
generate the final token representation:</p>
<ul>
<li>
<code>first</code>: only the embedding of the first subword is
used</li>
<li>
<code>last</code>: only the embedding of the last subword is
used</li>
<li>
<code>first_last</code>: embeddings of the first and last subwords
are concatenated and used</li>
<li>
<code>mean</code>: a <code>torch.mean</code> over all subword
embeddings is calculated and used</li>
</ul>
<p>You can choose which one to use by passing this in the
constructor:</p>
<div class="sourceCode" id="cb55"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># use first and last subtoken for each word</span></span>
<span><span class="va">embeddings</span> <span class="op">=</span> <span class="fu">TransformerWordEmbeddings</span><span class="op">(</span><span class="st">'bert-base-uncased'</span>, subtoken_pooling<span class="op">=</span><span class="st">'first_last'</span><span class="op">)</span></span>
<span><span class="va">embeddings</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Sentence[5]: "The grass is green."</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">size</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; torch.Size([9984])</span></span></code></pre></div>
</div>
</div>
<div class="section level4">
<h4 id="layer-mean">Layer Mean<a class="anchor" aria-label="anchor" href="#layer-mean"></a>
</h4>
<div style="text-align: justify">
<p>The Transformer-based models have a certain number of layers. By
default, all layers you select are concatenated as explained above.
Alternatively, you can set <code>layer_mean=True</code> to do a mean
over all selected layers. The resulting vector will then always have the
same dimensionality as a single layer:</p>
<div class="sourceCode" id="cb56"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># initiate embedding from transformer. This model will be downloaded from Flair NLP huggingface.</span></span>
<span><span class="va">embeddings</span> <span class="op">&lt;-</span> <span class="fu">TransformerWordEmbeddings</span><span class="op">(</span><span class="st">'bert-base-uncased'</span>, layers<span class="op">=</span><span class="st">"all"</span>, layer_mean<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># create a sentence object</span></span>
<span><span class="va">sentence</span> <span class="op">=</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">"The Oktoberfest is the world's largest Volksfest ."</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># embed words in sentence</span></span>
<span><span class="va">embedding</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Sentence[9]: "The Oktoberfest is the world's largest Volksfest ."</span></span></code></pre></div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="fine-tuneable-or-not">Fine-tuneable or Not<a class="anchor" aria-label="anchor" href="#fine-tuneable-or-not"></a>
</h3>
<div style="text-align: justify">
<p>Here’s an example of how it might be done: In some setups, you may
wish to fine-tune the transformer embeddings. In this case, set
<code>fine_tune=True</code> in the init method. When fine-tuning, you
should also only use the topmost layer, so best set
<code>layers='-1'</code>.</p>
<div class="sourceCode" id="cb57"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># use first and last subtoken for each word</span></span>
<span><span class="va">TransformerWordEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">TransformerWordEmbeddings</span></span>
<span><span class="va">embeddings</span> <span class="op">&lt;-</span> <span class="fu">TransformerWordEmbeddings</span><span class="op">(</span><span class="st">'bert-base-uncased'</span>, fine_tune<span class="op">=</span><span class="cn">TRUE</span>, layers<span class="op">=</span><span class="st">'-1'</span><span class="op">)</span></span>
<span><span class="va">embeddings</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Sentence[9]: "The Oktoberfest is the world's largest Volksfest ."</span></span></code></pre></div>
<p>This will print a tensor that now has a gradient function and can be
fine-tuned if you use it in a training routine.</p>
<div class="sourceCode" id="cb58"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">)</span></span>
<span><span class="co">#&gt; tensor([-6.5871e-01,  1.0410e-01,  3.4632e-01, -3.3775e-01, -2.1013e-01,</span></span>
<span><span class="co">#&gt;         -1.3036e-02,  5.1998e-01,  1.6574e+00, -5.2521e-02, -4.8632e-02,</span></span>
<span><span class="co">#&gt;         -7.8968e-01, -9.5547e-01, -1.9723e-01,  9.4999e-01, -1.0337e+00,</span></span>
<span><span class="co">#&gt;          8.6668e-02,  9.8104e-02,  5.6511e-02,  3.1075e-02,  2.4157e-01,</span></span>
<span><span class="co">#&gt;         -1.1427e-01, -2.3692e-01, -2.0700e-01,  7.7985e-01,  2.5460e-01,</span></span>
<span><span class="co">#&gt;         -5.0831e-03, -2.4110e-01,  2.2436e-01, -7.3249e-02, -8.1094e-01,</span></span>
<span><span class="co">#&gt;         -1.8778e-01,  2.1219e-01, -5.9514e-01,  6.3129e-02, -4.8880e-01,</span></span>
<span><span class="co">#&gt;         -3.2300e-02, -1.9125e-02, -1.0991e-01, -1.5604e-02,  4.3068e-01,</span></span>
<span><span class="co">#&gt;         -1.7968e-01, -5.4499e-01,  7.0608e-01, -4.0512e-01,  1.7761e-01,</span></span>
<span><span class="co">#&gt;         -8.5820e-01,  2.3438e-02, -1.4981e-01, -9.0368e-01, -2.1097e-01,</span></span>
<span><span class="co">#&gt;         -3.3535e-01,  1.4920e-01, -7.4529e-03,  1.0239e+00, -6.1777e-02,</span></span>
<span><span class="co">#&gt;          3.3913e-01,  8.5811e-02,  6.9401e-01, -7.7483e-02,  3.1484e-01,</span></span>
<span><span class="co">#&gt;         -4.3921e-01,  1.2933e+00,  5.7995e-03, -7.0992e-01,  2.7525e-01,</span></span>
<span><span class="co">#&gt;          8.8792e-01,  2.6303e-03,  1.3640e+00,  5.6885e-01, -2.4904e-01,</span></span>
<span><span class="co">#&gt;         -4.5158e-02, -1.7575e-01, -3.4730e-01,  5.8362e-02, -2.0346e-01,</span></span>
<span><span class="co">#&gt;         -1.2505e+00, -3.0592e-01, -3.6105e-02, -2.4066e-01, -5.1250e-01,</span></span>
<span><span class="co">#&gt;          2.6930e-01,  1.4068e-01,  3.4056e-01,  7.3297e-01,  2.6848e-01,</span></span>
<span><span class="co">#&gt;          2.4303e-01, -9.4885e-01, -9.0367e-01, -1.3184e-01,  6.7348e-01,</span></span>
<span><span class="co">#&gt;         -3.2994e-02,  4.7660e-01, -7.1617e-03, -3.4141e-01,  6.8473e-01,</span></span>
<span><span class="co">#&gt;         -4.4869e-01, -4.9831e-01, -8.0143e-01,  1.4073e+00,  5.3251e-01,</span></span>
<span><span class="co">#&gt;          2.4643e-01, -4.2529e-01,  9.1615e-02,  6.4496e-01,  1.7931e-01,</span></span>
<span><span class="co">#&gt;         -2.1473e-01,  1.5447e-01, -3.2978e-01,  1.0799e-01, -1.9402e+00,</span></span>
<span><span class="co">#&gt;         -5.0380e-01, -2.7636e-01, -1.1227e-01,  1.1576e-01,  2.5885e-01,</span></span>
<span><span class="co">#&gt;         -1.7916e-01,  6.6166e-01, -9.6098e-01, -5.1242e-01, -3.5424e-01,</span></span>
<span><span class="co">#&gt;          2.1383e-01,  6.6456e-01,  2.5498e-01,  3.7250e-01, -1.1821e+00,</span></span>
<span><span class="co">#&gt;         -4.9551e-01, -2.0858e-01,  1.1511e+00, -1.0366e-02, -1.0682e+00,</span></span>
<span><span class="co">#&gt;          3.7277e-01,  6.4048e-01,  2.3308e-01, -9.3824e-01,  9.5015e-02,</span></span>
<span><span class="co">#&gt;          5.7904e-01,  6.3969e-01,  8.2359e-02, -1.4075e-01,  3.0107e-01,</span></span>
<span><span class="co">#&gt;          3.5827e-03, -4.4684e-01, -2.6913e+00, -3.3933e-01,  2.8731e-03,</span></span>
<span><span class="co">#&gt;         -1.3639e-01, -7.1054e-01, -1.1048e+00,  2.2374e-01,  1.1830e-01,</span></span>
<span><span class="co">#&gt;          4.8416e-01, -2.9110e-01, -6.7650e-01,  2.3202e-01, -1.0123e-01,</span></span>
<span><span class="co">#&gt;         -1.9174e-01,  4.9959e-02,  5.2067e-01,  1.3272e+00,  6.8250e-01,</span></span>
<span><span class="co">#&gt;          5.5332e-01, -1.0886e+00,  4.5160e-01, -1.5010e-01, -9.8074e-01,</span></span>
<span><span class="co">#&gt;          8.5110e-02,  1.6498e-01,  6.6032e-01,  1.0815e-02,  1.8952e-01,</span></span>
<span><span class="co">#&gt;         -5.6607e-01, -1.3743e-02,  9.1171e-01,  2.7812e-01,  2.9551e-01,</span></span>
<span><span class="co">#&gt;         -3.5637e-01,  3.2030e-01,  5.6738e-01, -1.5707e-01,  3.5326e-01,</span></span>
<span><span class="co">#&gt;         -4.7747e-01,  7.8646e-01,  1.3765e-01,  2.2440e-01,  4.2422e-01,</span></span>
<span><span class="co">#&gt;         -2.6504e-01,  2.2014e-02, -6.7154e-01, -8.7998e-02,  1.4284e-01,</span></span>
<span><span class="co">#&gt;          4.0983e-01,  1.0933e-02, -1.0704e+00, -1.9350e-01,  6.0051e-01,</span></span>
<span><span class="co">#&gt;          5.0545e-02,  1.1434e-02, -8.0243e-01, -6.6871e-01,  5.3953e-01,</span></span>
<span><span class="co">#&gt;         -5.9856e-01, -1.6915e-01, -3.5307e-01,  4.4568e-01, -7.2761e-01,</span></span>
<span><span class="co">#&gt;          1.1629e+00, -3.1553e-01, -7.9747e-01, -2.0582e-01,  3.7320e-01,</span></span>
<span><span class="co">#&gt;          5.9379e-01, -3.1898e-01, -1.6932e-01, -6.2492e-01,  5.7047e-01,</span></span>
<span><span class="co">#&gt;         -2.9779e-01, -5.9106e-01,  8.5436e-02, -2.1839e-01, -2.2214e-01,</span></span>
<span><span class="co">#&gt;          7.9233e-01,  8.0537e-01, -5.9785e-01,  4.0474e-01,  3.9266e-01,</span></span>
<span><span class="co">#&gt;          5.8169e-01, -5.2506e-01,  6.9786e-01,  1.1163e-01,  8.7435e-02,</span></span>
<span><span class="co">#&gt;          1.7549e-01,  9.1438e-02,  5.8816e-01,  6.4338e-01, -2.7138e-01,</span></span>
<span><span class="co">#&gt;         -5.3449e-01, -1.0168e+00, -5.1338e-02,  3.0099e-01, -7.6696e-02,</span></span>
<span><span class="co">#&gt;         -2.1126e-01,  5.8143e-01,  1.3599e-01,  6.2759e-01, -6.2810e-01,</span></span>
<span><span class="co">#&gt;          5.9966e-01,  3.5836e-01, -3.0706e-02,  1.5563e-01, -1.4016e-01,</span></span>
<span><span class="co">#&gt;         -2.0155e-01, -1.3755e+00, -9.1876e-02, -6.9892e-01,  7.9438e-02,</span></span>
<span><span class="co">#&gt;         -4.2926e-01,  3.7988e-01,  7.6741e-01,  5.3094e-01,  8.5981e-01,</span></span>
<span><span class="co">#&gt;          4.4185e-02, -6.3507e-01,  3.9587e-01, -3.6635e-01, -7.0770e-01,</span></span>
<span><span class="co">#&gt;          8.3675e-04, -3.0055e-01,  2.1360e-01, -4.1649e-01,  6.9457e-01,</span></span>
<span><span class="co">#&gt;         -6.2715e-01, -5.1101e-01,  3.0331e-01, -2.3804e+00, -1.0566e-02,</span></span>
<span><span class="co">#&gt;         -9.4488e-01,  4.3317e-02,  2.4188e-01,  1.9204e-02,  1.5718e-03,</span></span>
<span><span class="co">#&gt;         -3.0374e-01,  3.1933e-01, -7.4432e-01,  1.4599e-01, -5.2102e-01,</span></span>
<span><span class="co">#&gt;         -5.2269e-01,  1.3274e-01, -2.8936e-01,  4.1706e-02,  2.6143e-01,</span></span>
<span><span class="co">#&gt;         -4.4796e-01,  7.3136e-01,  6.3893e-02,  4.7398e-01, -5.1062e-01,</span></span>
<span><span class="co">#&gt;         -1.3705e-01,  2.0763e-01, -3.9115e-01,  2.8822e-01, -3.5283e-01,</span></span>
<span><span class="co">#&gt;          3.4881e-02, -3.3602e-01,  1.7210e-01,  1.3537e-02, -5.3036e-01,</span></span>
<span><span class="co">#&gt;          1.2847e-01, -4.5576e-01, -3.7251e-01, -3.2254e+00, -3.1650e-01,</span></span>
<span><span class="co">#&gt;         -2.6144e-01, -9.4983e-02,  2.7651e-02, -2.3750e-01,  3.1001e-01,</span></span>
<span><span class="co">#&gt;          1.1428e-01, -1.2870e-01, -4.7496e-01,  4.4594e-01, -3.6138e-01,</span></span>
<span><span class="co">#&gt;         -3.1009e-01, -9.9613e-02,  5.3968e-01,  1.2840e-02,  1.4507e-01,</span></span>
<span><span class="co">#&gt;         -2.5181e-01,  1.9310e-01,  4.1073e-01,  5.9776e-01, -2.5585e-01,</span></span>
<span><span class="co">#&gt;          5.7184e-02, -5.1505e-01, -6.8708e-02,  4.7767e-01, -1.2078e-01,</span></span>
<span><span class="co">#&gt;         -5.0894e-01, -9.2884e-01,  7.8471e-01,  2.0216e-01,  4.3243e-01,</span></span>
<span><span class="co">#&gt;          3.2803e-01, -1.0122e-01,  3.3529e-01, -1.2183e-01, -5.5060e-01,</span></span>
<span><span class="co">#&gt;          3.5427e-01,  7.4558e-02, -3.1411e-01, -1.7512e-01,  2.2485e-01,</span></span>
<span><span class="co">#&gt;          4.2295e-01,  7.7110e-02,  1.8063e+00,  7.6634e-03, -1.1083e-02,</span></span>
<span><span class="co">#&gt;         -2.8603e-02,  7.7143e-02,  8.2345e-02,  8.0272e-02, -1.1858e+00,</span></span>
<span><span class="co">#&gt;          2.0523e-01,  3.4053e-01,  2.0424e-01, -2.0574e-02,  3.0466e-01,</span></span>
<span><span class="co">#&gt;         -2.1858e-01,  6.3737e-01, -5.6264e-01,  1.4153e-01,  2.4319e-01,</span></span>
<span><span class="co">#&gt;         -5.6688e-01,  7.2375e-02, -2.9329e-01,  4.6561e-02,  1.8977e-01,</span></span>
<span><span class="co">#&gt;          2.4977e-01,  9.1892e-01,  1.1346e-01,  3.8588e-01, -3.5543e-01,</span></span>
<span><span class="co">#&gt;         -1.3380e+00, -8.5644e-01, -5.5443e-01, -7.2317e-01, -2.9225e-01,</span></span>
<span><span class="co">#&gt;         -1.4389e-01,  6.9715e-01, -5.9852e-01, -6.8932e-01, -6.0952e-01,</span></span>
<span><span class="co">#&gt;          1.8234e-01, -7.5841e-02,  3.6445e-01, -3.8286e-01,  2.6545e-01,</span></span>
<span><span class="co">#&gt;         -2.6569e-01, -4.9999e-01, -3.8354e-01, -2.2809e-01,  8.8314e-01,</span></span>
<span><span class="co">#&gt;          2.9041e-01,  5.4803e-01, -1.0668e+00,  4.7406e-01,  7.8804e-02,</span></span>
<span><span class="co">#&gt;         -1.1559e+00, -3.0649e-01,  6.0479e-02, -7.1279e-01, -4.3335e-01,</span></span>
<span><span class="co">#&gt;         -8.2428e-04, -1.0236e-01,  3.5497e-01,  1.8665e-01,  1.2045e-01,</span></span>
<span><span class="co">#&gt;          1.2071e-01,  6.2911e-01,  3.1421e-01, -2.1635e-01, -8.9416e-01,</span></span>
<span><span class="co">#&gt;          6.6361e-01, -9.2981e-01,  6.9193e-01, -2.5403e-01, -2.5835e-02,</span></span>
<span><span class="co">#&gt;          1.2342e+00, -6.5908e-01,  7.5741e-01,  2.9014e-01,  3.0760e-01,</span></span>
<span><span class="co">#&gt;         -1.0249e+00, -2.7089e-01,  4.6132e-01,  6.1510e-02,  2.5385e-01,</span></span>
<span><span class="co">#&gt;         -5.2075e-01, -3.5107e-01,  3.3694e-01, -2.5047e-01, -2.7855e-01,</span></span>
<span><span class="co">#&gt;          2.0280e-01, -1.5703e-01,  4.1619e-02,  1.4451e-01, -1.6666e-01,</span></span>
<span><span class="co">#&gt;         -3.0519e-01, -9.4271e-02, -1.7083e-01,  5.2454e-01,  2.4524e-01,</span></span>
<span><span class="co">#&gt;          2.0731e-01,  3.7948e-01,  9.7358e-02, -3.2452e-02,  5.5792e-01,</span></span>
<span><span class="co">#&gt;         -2.4703e-01,  5.2864e-01,  5.6343e-01, -1.9198e-01, -8.3369e-02,</span></span>
<span><span class="co">#&gt;         -6.5377e-01, -5.4104e-01,  1.8289e-01, -4.9146e-01,  6.6422e-01,</span></span>
<span><span class="co">#&gt;         -5.2808e-01, -1.4797e-01, -4.5527e-02, -3.9593e-01,  1.2841e-01,</span></span>
<span><span class="co">#&gt;         -7.8591e-01, -3.7563e-02,  6.1912e-01,  3.2458e-01,  3.7858e-01,</span></span>
<span><span class="co">#&gt;          1.8744e-01, -5.0738e-01,  8.0223e-02, -3.1468e-02, -1.5145e-01,</span></span>
<span><span class="co">#&gt;          1.6657e-01, -5.2251e-01, -2.5940e-01, -3.8505e-01, -7.4941e-02,</span></span>
<span><span class="co">#&gt;          3.9530e-01, -2.1742e-01, -1.7113e-01, -5.2492e-01, -7.7780e-02,</span></span>
<span><span class="co">#&gt;         -6.9759e-01,  2.2570e-01, -1.2935e-01,  3.0749e-01, -1.3554e-01,</span></span>
<span><span class="co">#&gt;          6.0182e-02, -1.1479e-01,  4.7263e-01,  3.7957e-01,  8.9523e-01,</span></span>
<span><span class="co">#&gt;         -3.6411e-01, -6.6355e-01, -7.6647e-01, -1.4479e+00, -5.2238e-01,</span></span>
<span><span class="co">#&gt;          2.3336e-02, -4.5736e-01,  5.9981e-01,  6.8699e-01,  4.2190e-02,</span></span>
<span><span class="co">#&gt;          1.5894e-01,  2.0743e-02,  9.2333e-02, -7.2747e-01,  1.2388e-01,</span></span>
<span><span class="co">#&gt;         -4.7257e-01, -2.9889e-01,  4.8955e-01, -9.1618e-01, -1.9497e-01,</span></span>
<span><span class="co">#&gt;         -1.4157e-01, -1.7472e-01,  4.9251e-02, -2.2263e-01,  6.1700e-01,</span></span>
<span><span class="co">#&gt;         -2.4691e-01,  6.0936e-01,  3.6134e-01,  4.3398e-01, -2.7615e-01,</span></span>
<span><span class="co">#&gt;         -2.6582e-01, -1.3132e-01, -4.4156e-02,  5.3686e-01,  1.2956e-01,</span></span>
<span><span class="co">#&gt;         -6.4218e-01, -1.5820e-01, -1.0249e+00, -9.3591e-03, -3.5060e-01,</span></span>
<span><span class="co">#&gt;          3.6650e-01,  4.9503e-01,  7.4325e-01,  9.6526e-02,  4.3141e-01,</span></span>
<span><span class="co">#&gt;          3.9511e-02, -7.0727e-02,  6.2696e-01,  1.3066e-01,  1.0243e-01,</span></span>
<span><span class="co">#&gt;          3.3839e-01,  1.9224e-01,  4.8800e-01, -2.1052e-01,  3.9523e-02,</span></span>
<span><span class="co">#&gt;          7.7568e-01, -1.2005e-01, -1.1262e-01,  8.7001e-02,  2.7273e-01,</span></span>
<span><span class="co">#&gt;         -4.6830e-02, -2.4966e-01, -3.2083e-01, -2.6389e-01,  1.6225e-01,</span></span>
<span><span class="co">#&gt;          2.8800e-01, -1.0799e-01, -1.0841e-01,  6.6873e-01,  3.4369e-01,</span></span>
<span><span class="co">#&gt;          5.8675e-01,  9.2084e-01, -1.8131e-01,  5.6371e-02, -5.7125e-01,</span></span>
<span><span class="co">#&gt;          3.1048e-01,  3.1629e-02,  1.2097e+00,  4.4492e-01, -2.3792e-01,</span></span>
<span><span class="co">#&gt;         -9.9342e-02, -5.0657e-01, -3.1333e-02,  1.5045e-01,  3.1493e-01,</span></span>
<span><span class="co">#&gt;         -4.1287e-01, -1.8618e-01, -4.2639e-02,  1.8266e+00,  4.8565e-01,</span></span>
<span><span class="co">#&gt;          6.3892e-01, -2.9107e-01, -3.2557e-01,  1.1088e-01, -1.3213e+00,</span></span>
<span><span class="co">#&gt;          7.1113e-01,  2.3618e-01,  2.1473e-01,  1.6360e-01, -5.2535e-01,</span></span>
<span><span class="co">#&gt;          3.4322e-01,  9.0777e-01,  1.8697e-01, -3.0531e-01,  2.7574e-01,</span></span>
<span><span class="co">#&gt;          5.1452e-01, -2.6733e-01,  2.4208e-01, -3.3234e-01,  6.3520e-01,</span></span>
<span><span class="co">#&gt;          2.5884e-01, -5.7923e-01,  3.0204e-01,  4.1746e-02,  4.7538e-02,</span></span>
<span><span class="co">#&gt;         -6.7038e-01,  4.6699e-01, -1.6951e-01, -1.5161e-01, -1.2805e-01,</span></span>
<span><span class="co">#&gt;         -4.3990e-01,  1.0177e+00, -3.8138e-01,  4.3114e-01, -7.5444e-03,</span></span>
<span><span class="co">#&gt;          2.7385e-01,  4.6314e-01, -8.6565e-02, -7.9458e-01,  1.4370e-02,</span></span>
<span><span class="co">#&gt;          2.6016e-01,  9.2574e-03,  9.3968e-01,  7.9679e-01,  3.3147e-03,</span></span>
<span><span class="co">#&gt;         -5.6733e-01,  2.9052e-01, -9.5894e-02,  1.8630e-01,  1.4475e-01,</span></span>
<span><span class="co">#&gt;          1.8935e-01,  5.1735e-01, -1.2187e+00, -1.3298e-01, -4.3538e-01,</span></span>
<span><span class="co">#&gt;         -6.5398e-01, -2.9286e-01,  1.3199e-01,  3.9075e-01,  9.0172e-01,</span></span>
<span><span class="co">#&gt;          9.9439e-01,  6.2783e-01, -1.6103e-01,  1.4153e-03, -9.1476e-01,</span></span>
<span><span class="co">#&gt;          7.7760e-01,  1.2264e+00,  8.1482e-02,  6.6732e-01, -7.4576e-01,</span></span>
<span><span class="co">#&gt;         -1.0470e-01, -6.7781e-01,  8.0405e-01,  3.6676e-02,  3.6362e-01,</span></span>
<span><span class="co">#&gt;          4.4962e-01,  8.9600e-01, -1.8275e+00,  6.7828e-01, -9.4118e-03,</span></span>
<span><span class="co">#&gt;          3.8665e-01, -2.2149e-02,  7.4756e-02,  3.7438e-01, -1.2696e-01,</span></span>
<span><span class="co">#&gt;         -5.3397e-01, -3.5782e-01,  3.0400e-01,  7.7663e-01, -1.9122e-01,</span></span>
<span><span class="co">#&gt;         -1.3041e-01, -2.1522e-01,  1.1086e+00,  1.0237e+00, -4.7553e-02,</span></span>
<span><span class="co">#&gt;         -3.9538e-01,  1.1568e+00, -4.2549e-01, -2.5641e-02,  2.1993e-01,</span></span>
<span><span class="co">#&gt;         -4.7488e-01, -7.7624e-02, -5.5211e-01, -5.3169e-01, -5.3790e-02,</span></span>
<span><span class="co">#&gt;         -6.0536e-01,  4.2789e-01, -3.8606e-01,  9.8630e-01,  4.3331e-01,</span></span>
<span><span class="co">#&gt;          4.8414e-01, -1.3519e-01, -6.5505e-01, -2.2913e-01, -3.1254e-01,</span></span>
<span><span class="co">#&gt;          1.2920e-01, -7.7761e-02, -3.1123e-01,  8.2576e-01,  8.6486e-01,</span></span>
<span><span class="co">#&gt;         -3.4766e-01, -3.8491e-01,  3.5732e-02,  3.7518e-01, -3.7511e-01,</span></span>
<span><span class="co">#&gt;          5.2371e-01, -7.9721e-01,  3.3401e-01,  8.3976e-01, -3.2525e-01,</span></span>
<span><span class="co">#&gt;         -3.0268e-01, -1.3558e-01,  2.2812e-01,  1.5632e-01,  3.1584e-01,</span></span>
<span><span class="co">#&gt;          9.3903e-02, -3.8647e-01, -1.0177e-01, -2.8833e-01,  3.6028e-01,</span></span>
<span><span class="co">#&gt;          2.2565e-01, -1.5595e-01, -4.4974e-01, -5.0904e-01,  4.5058e-01,</span></span>
<span><span class="co">#&gt;          7.9031e-01,  2.7041e-01, -3.6712e-01, -3.9090e-01,  2.3358e-01,</span></span>
<span><span class="co">#&gt;          1.2162e+00, -1.1371e+00, -8.2702e-01, -9.2749e-02,  5.8958e-01,</span></span>
<span><span class="co">#&gt;          4.4429e-02, -2.3344e-01, -5.6492e-01,  4.9407e-01, -4.0301e-01,</span></span>
<span><span class="co">#&gt;          5.0950e-01, -1.6741e-01, -4.0176e+00, -8.2092e-01, -3.9132e-01,</span></span>
<span><span class="co">#&gt;         -2.9754e-01, -2.6798e-01, -2.5174e-01,  6.6283e-01, -5.7531e-02,</span></span>
<span><span class="co">#&gt;          7.7359e-01,  2.5238e-01,  2.5732e-02,  1.7694e-01,  9.4648e-02,</span></span>
<span><span class="co">#&gt;          2.6886e-01,  9.3711e-01, -8.3930e-02])</span></span></code></pre></div>
<!-- ```python -->
<!-- tensor([-0.0323, -0.3904, -1.1946,  ...,  0.1305, -0.1365, -0.4323], -->
<!--        device='cuda:0', grad_fn=<CatBackward>) -->
<!-- ``` -->
</div>
<p><strong>More Models</strong></p>
<div style="text-align: justify">
<p>Please have a look at the awesome <a href="https://huggingface.co/models" class="external-link">HuggingFace</a> for all supported
pre-trained models!</p>
</div>
<p> </p>
<hr>
<div class="section level4">
<h4 id="classic-word-embeddings">Classic Word Embeddings<a class="anchor" aria-label="anchor" href="#classic-word-embeddings"></a>
</h4>
<div style="text-align: justify">
<p>Classic word embeddings are static and word-level, meaning that each
distinct word gets exactly one pre-computed embedding. Most embeddings
fall under this class, including the popular GloVe or Komninos
embeddings.</p>
<p>Simply instantiate the <code>WordEmbeddings</code> class and pass a
string identifier of the embedding you wish to load. So, if you want to
use GloVe embeddings, pass the string ‘glove’ to the constructor:</p>
<div class="sourceCode" id="cb59"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://davidycliao.github.io/flaiR">flaiR</a></span><span class="op">)</span></span>
<span><span class="co"># initiate embedding with glove</span></span>
<span><span class="va">WordEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">WordEmbeddings</span></span>
<span><span class="va">glove_embedding</span> <span class="op">&lt;-</span>  <span class="fu">WordEmbeddings</span><span class="op">(</span><span class="st">'glove'</span><span class="op">)</span></span></code></pre></div>
<p>Now, create an example sentence and call the embedding’s
<code><a href="https://rdrr.io/r/stats/embed.html" class="external-link">embed()</a></code> method. You can also pass a list of sentences to
this method since some embedding types make use of batching to increase
speed.</p>
<div class="sourceCode" id="cb60"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://davidycliao.github.io/flaiR">flaiR</a></span><span class="op">)</span></span>
<span><span class="co"># initiate a sentence object</span></span>
<span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span>
<span></span>
<span><span class="co"># create sentence object.</span></span>
<span><span class="va">sentence</span> <span class="op">=</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">'The grass is green .'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># embed a sentence using glove.</span></span>
<span><span class="va">glove_embedding</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Sentence[5]: "The grass is green ."</span></span></code></pre></div>
<p>This prints out the tokens and their embeddings. GloVe embeddings are
Pytorch vectors of dimensionality 100.</p>
<div class="sourceCode" id="cb61"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># view embedded tokens.</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">token</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq_along</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">[</span><span class="va">token</span><span class="op">]</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">[</span><span class="va">token</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">numpy</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="co">#&gt; Token[0]: "The"</span></span>
<span><span class="co">#&gt;   [1] -0.038194 -0.244870  0.728120 -0.399610  0.083172  0.043953</span></span>
<span><span class="co">#&gt;   [7] -0.391410  0.334400 -0.575450  0.087459  0.287870 -0.067310</span></span>
<span><span class="co">#&gt;  [13]  0.309060 -0.263840 -0.132310 -0.207570  0.333950 -0.338480</span></span>
<span><span class="co">#&gt;  [19] -0.317430 -0.483360  0.146400 -0.373040  0.345770  0.052041</span></span>
<span><span class="co">#&gt;  [25]  0.449460 -0.469710  0.026280 -0.541550 -0.155180 -0.141070</span></span>
<span><span class="co">#&gt;  [31] -0.039722  0.282770  0.143930  0.234640 -0.310210  0.086173</span></span>
<span><span class="co">#&gt;  [37]  0.203970  0.526240  0.171640 -0.082378 -0.717870 -0.415310</span></span>
<span><span class="co">#&gt;  [43]  0.203350 -0.127630  0.413670  0.551870  0.579080 -0.334770</span></span>
<span><span class="co">#&gt;  [49] -0.365590 -0.548570 -0.062892  0.265840  0.302050  0.997750</span></span>
<span><span class="co">#&gt;  [55] -0.804810 -3.024300  0.012540 -0.369420  2.216700  0.722010</span></span>
<span><span class="co">#&gt;  [61] -0.249780  0.921360  0.034514  0.467450  1.107900 -0.193580</span></span>
<span><span class="co">#&gt;  [67] -0.074575  0.233530 -0.052062 -0.220440  0.057162 -0.158060</span></span>
<span><span class="co">#&gt;  [73] -0.307980 -0.416250  0.379720  0.150060 -0.532120 -0.205500</span></span>
<span><span class="co">#&gt;  [79] -1.252600  0.071624  0.705650  0.497440 -0.420630  0.261480</span></span>
<span><span class="co">#&gt;  [85] -1.538000 -0.302230 -0.073438 -0.283120  0.371040 -0.252170</span></span>
<span><span class="co">#&gt;  [91]  0.016215 -0.017099 -0.389840  0.874240 -0.725690 -0.510580</span></span>
<span><span class="co">#&gt;  [97] -0.520280 -0.145900  0.827800  0.270620</span></span>
<span><span class="co">#&gt; Token[1]: "grass"</span></span>
<span><span class="co">#&gt;   [1] -0.8135300  0.9404200 -0.2404800 -0.1350100  0.0556780  0.3362500</span></span>
<span><span class="co">#&gt;   [7]  0.0802090 -0.1014800 -0.5477600 -0.3536500  0.0733820  0.2586800</span></span>
<span><span class="co">#&gt;  [13]  0.1986600 -0.1432800  0.2507000  0.4281400  0.1949800  0.5345600</span></span>
<span><span class="co">#&gt;  [19]  0.7424100  0.0578160 -0.3178100  0.9435900  0.8145000 -0.0823750</span></span>
<span><span class="co">#&gt;  [25]  0.6165800  0.7284400 -0.3262300 -1.3641000  0.1232000  0.5372800</span></span>
<span><span class="co">#&gt;  [31] -0.5122800  0.0245900  1.0822001 -0.2295900  0.6038500  0.5541500</span></span>
<span><span class="co">#&gt;  [37] -0.9609900  0.4803300  0.0022260  0.5591300 -0.1636500 -0.8468100</span></span>
<span><span class="co">#&gt;  [43]  0.0740790 -0.6215700  0.0259670 -0.5162100 -0.0524620 -0.1417700</span></span>
<span><span class="co">#&gt;  [49] -0.0161230 -0.4971900 -0.5534500 -0.4037100  0.5095600  1.0276000</span></span>
<span><span class="co">#&gt;  [55] -0.0840000 -1.1179000  0.3225700  0.4928100  0.9487600  0.2040300</span></span>
<span><span class="co">#&gt;  [61]  0.5388300  0.8397200 -0.0688830  0.3136100  1.0450000 -0.2266900</span></span>
<span><span class="co">#&gt;  [67] -0.0896010 -0.6427100  0.6442900 -1.1001000 -0.0095814  0.2668200</span></span>
<span><span class="co">#&gt;  [73] -0.3230200 -0.6065200  0.0479150 -0.1663700  0.8571200  0.2335500</span></span>
<span><span class="co">#&gt;  [79]  0.2539500  1.2546000  0.5471600 -0.1979600 -0.7186300  0.2076000</span></span>
<span><span class="co">#&gt;  [85] -0.2587500 -0.3649900  0.0834360  0.6931700  0.1573700  1.0931000</span></span>
<span><span class="co">#&gt;  [91]  0.0912950 -1.3773000 -0.2717000  0.7070800  0.1872000 -0.3307200</span></span>
<span><span class="co">#&gt;  [97] -0.2835900  0.1029600  1.2228000  0.8374100</span></span>
<span><span class="co">#&gt; Token[2]: "is"</span></span>
<span><span class="co">#&gt;   [1] -0.5426400  0.4147600  1.0322000 -0.4024400  0.4669100  0.2181600</span></span>
<span><span class="co">#&gt;   [7] -0.0748640  0.4733200  0.0809960 -0.2207900 -0.1280800 -0.1144000</span></span>
<span><span class="co">#&gt;  [13]  0.5089100  0.1156800  0.0282110 -0.3628000  0.4382300  0.0475110</span></span>
<span><span class="co">#&gt;  [19]  0.2028200  0.4985700 -0.1006800  0.1326900  0.1697200  0.1165300</span></span>
<span><span class="co">#&gt;  [25]  0.3135500  0.2571300  0.0927830 -0.5682600 -0.5297500 -0.0514560</span></span>
<span><span class="co">#&gt;  [31] -0.6732600  0.9253300  0.2693000  0.2273400  0.6636500  0.2622100</span></span>
<span><span class="co">#&gt;  [37]  0.1971900  0.2609000  0.1877400 -0.3454000 -0.4263500  0.1397500</span></span>
<span><span class="co">#&gt;  [43]  0.5633800 -0.5690700  0.1239800 -0.1289400  0.7248400 -0.2610500</span></span>
<span><span class="co">#&gt;  [49] -0.2631400 -0.4360500  0.0789080 -0.8414600  0.5159500  1.3997000</span></span>
<span><span class="co">#&gt;  [55] -0.7646000 -3.1452999 -0.2920200 -0.3124700  1.5129000  0.5243500</span></span>
<span><span class="co">#&gt;  [61]  0.2145600  0.4245200 -0.0884110 -0.1780500  1.1876000  0.1057900</span></span>
<span><span class="co">#&gt;  [67]  0.7657100  0.2191400  0.3582400 -0.1163600  0.0932610 -0.6248300</span></span>
<span><span class="co">#&gt;  [73] -0.2189800  0.2179600  0.7405600 -0.4373500  0.1434300  0.1471900</span></span>
<span><span class="co">#&gt;  [79] -1.1605000 -0.0505080  0.1267700 -0.0143950 -0.9867600 -0.0912970</span></span>
<span><span class="co">#&gt;  [85] -1.2054000 -0.1197400  0.0478470 -0.5400100  0.5245700 -0.7096300</span></span>
<span><span class="co">#&gt;  [91] -0.3252800 -0.1346000 -0.4131400  0.3343500 -0.0072412  0.3225300</span></span>
<span><span class="co">#&gt;  [97] -0.0442190 -1.2969000  0.7621700  0.4634900</span></span>
<span><span class="co">#&gt; Token[3]: "green"</span></span>
<span><span class="co">#&gt;   [1] -0.67907000  0.34908000 -0.23984000 -0.99651998  0.73782003</span></span>
<span><span class="co">#&gt;   [6] -0.00065911  0.28009999  0.01728700 -0.36063001  0.03695500</span></span>
<span><span class="co">#&gt;  [11] -0.40395001  0.02409200  0.28957999  0.40496999  0.69992000</span></span>
<span><span class="co">#&gt;  [16]  0.25268999  0.80350000  0.04937000  0.15561999 -0.00632860</span></span>
<span><span class="co">#&gt;  [21] -0.29414001  0.14727999  0.18977000 -0.51791000  0.36985999</span></span>
<span><span class="co">#&gt;  [26]  0.74581999  0.08268900 -0.72601002 -0.40939000 -0.09782200</span></span>
<span><span class="co">#&gt;  [31] -0.14095999  0.71121001  0.61932999 -0.25014001  0.42250001</span></span>
<span><span class="co">#&gt;  [36]  0.48458001 -0.51915002  0.77125001  0.36684999  0.49652001</span></span>
<span><span class="co">#&gt;  [41] -0.04129800 -1.46829998  0.20038000  0.18591000  0.04986000</span></span>
<span><span class="co">#&gt;  [46] -0.17523000 -0.35528001  0.94152999 -0.11898000 -0.51902997</span></span>
<span><span class="co">#&gt;  [51] -0.01188700 -0.39186001 -0.17478999  0.93450999 -0.58930999</span></span>
<span><span class="co">#&gt;  [56] -2.77010012  0.34522000  0.86532998  1.08080006 -0.10291000</span></span>
<span><span class="co">#&gt;  [61] -0.09122000  0.55092001 -0.39473000  0.53675997  1.03830004</span></span>
<span><span class="co">#&gt;  [66] -0.40658000  0.24590001 -0.26797000 -0.26036000 -0.14150999</span></span>
<span><span class="co">#&gt;  [71] -0.12022000  0.16234000 -0.74320000 -0.64727998  0.04713300</span></span>
<span><span class="co">#&gt;  [76]  0.51642001  0.19898000  0.23919000  0.12549999  0.22471000</span></span>
<span><span class="co">#&gt;  [81]  0.82612997  0.07832800 -0.57020003  0.02393400 -0.15410000</span></span>
<span><span class="co">#&gt;  [86] -0.25738999  0.41262001 -0.46967000  0.87914002  0.72628999</span></span>
<span><span class="co">#&gt;  [91]  0.05386200 -1.15750003 -0.47835001  0.20139000 -1.00510001</span></span>
<span><span class="co">#&gt;  [96]  0.11515000 -0.96609002  0.12960000  0.18388000 -0.03038300</span></span>
<span><span class="co">#&gt; Token[4]: "."</span></span>
<span><span class="co">#&gt;   [1] -0.3397900  0.2094100  0.4634800 -0.6479200 -0.3837700  0.0380340</span></span>
<span><span class="co">#&gt;   [7]  0.1712700  0.1597800  0.4661900 -0.0191690  0.4147900 -0.3434900</span></span>
<span><span class="co">#&gt;  [13]  0.2687200  0.0446400  0.4213100 -0.4103200  0.1545900  0.0222390</span></span>
<span><span class="co">#&gt;  [19] -0.6465300  0.2525600  0.0431360 -0.1944500  0.4651600  0.4565100</span></span>
<span><span class="co">#&gt;  [25]  0.6858800  0.0912950  0.2187500 -0.7035100  0.1678500 -0.3507900</span></span>
<span><span class="co">#&gt;  [31] -0.1263400  0.6638400 -0.2582000  0.0365420 -0.1360500  0.4025300</span></span>
<span><span class="co">#&gt;  [37]  0.1428900  0.3813200 -0.1228300 -0.4588600 -0.2528200 -0.3043200</span></span>
<span><span class="co">#&gt;  [43] -0.1121500 -0.2618200 -0.2248200 -0.4455400  0.2991000 -0.8561200</span></span>
<span><span class="co">#&gt;  [49] -0.1450300 -0.4908600  0.0082973 -0.1749100  0.2752400  1.4401000</span></span>
<span><span class="co">#&gt;  [55] -0.2123900 -2.8434999 -0.2795800 -0.4572200  1.6386000  0.7880800</span></span>
<span><span class="co">#&gt;  [61] -0.5526200  0.6500000  0.0864260  0.3901200  1.0632000 -0.3537900</span></span>
<span><span class="co">#&gt;  [67]  0.4832800  0.3460000  0.8417400  0.0987070 -0.2421300 -0.2705300</span></span>
<span><span class="co">#&gt;  [73]  0.0452870 -0.4014700  0.1139500  0.0062226  0.0366730  0.0185180</span></span>
<span><span class="co">#&gt;  [79] -1.0213000 -0.2080600  0.6407200 -0.0687630 -0.5863500  0.3347600</span></span>
<span><span class="co">#&gt;  [85] -1.1432000 -0.1148000 -0.2509100 -0.4590700 -0.0968190 -0.1794600</span></span>
<span><span class="co">#&gt;  [91] -0.0633510 -0.6741200 -0.0688950  0.5360400 -0.8777300  0.3180200</span></span>
<span><span class="co">#&gt;  [97] -0.3924200 -0.2339400  0.4729800 -0.0288030</span></span></code></pre></div>
<p>You choose which pre-trained embeddings you load by passing the
appropriate id string to the constructor of the
<code>WordEmbeddings</code> class. Typically, you use the
<strong>two-letter language code</strong> to init an embedding, so ‘en’
for English and ‘de’ for German and so on. By default, this will
initialize FastText embeddings trained over Wikipedia. You can also
always use <em>FastText</em> embeddings over Web crawls, by
instantiating with ‘-crawl’. So ‘de-crawl’ to use embeddings trained
over German web crawls.</p>
<p>For English, we provide a few more options, so here you can choose
between instantiating ‘<code>en-glove</code>’, ‘<code>en-extvec</code>’
and so on.</p>
<div class="section level4">
<h4 id="suppored-models">Suppored Models:<a class="anchor" aria-label="anchor" href="#suppored-models"></a>
</h4>
<p>The following embeddings are currently supported:</p>
<table class="table">
<colgroup>
<col width="33%">
<col width="33%">
<col width="33%">
</colgroup>
<thead><tr class="header">
<th>ID</th>
<th>Language</th>
<th>Embedding</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>‘en-glove’ (or ‘glove’)</td>
<td>English</td>
<td>GloVe embeddings</td>
</tr>
<tr class="even">
<td>‘en-extvec’ (or ‘extvec’)</td>
<td>English</td>
<td>Komninos embeddings</td>
</tr>
<tr class="odd">
<td>‘en-crawl’ (or ‘crawl’)</td>
<td>English</td>
<td>FastText embeddings over Web crawls</td>
</tr>
<tr class="even">
<td>‘en-twitter’ (or ‘twitter’)</td>
<td>English</td>
<td>Twitter embeddings</td>
</tr>
<tr class="odd">
<td>‘en-turian’ (or ‘turian’)</td>
<td>English</td>
<td>Turian embeddings (small)</td>
</tr>
<tr class="even">
<td>‘en’ (or ‘en-news’ or ‘news’)</td>
<td>English</td>
<td>FastText embeddings over news and wikipedia data</td>
</tr>
<tr class="odd">
<td>‘de’</td>
<td>German</td>
<td>German FastText embeddings</td>
</tr>
<tr class="even">
<td>‘nl’</td>
<td>Dutch</td>
<td>Dutch FastText embeddings</td>
</tr>
<tr class="odd">
<td>‘fr’</td>
<td>French</td>
<td>French FastText embeddings</td>
</tr>
<tr class="even">
<td>‘it’</td>
<td>Italian</td>
<td>Italian FastText embeddings</td>
</tr>
<tr class="odd">
<td>‘es’</td>
<td>Spanish</td>
<td>Spanish FastText embeddings</td>
</tr>
<tr class="even">
<td>‘pt’</td>
<td>Portuguese</td>
<td>Portuguese FastText embeddings</td>
</tr>
<tr class="odd">
<td>‘ro’</td>
<td>Romanian</td>
<td>Romanian FastText embeddings</td>
</tr>
<tr class="even">
<td>‘ca’</td>
<td>Catalan</td>
<td>Catalan FastText embeddings</td>
</tr>
<tr class="odd">
<td>‘sv’</td>
<td>Swedish</td>
<td>Swedish FastText embeddings</td>
</tr>
<tr class="even">
<td>‘da’</td>
<td>Danish</td>
<td>Danish FastText embeddings</td>
</tr>
<tr class="odd">
<td>‘no’</td>
<td>Norwegian</td>
<td>Norwegian FastText embeddings</td>
</tr>
<tr class="even">
<td>‘fi’</td>
<td>Finnish</td>
<td>Finnish FastText embeddings</td>
</tr>
<tr class="odd">
<td>‘pl’</td>
<td>Polish</td>
<td>Polish FastText embeddings</td>
</tr>
<tr class="even">
<td>‘cz’</td>
<td>Czech</td>
<td>Czech FastText embeddings</td>
</tr>
<tr class="odd">
<td>‘sk’</td>
<td>Slovak</td>
<td>Slovak FastText embeddings</td>
</tr>
<tr class="even">
<td>‘sl’</td>
<td>Slovenian</td>
<td>Slovenian FastText embeddings</td>
</tr>
<tr class="odd">
<td>‘sr’</td>
<td>Serbian</td>
<td>Serbian FastText embeddings</td>
</tr>
<tr class="even">
<td>‘hr’</td>
<td>Croatian</td>
<td>Croatian FastText embeddings</td>
</tr>
<tr class="odd">
<td>‘bg’</td>
<td>Bulgarian</td>
<td>Bulgarian FastText embeddings</td>
</tr>
<tr class="even">
<td>‘ru’</td>
<td>Russian</td>
<td>Russian FastText embeddings</td>
</tr>
<tr class="odd">
<td>‘ar’</td>
<td>Arabic</td>
<td>Arabic FastText embeddings</td>
</tr>
<tr class="even">
<td>‘he’</td>
<td>Hebrew</td>
<td>Hebrew FastText embeddings</td>
</tr>
<tr class="odd">
<td>‘tr’</td>
<td>Turkish</td>
<td>Turkish FastText embeddings</td>
</tr>
<tr class="even">
<td>‘fa’</td>
<td>Persian</td>
<td>Persian FastText embeddings</td>
</tr>
<tr class="odd">
<td>‘ja’</td>
<td>Japanese</td>
<td>Japanese FastText embeddings</td>
</tr>
<tr class="even">
<td>‘ko’</td>
<td>Korean</td>
<td>Korean FastText embeddings</td>
</tr>
<tr class="odd">
<td>‘zh’</td>
<td>Chinese</td>
<td>Chinese FastText embeddings</td>
</tr>
<tr class="even">
<td>‘hi’</td>
<td>Hindi</td>
<td>Hindi FastText embeddings</td>
</tr>
<tr class="odd">
<td>‘id’</td>
<td>Indonesian</td>
<td>Indonesian FastText embeddings</td>
</tr>
<tr class="even">
<td>‘eu’</td>
<td>Basque</td>
<td>Basque FastText embeddings</td>
</tr>
</tbody>
</table>
<p>So, if you want to load German FastText embeddings, instantiate as
follows:</p>
<p>Alternatively, if you want to load German FastText embeddings trained
over crawls, instantiate as follows:</p>
</div>
</div>
<p> </p>
<hr>
</div>
</div>
</div>
<div class="section level2">
<h2 id="embedding-examples">Embedding Examples<a class="anchor" aria-label="anchor" href="#embedding-examples"></a>
</h2>
<div style="text-align: justify;">
<p>Flair is a very popular natural language processing library,
providing a variety of embedding methods for text representation. Flair
Embeddings is a word embedding framework developed by <a href="https://engineering.zalando.com/posts/2018/11/zalando-research-releases-flair.html" class="external-link">Zalando</a>.
It focuses on word-level representation and can capture contextual
information of words, allowing the same word to have different
embeddings in different contexts. Unlike traditional word embeddings
(such as Word2Vec or GloVe), Flair can dynamically generate word
embeddings based on context and has achieved excellent results in
various NLP tasks. Below are some key points about Flair Embeddings:</p>
</div>
<p><strong>Context-Aware</strong></p>
<div style="text-align: justify;">
<p>Flair is a dynamic word embedding technique that can understand the
meaning of words based on context. In contrast, static word embeddings,
such as Word2Vec or GloVe, provide a fixed embedding for each word
without considering its context in a sentence.</p>
<p>Therefore, context-sensitive embedding techniques, such as Flair, can
capture the meaning of words in specific sentences more accurately, thus
enhancing the performance of language models in various tasks.</p>
</div>
<p><strong>Example:</strong></p>
<div style="text-align: justify;">
<p>Consider the following two English sentences:</p>
<ul>
<li>“I am interested in the bank of the river.”</li>
<li>“I need to go to the bank to withdraw money.”</li>
</ul>
<p>Here, the word “bank” has two different meanings. In the first
sentence, it refers to the edge or shore of a river. In the second
sentence, it refers to a financial institution.</p>
<p>For static embeddings, the word “bank” might have an embedding that
lies somewhere between these two meanings because it doesn’t consider
context. But for dynamic embeddings like Flair, “bank” in the first
sentence will have an embedding related to rivers, and in the second
sentence, it will have an embedding related to finance.</p>
</div>
<div class="sourceCode" id="cb62"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Initialize Flair embeddings</span></span>
<span><span class="va">FlairEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">FlairEmbeddings</span></span>
<span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span>
<span><span class="va">flair_embedding_forward</span> <span class="op">&lt;-</span> <span class="fu">FlairEmbeddings</span><span class="op">(</span><span class="st">'news-forward'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Define the two sentences</span></span>
<span><span class="va">sentence1</span> <span class="op">&lt;-</span>  <span class="fu">Sentence</span><span class="op">(</span><span class="st">"I am interested in the bank of the river."</span><span class="op">)</span></span>
<span><span class="va">sentence2</span> <span class="op">&lt;-</span>  <span class="fu">Sentence</span><span class="op">(</span><span class="st">"I need to go to the bank to withdraw money."</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Get the embeddings</span></span>
<span></span>
<span><span class="va">flair_embedding_forward</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence1</span><span class="op">)</span></span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Sentence[10]: "I am interested in the bank of the river."</span></span>
<span><span class="va">flair_embedding_forward</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence2</span><span class="op">)</span></span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Sentence[11]: "I need to go to the bank to withdraw money."</span></span>
<span></span>
<span><span class="co"># Extract the embedding for "bank" from the sentences</span></span>
<span><span class="va">bank_embedding_sentence1</span> <span class="op">=</span> <span class="va">sentence1</span><span class="op">[</span><span class="fl">5</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span>  <span class="co"># "bank" is the seventh word</span></span>
<span><span class="va">bank_embedding_sentence2</span> <span class="op">=</span> <span class="va">sentence2</span><span class="op">[</span><span class="fl">6</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span>  <span class="co"># "bank" is the sixth word</span></span></code></pre></div>
<div style="text-align: justify;">
<p>Same word, similar vector representation, but essentially different.
In this way, you can see how the dynamic embeddings for “bank” in the
two sentences differ based on context. Although we printed the
embeddings here, in reality, they would be high-dimensional vectors, so
you might see a lot of numbers. If you want a more intuitive view of the
differences, you could compute the cosine similarity or other metrics
between the two embeddings.</p>
<p>This is just a simple demonstration. In practice, you can also
combine multiple embedding techniques, such as
<code>WordEmbeddings</code> and <code>FlairEmbeddings</code>, to get
richer word vectors.</p>
</div>
<div class="sourceCode" id="cb63"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va">lsa</span><span class="op">)</span></span>
<span><span class="co">#&gt; Loading required package: SnowballC</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/lsa/man/cosine.html" class="external-link">cosine</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/numeric.html" class="external-link">as.numeric</a></span><span class="op">(</span> <span class="va">bank_embedding_sentence1</span><span class="op">$</span><span class="fu">numpy</span><span class="op">(</span><span class="op">)</span><span class="op">)</span>, </span>
<span>       <span class="fu"><a href="https://rdrr.io/r/base/numeric.html" class="external-link">as.numeric</a></span><span class="op">(</span> <span class="va">bank_embedding_sentence2</span><span class="op">$</span><span class="fu">numpy</span><span class="op">(</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt;           [,1]</span></span>
<span><span class="co">#&gt; [1,] 0.7329552</span></span></code></pre></div>
<p><strong>Character-Based</strong></p>
<div style="text-align: justify;">
<p>Flair uses a character-level language model, meaning it can generate
embeddings for rare words or even misspelled words. This is an important
feature because it allows the model to understand and process words that
have never appeared in the training data. Flair uses a bidirectional
LSTM (Long Short-Term Memory) network that operates at a character
level. This allows it to feed individual characters into the LSTM
instead of words.</p>
</div>
<p><strong>Multilingual Support</strong></p>
<div style="text-align: justify;">
<p>Flair provides various pre-trained character-level language models,
supporting contextual word embeddings for multiple languages. It allows
you to easily combine different word embeddings (e.g., Flair Embeddings,
Word2Vec, GloVe, etc.) to create powerful stacked embeddings.</p>
</div>
<div class="section level3">
<h3 id="classic-wordembeddings">Classic Wordembeddings<a class="anchor" aria-label="anchor" href="#classic-wordembeddings"></a>
</h3>
<div style="text-align: justify;">
<p>In Flair, the simplest form of embeddings that still contains
semantic information about the word are called classic word embeddings.
These embeddings are pre-trained and non-contextual.</p>
<p>Let’s retrieve a few word embeddings and use FastText embeddings with
the following code. To do so, we simply instantiate a WordEmbeddings
class by passing in the ID of the embedding of our choice. Then, we
simply wrap our text into a Sentence object, and call the
<code>embed(sentence)</code> method on our WordEmbeddings class.</p>
</div>
<div class="sourceCode" id="cb64"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">WordEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">WordEmbeddings</span></span>
<span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span>
<span><span class="va">embedding</span> <span class="op">&lt;-</span> <span class="fu">WordEmbeddings</span><span class="op">(</span><span class="st">'crawl'</span><span class="op">)</span></span>
<span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">"one two three one"</span><span class="op">)</span> </span>
<span><span class="va">embedding</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span> </span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Sentence[4]: "one two three one"</span></span>
<span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq_along</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="va">tokens</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="va">tokens</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">)</span>, n <span class="op">=</span><span class="fl">5</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="co">#&gt; tensor([-0.0535, -0.0368, -0.2851, -0.0381, -0.0486,  0.2383])</span></span>
<span><span class="co">#&gt; tensor([ 0.0282, -0.0786, -0.1236,  0.1756, -0.1199,  0.0964])</span></span>
<span><span class="co">#&gt; tensor([-0.0920, -0.0690, -0.1475,  0.2313, -0.0872,  0.0799])</span></span>
<span><span class="co">#&gt; tensor([-0.0535, -0.0368, -0.2851, -0.0381, -0.0486,  0.2383])</span></span></code></pre></div>
<div style="text-align: justify;">
<p>Flair supports a range of classic word embeddings, each offering
unique features and application scopes. Below is an overview, detailing
the ID required to load each embedding and its corresponding
language.</p>
</div>
<table class="table">
<thead><tr class="header">
<th>Embedding Type</th>
<th>ID</th>
<th>Language</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>GloVe</td>
<td>glove</td>
<td>English</td>
</tr>
<tr class="even">
<td>Komninos</td>
<td>extvec</td>
<td>English</td>
</tr>
<tr class="odd">
<td>Twitter</td>
<td>twitter</td>
<td>English</td>
</tr>
<tr class="even">
<td>Turian (small)</td>
<td>turian</td>
<td>English</td>
</tr>
<tr class="odd">
<td>FastText (crawl)</td>
<td>crawl</td>
<td>English</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>ar</td>
<td>Arabic</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>bg</td>
<td>Bulgarian</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>ca</td>
<td>Catalan</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>cz</td>
<td>Czech</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>da</td>
<td>Danish</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>de</td>
<td>German</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>es</td>
<td>Spanish</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>en</td>
<td>English</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>eu</td>
<td>Basque</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>fa</td>
<td>Persian</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>fi</td>
<td>Finnish</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>fr</td>
<td>French</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>he</td>
<td>Hebrew</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>hi</td>
<td>Hindi</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>hr</td>
<td>Croatian</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>id</td>
<td>Indonesian</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>it</td>
<td>Italian</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>ja</td>
<td>Japanese</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>ko</td>
<td>Korean</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>nl</td>
<td>Dutch</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>no</td>
<td>Norwegian</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>pl</td>
<td>Polish</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>pt</td>
<td>Portuguese</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>ro</td>
<td>Romanian</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>ru</td>
<td>Russian</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>si</td>
<td>Slovenian</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>sk</td>
<td>Slovak</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>sr</td>
<td>Serbian</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>sv</td>
<td>Swedish</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>tr</td>
<td>Turkish</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>zh</td>
<td>Chinese</td>
</tr>
</tbody>
</table>
</div>
<div class="section level3">
<h3 id="contexual-embeddings">Contexual Embeddings<a class="anchor" aria-label="anchor" href="#contexual-embeddings"></a>
</h3>
<div style="text-align: justify;">
<p>The idea behind contextual string embeddings is that each word
embedding should be defined by not only its syntactic-semantic meaning
but also the context it appears in. What this means is that each word
will have a different embedding for every context it appears in. Each
pre-trained Flair model offers a <strong>forward</strong> version and a
<strong>backward</strong> version. Let’s assume you are processing a
language that, just like this text, uses the left-to-right script. The
forward version takes into account the context that happens before the
word – on the left-hand side. The backward version works in the opposite
direction. It takes into account the context after the word – on the
right-hand side of the word. If this is true, then two same words that
appear at the beginning of two different sentences should have identical
forward embeddings, because their context is null. Let’s test this
out:</p>
<p>Because we are using a forward model, it only takes into account the
context that occurs before a word. Additionally, since our word has no
context on the left-hand side of its position in the sentence, the two
embeddings are identical, and the code assumes they are identical,
indeed output is <strong>True</strong>.</p>
</div>
<div class="sourceCode" id="cb65"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">FlairEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">FlairEmbeddings</span></span>
<span><span class="va">embedding</span> <span class="op">&lt;-</span> <span class="fu">FlairEmbeddings</span><span class="op">(</span><span class="st">'news-forward'</span><span class="op">)</span></span>
<span><span class="va">s1</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">"nice shirt"</span><span class="op">)</span> </span>
<span><span class="va">s2</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">"nice pants"</span><span class="op">)</span> </span>
<span></span>
<span><span class="va">embedding</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">s1</span><span class="op">)</span> </span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Sentence[2]: "nice shirt"</span></span>
<span><span class="va">embedding</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">s2</span><span class="op">)</span> </span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Sentence[2]: "nice pants"</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">" s1 sentence:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste</a></span><span class="op">(</span><span class="va">s1</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span>, sep <span class="op">=</span> <span class="st">""</span><span class="op">)</span>, <span class="st">"\n"</span>, <span class="st">"s2 sentence:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste</a></span><span class="op">(</span><span class="va">s2</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span>, sep <span class="op">=</span> <span class="st">""</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt;  s1 sentence: Token[0]: "nice" </span></span>
<span><span class="co">#&gt;  s2 sentence: Token[0]: "nice"</span></span></code></pre></div>
<div style="text-align: justify;">
<p>We test whether the sum of the two 2048 embeddings of
<code>nice</code> is equal to 2048. If it is true, it indicates that the
embedding results are consistent, which should theoretically be the
case.</p>
</div>
<div class="sourceCode" id="cb66"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">s1</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">numpy</span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">==</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">s1</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">numpy</span><span class="op">(</span><span class="op">)</span> <span class="op">==</span>  <span class="va">s2</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">numpy</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] TRUE</span></span></code></pre></div>
<p>Now we separately add a few more words, <code>very</code> and
<code>pretty</code>, into two sentence objects.</p>
<div class="sourceCode" id="cb67"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">s1</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">"very nice shirt"</span><span class="op">)</span> </span>
<span><span class="va">s2</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">"pretty nice pants"</span><span class="op">)</span> </span>
<span></span>
<span><span class="va">embedding</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">s1</span><span class="op">)</span> </span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Sentence[3]: "very nice shirt"</span></span>
<span><span class="va">embedding</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">s2</span><span class="op">)</span> </span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Sentence[3]: "pretty nice pants"</span></span></code></pre></div>
<p>The two sets of embeddings are not identical because the words are
different, so it returns <strong>FALSE</strong>.</p>
<div class="sourceCode" id="cb68"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">s1</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">numpy</span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">==</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">s1</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">numpy</span><span class="op">(</span><span class="op">)</span> <span class="op">==</span>  <span class="va">s2</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">numpy</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] FALSE</span></span></code></pre></div>
<div style="text-align: justify;">
<p>The measure of similarity between two vectors in an inner product
space is known as cosine similarity. The formula for calculating cosine
similarity between two vectors, such as vectors A and B, is as
follows:</p>
<p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>n</mi><mi>e</mi><mi>S</mi><mi>i</mi><mi>m</mi><mi>i</mi><mi>l</mi><mi>a</mi><mi>r</mi><mi>i</mi><mi>t</mi><mi>y</mi><mo>=</mo><mfrac><mrow><msub><mo>∑</mo><mi>i</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>A</mi><mi>i</mi></msub><mo>⋅</mo><msub><mi>B</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><msqrt><mrow><msub><mo>∑</mo><mi>i</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mi>A</mi><mi>i</mi><mn>2</mn></msubsup><mo stretchy="true" form="postfix">)</mo></mrow></mrow></msqrt><mo>⋅</mo><msqrt><mrow><msub><mo>∑</mo><mi>i</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mi>B</mi><mi>i</mi><mn>2</mn></msubsup><mo stretchy="true" form="postfix">)</mo></mrow></mrow></msqrt></mrow></mfrac></mrow><annotation encoding="application/x-tex">Cosine Similarity = \frac{\sum_{i} (A_i \cdot B_i)}{\sqrt{\sum_{i} (A_i^2)} \cdot \sqrt{\sum_{i} (B_i^2)}}</annotation></semantics></math></p>
</div>
<div class="sourceCode" id="cb69"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va">lsa</span><span class="op">)</span></span>
<span><span class="va">vector1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html" class="external-link">as.numeric</a></span><span class="op">(</span><span class="va">s1</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">numpy</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">vector2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html" class="external-link">as.numeric</a></span><span class="op">(</span><span class="va">s2</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">numpy</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>We can observe that the similarity between the two words is 0.55.</p>
<div class="sourceCode" id="cb70"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cosine_similarity</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/lsa/man/cosine.html" class="external-link">cosine</a></span><span class="op">(</span><span class="va">vector1</span>, <span class="va">vector2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">cosine_similarity</span><span class="op">)</span></span>
<span><span class="co">#&gt;           [,1]</span></span>
<span><span class="co">#&gt; [1,] 0.5571664</span></span></code></pre></div>
<hr>
</div>
<div class="section level3">
<h3 id="extracting-embeddings-from-bert">Extracting Embeddings from BERT<a class="anchor" aria-label="anchor" href="#extracting-embeddings-from-bert"></a>
</h3>
<div style="text-align: justify;">
<p>First, we utilize the <code>TransformerWordEmbeddings</code> function
to download BERT, and more transformer models can also be found on <a href="https://huggingface.co/flair" class="external-link">Flair NLP’s Hugging Face</a>.</p>
</div>
<div class="sourceCode" id="cb71"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://davidycliao.github.io/flaiR">flaiR</a></span><span class="op">)</span></span>
<span><span class="va">TransformerWordEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="fu">TransformerWordEmbeddings</span><span class="op">(</span><span class="st">"bert-base-uncased"</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb72"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">embedding</span> <span class="op">&lt;-</span> <span class="va">TransformerWordEmbeddings</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span></code></pre></div>
<p>Next, we traverse each token in the sentence and print them.</p>
<div class="sourceCode" id="cb73"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Iterate through each token in the sentence, printing them. </span></span>
<span><span class="co"># Utilize reticulate::py_str(token) to view each token, given that the sentence is a Python object.</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq_along</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="va">tokens</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"Token: "</span>, <span class="fu">reticulate</span><span class="fu">::</span><span class="fu"><a href="https://rstudio.github.io/reticulate/reference/py_str.html" class="external-link">py_str</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="va">tokens</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span><span class="op">)</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span>  <span class="co"># Access the embedding of the token, converting it to an R object, </span></span>
<span>  <span class="co"># and print the first 10 elements of the vector.</span></span>
<span>  <span class="va">token_embedding</span> <span class="op">&lt;-</span> <span class="va">sentence</span><span class="op">$</span><span class="va">tokens</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">token_embedding</span>, <span class="fl">10</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="co">#&gt; Token:  Token[0]: "one" </span></span>
<span><span class="co">#&gt; tensor([-0.0535, -0.0368, -0.2851, -0.0381, -0.0486,  0.2383, -0.1200,  0.2620,</span></span>
<span><span class="co">#&gt;         -0.0575,  0.0228])</span></span>
<span><span class="co">#&gt; Token:  Token[1]: "two" </span></span>
<span><span class="co">#&gt; tensor([ 0.0282, -0.0786, -0.1236,  0.1756, -0.1199,  0.0964, -0.1327,  0.4449,</span></span>
<span><span class="co">#&gt;         -0.0264, -0.1168])</span></span>
<span><span class="co">#&gt; Token:  Token[2]: "three" </span></span>
<span><span class="co">#&gt; tensor([-0.0920, -0.0690, -0.1475,  0.2313, -0.0872,  0.0799, -0.0901,  0.4403,</span></span>
<span><span class="co">#&gt;         -0.0103, -0.1494])</span></span>
<span><span class="co">#&gt; Token:  Token[3]: "one" </span></span>
<span><span class="co">#&gt; tensor([-0.0535, -0.0368, -0.2851, -0.0381, -0.0486,  0.2383, -0.1200,  0.2620,</span></span>
<span><span class="co">#&gt;         -0.0575,  0.0228])</span></span></code></pre></div>
<hr>
</div>
<div class="section level3">
<h3 id="visialized-embeddings">Visialized Embeddings<a class="anchor" aria-label="anchor" href="#visialized-embeddings"></a>
</h3>
<div class="section level4">
<h4 id="word-embeddings-glove">Word Embeddings (GloVe)<a class="anchor" aria-label="anchor" href="#word-embeddings-glove"></a>
</h4>
<ul>
<li><p>GloVe embeddings are Pytorch vectors of dimensionality
100.</p></li>
<li><p>For English, Flair provides a few more options. Here, you can use
<code>en-glove</code> and <code>en-extvec</code> with the
<strong>WordEmbeddings</strong> class.</p></li>
</ul>
<div class="sourceCode" id="cb74"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Initialize Text Processing Tools ---------------------------</span></span>
<span><span class="co"># Import Sentence class for text operations</span></span>
<span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span>
<span></span>
<span><span class="co"># Configure GloVe Embeddings --------------------------------</span></span>
<span><span class="co"># Load WordEmbeddings class and initialize GloVe model</span></span>
<span><span class="va">WordEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">WordEmbeddings</span></span>
<span><span class="va">embedding</span> <span class="op">&lt;-</span> <span class="fu">WordEmbeddings</span><span class="op">(</span><span class="st">"glove"</span><span class="op">)</span> </span>
<span></span>
<span><span class="co"># Text Processing and Embedding -----------------------------</span></span>
<span><span class="co"># Create sentence with semantic relationship pairs</span></span>
<span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">"King Queen man woman Paris London apple orange Taiwan Dublin Bamberg"</span><span class="op">)</span> </span>
<span></span>
<span><span class="co"># Apply GloVe embeddings to the sentence</span></span>
<span><span class="va">embedding</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Sentence[11]: "King Queen man woman Paris London apple orange Taiwan Dublin Bamberg"</span></span>
<span></span>
<span><span class="co"># Extract embeddings into matrix format</span></span>
<span><span class="va">sen_df</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/process_embeddings.html">process_embeddings</a></span><span class="op">(</span><span class="va">sentence</span>, </span>
<span>                          verbose <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="co">#&gt; Extracting token embeddings...</span></span>
<span><span class="co">#&gt; Converting embeddings to matrix format...Processing completed in 0.004 seconds</span></span>
<span><span class="co">#&gt; Generated embedding matrix with 11 tokens and 100 dimensions</span></span>
<span></span>
<span><span class="co"># Dimensionality Reduction ---------------------------------</span></span>
<span><span class="co"># Set random seed for reproducibility</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Apply PCA to reduce dimensions to 3 components</span></span>
<span><span class="va">pca_result</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/prcomp.html" class="external-link">prcomp</a></span><span class="op">(</span><span class="va">sen_df</span>, center <span class="op">=</span> <span class="cn">TRUE</span>, scale. <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Extract first three principal components</span></span>
<span><span class="va">word_embeddings_matrix</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html" class="external-link">as.data.frame</a></span><span class="op">(</span><span class="va">pca_result</span><span class="op">$</span><span class="va">x</span><span class="op">[</span>,<span class="fl">1</span><span class="op">:</span><span class="fl">3</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="va">word_embeddings_matrix</span></span>
<span><span class="co">#&gt;                PC1       PC2         PC3</span></span>
<span><span class="co">#&gt; King    -2.9120910  1.285200 -1.95053854</span></span>
<span><span class="co">#&gt; Queen   -2.2413804  2.266714 -1.09020972</span></span>
<span><span class="co">#&gt; man     -5.6381902  2.984461  3.55462010</span></span>
<span><span class="co">#&gt; woman   -6.4891003  2.458607  3.56693660</span></span>
<span><span class="co">#&gt; Paris    3.0702212  5.039061 -2.65962020</span></span>
<span><span class="co">#&gt; London   5.3196216  4.368433 -2.60726627</span></span>
<span><span class="co">#&gt; apple    0.3362535 -8.679358 -0.44752722</span></span>
<span><span class="co">#&gt; orange  -0.0485467 -4.404101  0.77151480</span></span>
<span><span class="co">#&gt; Taiwan  -2.7993829 -4.149287 -6.33296039</span></span>
<span><span class="co">#&gt; Dublin   5.8994096  1.063291 -0.09271925</span></span>
<span><span class="co">#&gt; Bamberg  5.5031854 -2.233020  7.28777009</span></span></code></pre></div>
<div class="section level5">
<h5 id="d-plot">2D Plot<a class="anchor" aria-label="anchor" href="#d-plot"></a>
</h5>
<div class="sourceCode" id="cb75"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org" class="external-link">ggplot2</a></span><span class="op">)</span></span>
<span><span class="va">glove_plot2D</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html" class="external-link">ggplot</a></span><span class="op">(</span><span class="va">word_embeddings_matrix</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html" class="external-link">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">PC1</span>, y <span class="op">=</span> <span class="va">PC2</span>, color <span class="op">=</span> <span class="va">PC3</span>, </span>
<span>                                             label <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/colnames.html" class="external-link">rownames</a></span><span class="op">(</span><span class="va">word_embeddings_matrix</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html" class="external-link">geom_point</a></span><span class="op">(</span>size <span class="op">=</span> <span class="fl">3</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_text.html" class="external-link">geom_text</a></span><span class="op">(</span>vjust <span class="op">=</span> <span class="fl">1.5</span>, hjust <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span> <span class="op">+</span>  </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_gradient.html" class="external-link">scale_color_gradient</a></span><span class="op">(</span>low <span class="op">=</span> <span class="st">"blue"</span>, high <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html" class="external-link">theme_minimal</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span>  </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html" class="external-link">labs</a></span><span class="op">(</span>title <span class="op">=</span> <span class="st">""</span>, x <span class="op">=</span> <span class="st">"PC1"</span>, y <span class="op">=</span> <span class="st">"PC2"</span>, color <span class="op">=</span> <span class="st">"PC3"</span><span class="op">)</span> </span>
<span>  <span class="co"># guides(color = "none")  </span></span>
<span><span class="va">glove_plot2D</span></span></code></pre></div>
<p><img src="tutorial_files/figure-html/unnamed-chunk-81-1.png" width="95%"></p>
</div>
<div class="section level5">
<h5 id="d-plot-1">3D Plot<a class="anchor" aria-label="anchor" href="#d-plot-1"></a>
</h5>
<p><a href="https://plotly.com/r/" class="external-link">plotly</a> in R API: <a href="https://plotly.com/r/" class="external-link uri">https://plotly.com/r/</a></p>
<div class="sourceCode" id="cb76"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://plotly-r.com" class="external-link">plotly</a></span><span class="op">)</span></span>
<span><span class="va">glove_plot3D</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/plotly/man/plot_ly.html" class="external-link">plot_ly</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">word_embeddings_matrix</span>, </span>
<span>                  x <span class="op">=</span> <span class="op">~</span><span class="va">PC1</span>, y <span class="op">=</span> <span class="op">~</span><span class="va">PC2</span>, z <span class="op">=</span> <span class="op">~</span><span class="va">PC3</span>, </span>
<span>                  type <span class="op">=</span> <span class="st">"scatter3d"</span>, mode <span class="op">=</span> <span class="st">"markers"</span>,</span>
<span>                  marker <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>size <span class="op">=</span> <span class="fl">5</span><span class="op">)</span>, </span>
<span>                  text <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/colnames.html" class="external-link">rownames</a></span><span class="op">(</span><span class="va">word_embeddings_matrix</span><span class="op">)</span>, hoverinfo <span class="op">=</span> <span class="st">'text'</span><span class="op">)</span></span>
<span></span>
<span><span class="va">glove_plot3D</span></span></code></pre></div>
<div class="plotly html-widget html-fill-item" id="htmlwidget-ac96cb3ee4656e2e9ec3" style="width:95%;height:432.632880098888px;"></div>
<script type="application/json" data-for="htmlwidget-ac96cb3ee4656e2e9ec3">{"x":{"visdat":{"f14bc9e092":["function () ","plotlyVisDat"]},"cur_data":"f14bc9e092","attrs":{"f14bc9e092":{"x":{},"y":{},"z":{},"mode":"markers","marker":{"size":5},"text":["King","Queen","man","woman","Paris","London","apple","orange","Taiwan","Dublin","Bamberg"],"hoverinfo":"text","alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d"}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"PC1"},"yaxis":{"title":"PC2"},"zaxis":{"title":"PC3"}},"hovermode":"closest","showlegend":false},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"x":[-2.9120909537095603,-2.2413803786431767,-5.6381902126446377,-6.489100263080025,3.0702212168953986,5.3196216302548889,0.33625352733862268,-0.048546700064410864,-2.7993829314866114,5.8994096251956245,5.503185439943886],"y":[1.2851996577231659,2.2667143730326891,2.9844609788519225,2.4586065811283584,5.0390609708283636,4.3684329527277832,-8.6793584684395366,-4.404100922374389,-4.149287004719004,1.0632913485136049,-2.2330204672729552],"z":[-1.9505385365242927,-1.0902097244737821,3.5546200951647635,3.5669365986108126,-2.6596201976138238,-2.6072662735213945,-0.44752721605925688,0.7715148004016178,-6.3329603909196477,-0.09271924783335489,7.2877700927683549],"mode":"markers","marker":{"color":"rgba(31,119,180,1)","size":5,"line":{"color":"rgba(31,119,180,1)"}},"text":["King","Queen","man","woman","Paris","London","apple","orange","Taiwan","Dublin","Bamberg"],"hoverinfo":["text","text","text","text","text","text","text","text","text","text","text"],"type":"scatter3d","error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.20000000000000001,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
</div>
</div>
<div class="section level4">
<h4 id="stack-embeddings-method-glove-backforwad-flairembeddings-or-more">Stack Embeddings Method (GloVe + Back/forwad FlairEmbeddings or
More)<a class="anchor" aria-label="anchor" href="#stack-embeddings-method-glove-backforwad-flairembeddings-or-more"></a>
</h4>
<div class="sourceCode" id="cb77"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Initialize Embeddings -----------------------------</span></span>
<span><span class="co"># Load embedding types from flaiR</span></span>
<span><span class="va">WordEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">WordEmbeddings</span></span>
<span><span class="va">FlairEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">FlairEmbeddings</span></span>
<span><span class="va">StackedEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">StackedEmbeddings</span></span>
<span></span>
<span><span class="co"># Configure Embeddings ----------------------------</span></span>
<span><span class="co"># Initialize GloVe word embeddings</span></span>
<span><span class="va">glove_embedding</span> <span class="op">&lt;-</span> <span class="fu">WordEmbeddings</span><span class="op">(</span><span class="st">'glove'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Initialize Flair contextual embeddings</span></span>
<span><span class="va">flair_embedding_forward</span> <span class="op">&lt;-</span> <span class="fu">FlairEmbeddings</span><span class="op">(</span><span class="st">'news-forward'</span><span class="op">)</span></span>
<span><span class="va">flair_embedding_backward</span> <span class="op">&lt;-</span> <span class="fu">FlairEmbeddings</span><span class="op">(</span><span class="st">'news-backward'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Initialize GloVe for individual use</span></span>
<span><span class="va">embedding</span> <span class="op">&lt;-</span> <span class="fu">WordEmbeddings</span><span class="op">(</span><span class="st">"glove"</span><span class="op">)</span> </span>
<span></span>
<span><span class="co"># Create stacked embeddings combining GloVe and bidirectional Flair</span></span>
<span><span class="va">stacked_embeddings</span> <span class="op">&lt;-</span> <span class="fu">StackedEmbeddings</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="va">glove_embedding</span>,</span>
<span>                                         <span class="va">flair_embedding_forward</span>,</span>
<span>                                         <span class="va">flair_embedding_backward</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Text Processing --------------------------------</span></span>
<span><span class="co"># Load Sentence class from flaiR</span></span>
<span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span>
<span></span>
<span><span class="co"># Create test sentence with semantic relationships</span></span>
<span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">"King Queen man woman Paris London apple orange Taiwan Dublin Bamberg"</span><span class="op">)</span> </span>
<span></span>
<span><span class="co"># Apply embeddings and extract features ----------</span></span>
<span><span class="co"># Embed text using stacked embeddings</span></span>
<span><span class="va">stacked_embeddings</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Extract embeddings matrix with processing details</span></span>
<span><span class="va">sen_df</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/process_embeddings.html">process_embeddings</a></span><span class="op">(</span><span class="va">sentence</span>, </span>
<span>                           verbose <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="co">#&gt; Extracting token embeddings...</span></span>
<span><span class="co">#&gt; Converting embeddings to matrix format...Processing completed in 0.004 seconds</span></span>
<span><span class="co">#&gt; Generated embedding matrix with 11 tokens and 4196 dimensions</span></span>
<span></span>
<span><span class="co"># Dimensionality Reduction -----------------------</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Perform PCA for visualization</span></span>
<span><span class="va">pca_result</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/prcomp.html" class="external-link">prcomp</a></span><span class="op">(</span><span class="va">sen_df</span>, center <span class="op">=</span> <span class="cn">TRUE</span>, scale. <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Extract first three principal components</span></span>
<span><span class="va">word_embeddings_matrix</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html" class="external-link">as.data.frame</a></span><span class="op">(</span><span class="va">pca_result</span><span class="op">$</span><span class="va">x</span><span class="op">[</span>,<span class="fl">1</span><span class="op">:</span><span class="fl">3</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="va">word_embeddings_matrix</span></span>
<span><span class="co">#&gt;                PC1         PC2         PC3</span></span>
<span><span class="co">#&gt; King     -8.607464  67.2291073  32.4862897</span></span>
<span><span class="co">#&gt; Queen     1.757712  12.0477242 -26.8302591</span></span>
<span><span class="co">#&gt; man      70.603192  -6.6184846  13.1651727</span></span>
<span><span class="co">#&gt; woman    22.532037  -8.1126281  -0.9074069</span></span>
<span><span class="co">#&gt; Paris   -11.395620  -0.3051653 -17.5197047</span></span>
<span><span class="co">#&gt; London   -8.709174  -2.7450590 -14.1780509</span></span>
<span><span class="co">#&gt; apple    -8.739480 -15.7725165  -6.3796332</span></span>
<span><span class="co">#&gt; orange  -25.178335 -38.8501335  51.4907560</span></span>
<span><span class="co">#&gt; Taiwan   -9.132397  -5.0252071 -11.0918852</span></span>
<span><span class="co">#&gt; Dublin  -10.925014  -3.3407305 -10.1367702</span></span>
<span><span class="co">#&gt; Bamberg -12.205457   1.4930931 -10.0985081</span></span></code></pre></div>
<div class="sourceCode" id="cb78"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># 2D Plot</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org" class="external-link">ggplot2</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">stacked_plot2D</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html" class="external-link">ggplot</a></span><span class="op">(</span><span class="va">word_embeddings_matrix</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html" class="external-link">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">PC1</span>, y <span class="op">=</span> <span class="va">PC2</span>, color <span class="op">=</span> <span class="va">PC3</span>, </span>
<span>                                             label <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/colnames.html" class="external-link">rownames</a></span><span class="op">(</span><span class="va">word_embeddings_matrix</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html" class="external-link">geom_point</a></span><span class="op">(</span>size <span class="op">=</span> <span class="fl">2</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_text.html" class="external-link">geom_text</a></span><span class="op">(</span>vjust <span class="op">=</span> <span class="fl">1.5</span>, hjust <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span> <span class="op">+</span>  </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_gradient.html" class="external-link">scale_color_gradient</a></span><span class="op">(</span>low <span class="op">=</span> <span class="st">"blue"</span>, high <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html" class="external-link">theme_minimal</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span>  </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html" class="external-link">labs</a></span><span class="op">(</span>title <span class="op">=</span> <span class="st">""</span>, x <span class="op">=</span> <span class="st">"PC1"</span>, y <span class="op">=</span> <span class="st">"PC2"</span>, color <span class="op">=</span> <span class="st">"PC3"</span><span class="op">)</span> </span>
<span></span>
<span><span class="va">stacked_plot2D</span></span></code></pre></div>
<p><img src="tutorial_files/figure-html/unnamed-chunk-84-1.png" width="95%"></p>
</div>
<div class="section level4">
<h4 id="transformer-embeddings-bert-or-more">Transformer Embeddings (BERT or More)<a class="anchor" aria-label="anchor" href="#transformer-embeddings-bert-or-more"></a>
</h4>
<div class="sourceCode" id="cb79"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load Required Package ----------------------------</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://davidycliao.github.io/flaiR">flaiR</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Initialize BERT and Text Processing --------------</span></span>
<span><span class="co"># Import Sentence class for text operations</span></span>
<span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span>
<span></span>
<span><span class="co"># Initialize BERT model (base uncased version)</span></span>
<span><span class="va">TransformerWordEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="fu">TransformerWordEmbeddings</span><span class="op">(</span><span class="st">"bert-base-uncased"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Text Processing and Embedding --------------------</span></span>
<span><span class="co"># Create sentence with semantic relationship pairs</span></span>
<span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">"King Queen man woman Paris London apple orange Taiwan Dublin Bamberg"</span><span class="op">)</span> </span>
<span></span>
<span><span class="co"># Apply BERT embeddings to the sentence</span></span>
<span><span class="va">TransformerWordEmbeddings</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Sentence[11]: "King Queen man woman Paris London apple orange Taiwan Dublin Bamberg"</span></span>
<span></span>
<span><span class="co"># Extract embeddings into matrix format</span></span>
<span><span class="va">sen_df</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/process_embeddings.html">process_embeddings</a></span><span class="op">(</span><span class="va">sentence</span>, verbose <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="co">#&gt; Extracting token embeddings...</span></span>
<span><span class="co">#&gt; Converting embeddings to matrix format...Processing completed in 0.004 seconds</span></span>
<span><span class="co">#&gt; Generated embedding matrix with 11 tokens and 768 dimensions</span></span>
<span></span>
<span><span class="co"># Dimensionality Reduction ------------------------</span></span>
<span><span class="co"># Set random seed for reproducibility</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Apply PCA to reduce dimensions to 3 components</span></span>
<span><span class="va">pca_result</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/prcomp.html" class="external-link">prcomp</a></span><span class="op">(</span><span class="va">sen_df</span>, center <span class="op">=</span> <span class="cn">TRUE</span>, scale. <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Extract first three principal components</span></span>
<span><span class="va">word_embeddings_matrix</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html" class="external-link">as.data.frame</a></span><span class="op">(</span><span class="va">pca_result</span><span class="op">$</span><span class="va">x</span><span class="op">[</span>,<span class="fl">1</span><span class="op">:</span><span class="fl">3</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="va">word_embeddings_matrix</span></span>
<span><span class="co">#&gt;                 PC1        PC2        PC3</span></span>
<span><span class="co">#&gt; King    -11.6842542   4.264797  -2.539303</span></span>
<span><span class="co">#&gt; Queen   -18.6452604  13.278048  -9.781884</span></span>
<span><span class="co">#&gt; man     -18.3566181  10.571207  -1.609842</span></span>
<span><span class="co">#&gt; woman   -10.6397232  -2.072423   2.264132</span></span>
<span><span class="co">#&gt; Paris     6.9079152  -9.409334 -10.378022</span></span>
<span><span class="co">#&gt; London   10.7817678  -8.882271 -11.447016</span></span>
<span><span class="co">#&gt; apple    -0.6539754  -8.305123  12.104275</span></span>
<span><span class="co">#&gt; orange   -3.4409765  -4.756996  19.216482</span></span>
<span><span class="co">#&gt; Taiwan    4.5377967 -12.949604   4.782874</span></span>
<span><span class="co">#&gt; Dublin   13.4845075 -11.327307  -7.191567</span></span>
<span><span class="co">#&gt; Bamberg  27.7088205  29.589006   4.579870</span></span></code></pre></div>
<div class="sourceCode" id="cb80"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># 2D Plot</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org" class="external-link">ggplot2</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">bert_plot2D</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html" class="external-link">ggplot</a></span><span class="op">(</span><span class="va">word_embeddings_matrix</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html" class="external-link">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">PC1</span>, y <span class="op">=</span> <span class="va">PC2</span>, color <span class="op">=</span> <span class="va">PC3</span>, </span>
<span>                                             label <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/colnames.html" class="external-link">rownames</a></span><span class="op">(</span><span class="va">word_embeddings_matrix</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html" class="external-link">geom_point</a></span><span class="op">(</span>size <span class="op">=</span> <span class="fl">2</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_text.html" class="external-link">geom_text</a></span><span class="op">(</span>vjust <span class="op">=</span> <span class="fl">1.5</span>, hjust <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span> <span class="op">+</span>  </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_gradient.html" class="external-link">scale_color_gradient</a></span><span class="op">(</span>low <span class="op">=</span> <span class="st">"blue"</span>, high <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html" class="external-link">theme_minimal</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span>  </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html" class="external-link">labs</a></span><span class="op">(</span>title <span class="op">=</span> <span class="st">""</span>, x <span class="op">=</span> <span class="st">"PC1"</span>, y <span class="op">=</span> <span class="st">"PC2"</span>, color <span class="op">=</span> <span class="st">"PC3"</span><span class="op">)</span> </span>
<span>  <span class="co"># guides(color = "none") </span></span>
<span></span>
<span><span class="va">stacked_plot2D</span></span></code></pre></div>
<p><img src="tutorial_files/figure-html/unnamed-chunk-86-1.png" width="95%"></p>
</div>
<div class="section level4">
<h4 id="embedding-models-comparison">Embedding Models Comparison<a class="anchor" aria-label="anchor" href="#embedding-models-comparison"></a>
</h4>
<div class="sourceCode" id="cb81"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://rpkgs.datanovia.com/ggpubr/" class="external-link">ggpubr</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">figure</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rpkgs.datanovia.com/ggpubr/reference/ggarrange.html" class="external-link">ggarrange</a></span><span class="op">(</span><span class="va">glove_plot2D</span>, <span class="va">stacked_plot2D</span>, <span class="va">bert_plot2D</span>,</span>
<span>                   labels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"Glove"</span>, <span class="st">"Stacked Embedding"</span>, <span class="st">"BERT"</span><span class="op">)</span>,</span>
<span>                   ncol <span class="op">=</span> <span class="fl">3</span>, nrow <span class="op">=</span> <span class="fl">1</span>,</span>
<span>                   common.legend <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                   legend <span class="op">=</span> <span class="st">"bottom"</span>,</span>
<span>                   font.label <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>size <span class="op">=</span> <span class="fl">8</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">figure</span></span></code></pre></div>
<p><img src="tutorial_files/figure-html/unnamed-chunk-87-1.png" width="95%"></p>
<p> </p>
<hr>
</div>
</div>
</div>
<div class="section level2">
<h2 id="training-a-binary-classifier">Training a Binary Classifier<a class="anchor" aria-label="anchor" href="#training-a-binary-classifier"></a>
</h2>
<p>In this section, we’ll train a sentiment analysis model that can
categorize text as either positive or negative. This case study is
adapted from pages 116 to 130 of Tadej Magajna’s book, ‘<a href="https://www.packtpub.com/product/natural-language-processing-with-flair/9781801072311" class="external-link">Natural
Language Processing with Flair</a>’. The process for training text
classifiers in Flair mirrors the process followed for sequence labeling
models. Specifically, the steps to train text classifiers are:</p>
<ul>
<li>Load a tagged corpus and compute the label dictionary map.</li>
<li>Prepare the document embeddings.</li>
<li>Initialize the <code>TextClassifier</code> class.</li>
<li>Train the model.</li>
</ul>
<div class="section level3">
<h3 id="loading-a-tagged-corpus">Loading a Tagged Corpus<a class="anchor" aria-label="anchor" href="#loading-a-tagged-corpus"></a>
</h3>
<div style="text-align: justify;">
<p>Training text classification models requires a set of text documents
(typically, sentences or paragraphs) where each document is associated
with one or more classification labels. To train our sentiment analysis
text classification model, we will be using the famous Internet Movie
Database (IMDb) dataset, which contains 50,000 movie reviews from IMDB,
where each review is labeled as either positive or negative. References
to this dataset are already baked into Flair, so loading the dataset
couldn’t be easier:</p>
</div>
<div class="sourceCode" id="cb82"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://davidycliao.github.io/flaiR">flaiR</a></span><span class="op">)</span></span>
<span><span class="co"># load IMDB from flair_datasets module</span></span>
<span><span class="va">Corpus</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Corpus</span></span>
<span><span class="va">IMDB</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_datasets.html">flair_datasets</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">IMDB</span></span></code></pre></div>
<div class="sourceCode" id="cb83"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># downsize to 0.05</span></span>
<span><span class="va">corpus</span> <span class="op">=</span> <span class="fu">IMDB</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:21,350 Reading data from /Users/yenchiehliao/.flair/datasets/imdb_v4-rebalanced</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:21,351 Train: /Users/yenchiehliao/.flair/datasets/imdb_v4-rebalanced/train.txt</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:21,351 Dev: None</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:21,351 Test: None</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:21,952 No test split found. Using 10% (i.e. 5000 samples) of the train split as test data</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:21,967 No dev split found. Using 10% (i.e. 4500 samples) of the train split as dev data</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:21,967 Initialized corpus /Users/yenchiehliao/.flair/datasets/imdb_v4-rebalanced (label type name is 'sentiment')</span></span>
<span><span class="va">corpus</span><span class="op">$</span><span class="fu">downsample</span><span class="op">(</span><span class="fl">0.05</span><span class="op">)</span></span>
<span><span class="co">#&gt; &lt;flair.datasets.document_classification.IMDB object at 0x5162fc6d0&gt;</span></span></code></pre></div>
<p>Print the sizes in the corpus object as follows - test: %d | train:
%d | dev: %d”</p>
<div class="sourceCode" id="cb84"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">test_size</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">corpus</span><span class="op">$</span><span class="va">test</span><span class="op">)</span></span>
<span><span class="va">train_size</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">corpus</span><span class="op">$</span><span class="va">train</span><span class="op">)</span></span>
<span><span class="va">dev_size</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">corpus</span><span class="op">$</span><span class="va">dev</span><span class="op">)</span></span>
<span><span class="va">output</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sprintf.html" class="external-link">sprintf</a></span><span class="op">(</span><span class="st">"Corpus object sizes - Test: %d | Train: %d | Dev: %d"</span>, <span class="va">test_size</span>, <span class="va">train_size</span>, <span class="va">dev_size</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">output</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "Corpus object sizes - Test: 250 | Train: 2025 | Dev: 225"</span></span></code></pre></div>
<div class="sourceCode" id="cb85"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">lbl_type</span> <span class="op">=</span> <span class="st">'sentiment'</span></span>
<span><span class="va">label_dict</span> <span class="op">=</span> <span class="va">corpus</span><span class="op">$</span><span class="fu">make_label_dictionary</span><span class="op">(</span>label_type<span class="op">=</span><span class="va">lbl_type</span><span class="op">)</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:22,060 Computing label dictionary. Progress:</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:26,436 Dictionary created for label 'sentiment' with 2 values: POSITIVE (seen 1014 times), NEGATIVE (seen 1011 times)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="loading-the-embeddings">Loading the Embeddings<a class="anchor" aria-label="anchor" href="#loading-the-embeddings"></a>
</h3>
<div style="text-align: justify;">
<p>flaiR covers all the different types of document embeddings that we
can use. Here, we simply use <code>DocumentPoolEmbeddings</code>. They
require no training prior to training the classification model
itself:</p>
</div>
<div class="sourceCode" id="cb86"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">DocumentPoolEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">DocumentPoolEmbeddings</span></span>
<span><span class="va">WordEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">WordEmbeddings</span></span>
<span><span class="va">glove</span> <span class="op">=</span> <span class="fu">WordEmbeddings</span><span class="op">(</span><span class="st">'glove'</span><span class="op">)</span></span>
<span><span class="va">document_embeddings</span> <span class="op">=</span> <span class="fu">DocumentPoolEmbeddings</span><span class="op">(</span><span class="va">glove</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="initializing-the-textclassifier">Initializing the TextClassifier<a class="anchor" aria-label="anchor" href="#initializing-the-textclassifier"></a>
</h3>
<div class="sourceCode" id="cb87"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># initiate TextClassifier</span></span>
<span><span class="va">TextClassifier</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_models.html">flair_models</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">TextClassifier</span></span>
<span><span class="va">classifier</span> <span class="op">&lt;-</span> <span class="fu">TextClassifier</span><span class="op">(</span><span class="va">document_embeddings</span>,</span>
<span>                             label_dictionary <span class="op">=</span> <span class="va">label_dict</span>,</span>
<span>                             label_type <span class="op">=</span> <span class="va">lbl_type</span><span class="op">)</span></span></code></pre></div>
<p><code>$to</code> allows you to set the device to use CPU, GPU, or
specific MPS devices on Mac (such as mps:0, mps:1, mps:2).</p>
<div class="sourceCode" id="cb88"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">classifier</span><span class="op">$</span><span class="fu">to</span><span class="op">(</span><span class="fu"><a href="../reference/flair_device.html">flair_device</a></span><span class="op">(</span><span class="st">"mps"</span><span class="op">)</span><span class="op">)</span> </span></code></pre></div>
<pre><code><span><span class="fu">TextClassifier</span><span class="op">(</span></span>
<span>  <span class="op">(</span><span class="va">embeddings</span><span class="op">)</span><span class="op">:</span> <span class="fu">DocumentPoolEmbeddings</span><span class="op">(</span></span>
<span>    fine_tune_mode<span class="op">=</span><span class="va">none</span>, pooling<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span></span>
<span>    <span class="op">(</span><span class="va">embeddings</span><span class="op">)</span><span class="op">:</span> <span class="fu">StackedEmbeddings</span><span class="op">(</span></span>
<span>      <span class="op">(</span><span class="va">list_embedding_0</span><span class="op">)</span><span class="op">:</span> <span class="fu">WordEmbeddings</span><span class="op">(</span></span>
<span>        <span class="st">'glove'</span></span>
<span>        <span class="op">(</span><span class="va">embedding</span><span class="op">)</span><span class="op">:</span> <span class="fu">Embedding</span><span class="op">(</span><span class="fl">400001</span>, <span class="fl">100</span><span class="op">)</span></span>
<span>      <span class="op">)</span></span>
<span>    <span class="op">)</span></span>
<span>  <span class="op">)</span></span>
<span>  <span class="op">(</span><span class="va">decoder</span><span class="op">)</span><span class="op">:</span> <span class="fu">Linear</span><span class="op">(</span>in_features<span class="op">=</span><span class="fl">100</span>, out_features<span class="op">=</span><span class="fl">3</span>, bias<span class="op">=</span><span class="va">True</span><span class="op">)</span></span>
<span>  <span class="op">(</span><span class="va">dropout</span><span class="op">)</span><span class="op">:</span> <span class="fu">Dropout</span><span class="op">(</span>p<span class="op">=</span><span class="fl">0.0</span>, inplace<span class="op">=</span><span class="va">False</span><span class="op">)</span></span>
<span>  <span class="op">(</span><span class="va">locked_dropout</span><span class="op">)</span><span class="op">:</span> <span class="fu">LockedDropout</span><span class="op">(</span>p<span class="op">=</span><span class="fl">0.0</span><span class="op">)</span></span>
<span>  <span class="op">(</span><span class="va">word_dropout</span><span class="op">)</span><span class="op">:</span> <span class="fu">WordDropout</span><span class="op">(</span>p<span class="op">=</span><span class="fl">0.0</span><span class="op">)</span></span>
<span>  <span class="op">(</span><span class="va">loss_function</span><span class="op">)</span><span class="op">:</span> <span class="fu">CrossEntropyLoss</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre>
</div>
<div class="section level3">
<h3 id="training-the-model">Training the Model<a class="anchor" aria-label="anchor" href="#training-the-model"></a>
</h3>
<p>Training the text classifier model involves two simple steps:</p>
<ul>
<li>Defining the model trainer class by passing in the classifier model
and the corpus</li>
<li>Setting off the training process passing in the required training
hyper-parameters.</li>
</ul>
<div style="text-align: justify;">
<p><strong>It is worth noting that the ‘L’ in numbers like 32L and 5L is
used in R to denote that the number is an integer. Without the ‘L’
suffix, numbers in R are treated as numeric, which are by default
double-precision floating-point numbers. In contrast, Python determines
the type based on the value of the number itself. Whole numbers (e.g., 5
or 32) are of type int, while numbers with decimal points (e.g., 5.0)
are of type float. Floating-point numbers in both languages are
representations of real numbers but can have some approximation due to
the way they are stored in memory.</strong></p>
</div>
<div class="sourceCode" id="cb90"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># initiate ModelTrainer</span></span>
<span><span class="va">ModelTrainer</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_trainers.html">flair_trainers</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">ModelTrainer</span></span>
<span></span>
<span><span class="co"># fit the model</span></span>
<span><span class="va">trainer</span> <span class="op">&lt;-</span> <span class="fu">ModelTrainer</span><span class="op">(</span><span class="va">classifier</span>, <span class="va">corpus</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># start to train</span></span>
<span><span class="co"># note: the 'L' in 32L is used in R to denote that the number is an integer.</span></span>
<span><span class="va">trainer</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="st">'classifier'</span>,</span>
<span>              learning_rate<span class="op">=</span><span class="fl">0.1</span>,</span>
<span>              mini_batch_size<span class="op">=</span><span class="fl">32L</span>,</span>
<span>              <span class="co"># specifies how embeddings are stored in RAM, ie."cpu", "cuda", "gpu", "mps".</span></span>
<span>              <span class="co"># embeddings_storage_mode = "mps",</span></span>
<span>              max_epochs<span class="op">=</span><span class="fl">10L</span><span class="op">)</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:28,413 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:28,413 Model: "TextClassifier(</span></span>
<span><span class="co">#&gt;   (embeddings): DocumentPoolEmbeddings(</span></span>
<span><span class="co">#&gt;     fine_tune_mode=none, pooling=mean</span></span>
<span><span class="co">#&gt;     (embeddings): StackedEmbeddings(</span></span>
<span><span class="co">#&gt;       (list_embedding_0): WordEmbeddings(</span></span>
<span><span class="co">#&gt;         'glove'</span></span>
<span><span class="co">#&gt;         (embedding): Embedding(400001, 100)</span></span>
<span><span class="co">#&gt;       )</span></span>
<span><span class="co">#&gt;     )</span></span>
<span><span class="co">#&gt;   )</span></span>
<span><span class="co">#&gt;   (decoder): Linear(in_features=100, out_features=2, bias=True)</span></span>
<span><span class="co">#&gt;   (dropout): Dropout(p=0.0, inplace=False)</span></span>
<span><span class="co">#&gt;   (locked_dropout): LockedDropout(p=0.0)</span></span>
<span><span class="co">#&gt;   (word_dropout): WordDropout(p=0.0)</span></span>
<span><span class="co">#&gt;   (loss_function): CrossEntropyLoss()</span></span>
<span><span class="co">#&gt;   (weights): None</span></span>
<span><span class="co">#&gt;   (weight_tensor) None</span></span>
<span><span class="co">#&gt; )"</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:28,413 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:28,413 Corpus: 2025 train + 225 dev + 250 test sentences</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:28,413 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:28,413 Train:  2025 sentences</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:28,413         (train_with_dev=False, train_with_test=False)</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:28,413 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:28,413 Training Params:</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:28,413  - learning_rate: "0.1" </span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:28,414  - mini_batch_size: "32"</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:28,414  - max_epochs: "10"</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:28,414  - shuffle: "True"</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:28,414 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:28,414 Plugins:</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:28,414  - AnnealOnPlateau | patience: '3', anneal_factor: '0.5', min_learning_rate: '0.0001'</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:28,414 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:28,414 Final evaluation on model from best epoch (best-model.pt)</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:28,414  - metric: "('micro avg', 'f1-score')"</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:28,414 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:28,414 Computation:</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:28,414  - compute on device: cpu</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:28,414  - embedding storage: cpu</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:28,414 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:28,414 Model training base path: "classifier"</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:28,414 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:28,414 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:29,257 epoch 1 - iter 6/64 - loss 1.03482102 - time (sec): 0.84 - samples/sec: 227.94 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:30,095 epoch 1 - iter 12/64 - loss 0.97625112 - time (sec): 1.68 - samples/sec: 228.49 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:31,264 epoch 1 - iter 18/64 - loss 0.97130605 - time (sec): 2.85 - samples/sec: 202.15 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:32,131 epoch 1 - iter 24/64 - loss 0.96554917 - time (sec): 3.72 - samples/sec: 206.65 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:33,013 epoch 1 - iter 30/64 - loss 0.95172536 - time (sec): 4.60 - samples/sec: 208.74 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:33,973 epoch 1 - iter 36/64 - loss 0.93596985 - time (sec): 5.56 - samples/sec: 207.24 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:34,821 epoch 1 - iter 42/64 - loss 0.93621534 - time (sec): 6.41 - samples/sec: 209.79 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:35,728 epoch 1 - iter 48/64 - loss 0.93329721 - time (sec): 7.31 - samples/sec: 210.01 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:36,601 epoch 1 - iter 54/64 - loss 0.92732978 - time (sec): 8.19 - samples/sec: 211.08 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:37,744 epoch 1 - iter 60/64 - loss 0.91905521 - time (sec): 9.33 - samples/sec: 205.80 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:38,082 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:38,082 EPOCH 1 done: loss 0.9183 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:39,411 DEV : loss 0.969170331954956 - f1-score (micro avg)  0.4533</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:40,208  - 0 epochs without improvement</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:40,209 saving best model</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:40,650 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:41,648 epoch 2 - iter 6/64 - loss 0.89565008 - time (sec): 1.00 - samples/sec: 192.47 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:42,546 epoch 2 - iter 12/64 - loss 0.91454447 - time (sec): 1.90 - samples/sec: 202.57 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:43,654 epoch 2 - iter 18/64 - loss 0.87032511 - time (sec): 3.00 - samples/sec: 191.82 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:44,940 epoch 2 - iter 24/64 - loss 0.89476166 - time (sec): 4.29 - samples/sec: 179.04 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:45,965 epoch 2 - iter 30/64 - loss 0.89790537 - time (sec): 5.31 - samples/sec: 180.64 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:46,893 epoch 2 - iter 36/64 - loss 0.88154723 - time (sec): 6.24 - samples/sec: 184.56 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:47,900 epoch 2 - iter 42/64 - loss 0.89160718 - time (sec): 7.25 - samples/sec: 185.41 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:48,882 epoch 2 - iter 48/64 - loss 0.88830253 - time (sec): 8.23 - samples/sec: 186.62 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:49,723 epoch 2 - iter 54/64 - loss 0.88188778 - time (sec): 9.07 - samples/sec: 190.48 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:50,594 epoch 2 - iter 60/64 - loss 0.87024007 - time (sec): 9.94 - samples/sec: 193.10 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:51,221 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:51,221 EPOCH 2 done: loss 0.8670 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:52,640 DEV : loss 0.6613298654556274 - f1-score (micro avg)  0.56</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:53,510  - 0 epochs without improvement</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:53,511 saving best model</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:53,853 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:54,970 epoch 3 - iter 6/64 - loss 0.97559432 - time (sec): 1.12 - samples/sec: 171.99 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:55,906 epoch 3 - iter 12/64 - loss 0.96988676 - time (sec): 2.05 - samples/sec: 187.15 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:56,919 epoch 3 - iter 18/64 - loss 0.95166033 - time (sec): 3.07 - samples/sec: 187.90 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:57,805 epoch 3 - iter 24/64 - loss 0.94594647 - time (sec): 3.95 - samples/sec: 194.39 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:58,731 epoch 3 - iter 30/64 - loss 0.92877954 - time (sec): 4.88 - samples/sec: 196.85 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:02:59,694 epoch 3 - iter 36/64 - loss 0.90791758 - time (sec): 5.84 - samples/sec: 197.24 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:00,675 epoch 3 - iter 42/64 - loss 0.90284721 - time (sec): 6.82 - samples/sec: 197.05 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:01,694 epoch 3 - iter 48/64 - loss 0.89997107 - time (sec): 7.84 - samples/sec: 195.91 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:02,773 epoch 3 - iter 54/64 - loss 0.89072626 - time (sec): 8.92 - samples/sec: 193.73 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:03,973 epoch 3 - iter 60/64 - loss 0.89468155 - time (sec): 10.12 - samples/sec: 189.74 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:04,405 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:04,406 EPOCH 3 done: loss 0.8853 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:06,045 DEV : loss 0.9235679507255554 - f1-score (micro avg)  0.4533</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:06,628  - 1 epochs without improvement</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:06,630 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:08,014 epoch 4 - iter 6/64 - loss 0.82624254 - time (sec): 1.38 - samples/sec: 138.74 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:09,249 epoch 4 - iter 12/64 - loss 0.90411119 - time (sec): 2.62 - samples/sec: 146.65 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:11,007 epoch 4 - iter 18/64 - loss 0.85366339 - time (sec): 4.38 - samples/sec: 131.61 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:12,098 epoch 4 - iter 24/64 - loss 0.83558531 - time (sec): 5.47 - samples/sec: 140.47 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:13,134 epoch 4 - iter 30/64 - loss 0.85402448 - time (sec): 6.50 - samples/sec: 147.61 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:14,263 epoch 4 - iter 36/64 - loss 0.85074003 - time (sec): 7.63 - samples/sec: 150.94 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:15,365 epoch 4 - iter 42/64 - loss 0.83589199 - time (sec): 8.73 - samples/sec: 153.88 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:16,395 epoch 4 - iter 48/64 - loss 0.84322071 - time (sec): 9.76 - samples/sec: 157.30 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:17,312 epoch 4 - iter 54/64 - loss 0.83743314 - time (sec): 10.68 - samples/sec: 161.77 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:18,262 epoch 4 - iter 60/64 - loss 0.83712479 - time (sec): 11.63 - samples/sec: 165.06 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:18,871 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:18,871 EPOCH 4 done: loss 0.8385 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:20,146 DEV : loss 0.902965784072876 - f1-score (micro avg)  0.4533</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:20,953  - 2 epochs without improvement</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:20,956 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:21,826 epoch 5 - iter 6/64 - loss 0.83386131 - time (sec): 0.87 - samples/sec: 220.80 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:22,721 epoch 5 - iter 12/64 - loss 0.81110337 - time (sec): 1.76 - samples/sec: 217.58 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:23,645 epoch 5 - iter 18/64 - loss 0.81918713 - time (sec): 2.69 - samples/sec: 214.20 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:24,551 epoch 5 - iter 24/64 - loss 0.82163843 - time (sec): 3.59 - samples/sec: 213.65 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:25,493 epoch 5 - iter 30/64 - loss 0.80355368 - time (sec): 4.54 - samples/sec: 211.62 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:26,718 epoch 5 - iter 36/64 - loss 0.82123225 - time (sec): 5.76 - samples/sec: 199.92 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:27,734 epoch 5 - iter 42/64 - loss 0.83147700 - time (sec): 6.78 - samples/sec: 198.31 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:28,627 epoch 5 - iter 48/64 - loss 0.82776047 - time (sec): 7.67 - samples/sec: 200.24 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:29,564 epoch 5 - iter 54/64 - loss 0.82137293 - time (sec): 8.61 - samples/sec: 200.74 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:30,468 epoch 5 - iter 60/64 - loss 0.82507927 - time (sec): 9.51 - samples/sec: 201.85 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:31,095 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:31,095 EPOCH 5 done: loss 0.8266 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:32,352 DEV : loss 0.8814549446105957 - f1-score (micro avg)  0.4533</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:33,076  - 3 epochs without improvement</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:33,077 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:34,090 epoch 6 - iter 6/64 - loss 0.86599507 - time (sec): 1.01 - samples/sec: 189.54 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:35,050 epoch 6 - iter 12/64 - loss 0.83119471 - time (sec): 1.97 - samples/sec: 194.62 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:36,034 epoch 6 - iter 18/64 - loss 0.83965523 - time (sec): 2.96 - samples/sec: 194.80 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:36,911 epoch 6 - iter 24/64 - loss 0.82790930 - time (sec): 3.83 - samples/sec: 200.35 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:37,794 epoch 6 - iter 30/64 - loss 0.82493706 - time (sec): 4.72 - samples/sec: 203.52 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:38,736 epoch 6 - iter 36/64 - loss 0.82221150 - time (sec): 5.66 - samples/sec: 203.59 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:39,646 epoch 6 - iter 42/64 - loss 0.82115991 - time (sec): 6.57 - samples/sec: 204.62 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:40,549 epoch 6 - iter 48/64 - loss 0.82263479 - time (sec): 7.47 - samples/sec: 205.58 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:41,435 epoch 6 - iter 54/64 - loss 0.81930931 - time (sec): 8.36 - samples/sec: 206.76 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:42,305 epoch 6 - iter 60/64 - loss 0.81072097 - time (sec): 9.23 - samples/sec: 208.07 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:42,932 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:42,932 EPOCH 6 done: loss 0.8158 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:44,188 DEV : loss 0.7792848944664001 - f1-score (micro avg)  0.5556</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:44,915  - 4 epochs without improvement (above 'patience')-&gt; annealing learning_rate to [0.05]</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:44,917 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:45,764 epoch 7 - iter 6/64 - loss 0.62141190 - time (sec): 0.85 - samples/sec: 226.88 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:46,665 epoch 7 - iter 12/64 - loss 0.60887330 - time (sec): 1.75 - samples/sec: 219.70 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:47,536 epoch 7 - iter 18/64 - loss 0.60901921 - time (sec): 2.62 - samples/sec: 219.94 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:48,443 epoch 7 - iter 24/64 - loss 0.62312240 - time (sec): 3.53 - samples/sec: 217.82 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:49,515 epoch 7 - iter 30/64 - loss 0.64444851 - time (sec): 4.60 - samples/sec: 208.79 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:50,426 epoch 7 - iter 36/64 - loss 0.64126529 - time (sec): 5.51 - samples/sec: 209.15 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:51,306 epoch 7 - iter 42/64 - loss 0.65158008 - time (sec): 6.39 - samples/sec: 210.40 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:52,234 epoch 7 - iter 48/64 - loss 0.65823784 - time (sec): 7.32 - samples/sec: 209.93 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:53,223 epoch 7 - iter 54/64 - loss 0.65939955 - time (sec): 8.31 - samples/sec: 208.05 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:54,190 epoch 7 - iter 60/64 - loss 0.66072118 - time (sec): 9.27 - samples/sec: 207.08 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:54,800 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:54,800 EPOCH 7 done: loss 0.6589 - lr: 0.050000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:55,992 DEV : loss 0.6003721356391907 - f1-score (micro avg)  0.6978</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:56,760  - 0 epochs without improvement</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:56,762 saving best model</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:57,171 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:58,053 epoch 8 - iter 6/64 - loss 0.65252212 - time (sec): 0.88 - samples/sec: 217.70 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:59,057 epoch 8 - iter 12/64 - loss 0.63390189 - time (sec): 1.89 - samples/sec: 203.62 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:03:59,952 epoch 8 - iter 18/64 - loss 0.63412659 - time (sec): 2.78 - samples/sec: 207.11 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:01,075 epoch 8 - iter 24/64 - loss 0.63389341 - time (sec): 3.90 - samples/sec: 196.72 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:02,016 epoch 8 - iter 30/64 - loss 0.63322895 - time (sec): 4.84 - samples/sec: 198.17 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:03,338 epoch 8 - iter 36/64 - loss 0.63415459 - time (sec): 6.17 - samples/sec: 186.83 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:04,331 epoch 8 - iter 42/64 - loss 0.63968469 - time (sec): 7.16 - samples/sec: 187.71 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:05,300 epoch 8 - iter 48/64 - loss 0.65097602 - time (sec): 8.13 - samples/sec: 188.95 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:06,243 epoch 8 - iter 54/64 - loss 0.65496666 - time (sec): 9.07 - samples/sec: 190.47 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:07,227 epoch 8 - iter 60/64 - loss 0.64874915 - time (sec): 10.06 - samples/sec: 190.93 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:07,854 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:07,855 EPOCH 8 done: loss 0.6494 - lr: 0.050000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:09,156 DEV : loss 0.6876248717308044 - f1-score (micro avg)  0.5467</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:09,920  - 1 epochs without improvement</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:09,922 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:10,904 epoch 9 - iter 6/64 - loss 0.64467014 - time (sec): 0.98 - samples/sec: 195.52 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:11,961 epoch 9 - iter 12/64 - loss 0.64915569 - time (sec): 2.04 - samples/sec: 188.35 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:12,911 epoch 9 - iter 18/64 - loss 0.63960192 - time (sec): 2.99 - samples/sec: 192.73 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:13,928 epoch 9 - iter 24/64 - loss 0.62827631 - time (sec): 4.01 - samples/sec: 191.72 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:14,936 epoch 9 - iter 30/64 - loss 0.63651646 - time (sec): 5.01 - samples/sec: 191.46 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:15,889 epoch 9 - iter 36/64 - loss 0.63269677 - time (sec): 5.97 - samples/sec: 193.07 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:16,828 epoch 9 - iter 42/64 - loss 0.64019648 - time (sec): 6.91 - samples/sec: 194.62 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:17,868 epoch 9 - iter 48/64 - loss 0.64170534 - time (sec): 7.95 - samples/sec: 193.30 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:18,832 epoch 9 - iter 54/64 - loss 0.63846545 - time (sec): 8.91 - samples/sec: 193.94 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:20,097 epoch 9 - iter 60/64 - loss 0.63938779 - time (sec): 10.17 - samples/sec: 188.70 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:20,475 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:20,476 EPOCH 9 done: loss 0.6397 - lr: 0.050000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:21,799 DEV : loss 0.5814307332038879 - f1-score (micro avg)  0.7156</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:22,610  - 0 epochs without improvement</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:22,613 saving best model</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:22,982 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:23,847 epoch 10 - iter 6/64 - loss 0.60317001 - time (sec): 0.86 - samples/sec: 222.09 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:24,865 epoch 10 - iter 12/64 - loss 0.61682722 - time (sec): 1.88 - samples/sec: 204.02 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:25,971 epoch 10 - iter 18/64 - loss 0.61251386 - time (sec): 2.99 - samples/sec: 192.72 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:27,248 epoch 10 - iter 24/64 - loss 0.61089186 - time (sec): 4.27 - samples/sec: 180.04 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:28,175 epoch 10 - iter 30/64 - loss 0.62348493 - time (sec): 5.19 - samples/sec: 184.86 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:29,032 epoch 10 - iter 36/64 - loss 0.61944086 - time (sec): 6.05 - samples/sec: 190.42 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:30,011 epoch 10 - iter 42/64 - loss 0.62064590 - time (sec): 7.03 - samples/sec: 191.22 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:31,039 epoch 10 - iter 48/64 - loss 0.62229136 - time (sec): 8.06 - samples/sec: 190.65 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:32,118 epoch 10 - iter 54/64 - loss 0.62610477 - time (sec): 9.14 - samples/sec: 189.15 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:33,054 epoch 10 - iter 60/64 - loss 0.62748551 - time (sec): 10.07 - samples/sec: 190.63 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:33,629 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:33,629 EPOCH 10 done: loss 0.6305 - lr: 0.050000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:34,963 DEV : loss 0.5568863153457642 - f1-score (micro avg)  0.7244</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:35,828  - 0 epochs without improvement</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:35,830 saving best model</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:36,712 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:36,713 Loading model from best epoch ...</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:38,435 </span></span>
<span><span class="co">#&gt; Results:</span></span>
<span><span class="co">#&gt; - F-score (micro) 0.712</span></span>
<span><span class="co">#&gt; - F-score (macro) 0.7078</span></span>
<span><span class="co">#&gt; - Accuracy 0.712</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; By class:</span></span>
<span><span class="co">#&gt;               precision    recall  f1-score   support</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     NEGATIVE     0.6541    0.8595    0.7429       121</span></span>
<span><span class="co">#&gt;     POSITIVE     0.8132    0.5736    0.6727       129</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     accuracy                         0.7120       250</span></span>
<span><span class="co">#&gt;    macro avg     0.7336    0.7166    0.7078       250</span></span>
<span><span class="co">#&gt; weighted avg     0.7362    0.7120    0.7067       250</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:38,435 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; $test_score</span></span>
<span><span class="co">#&gt; [1] 0.712</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="loading-and-using-the-classifiers">Loading and Using the Classifiers<a class="anchor" aria-label="anchor" href="#loading-and-using-the-classifiers"></a>
</h3>
<div style="text-align: justify;">
<p>After training the text classification model, the resulting
classifier will already be stored in memory as part of the classifier
variable. It is possible, however, that your Python session exited after
training. If so, you’ll need to load the model into memory with the
following:</p>
</div>
<div class="sourceCode" id="cb91"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">TextClassifier</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_models.html">flair_models</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">TextClassifier</span></span>
<span><span class="va">classifier</span> <span class="op">&lt;-</span> <span class="va">TextClassifier</span><span class="op">$</span><span class="fu">load</span><span class="op">(</span><span class="st">'classifier/best-model.pt'</span><span class="op">)</span></span></code></pre></div>
<p>We import the Sentence object. Now, we can generate predictions on
some example text inputs.</p>
<div class="sourceCode" id="cb92"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span></code></pre></div>
<div class="sourceCode" id="cb93"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">"great"</span><span class="op">)</span></span>
<span><span class="va">classifier</span><span class="op">$</span><span class="fu">predict</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="va">labels</span><span class="op">)</span></span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; 'Sentence[1]: "great"'/'POSITIVE' (1.0)</span></span></code></pre></div>
<div class="sourceCode" id="cb94"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">"sad"</span><span class="op">)</span></span>
<span><span class="va">classifier</span><span class="op">$</span><span class="fu">predict</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="va">labels</span><span class="op">)</span></span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; 'Sentence[1]: "sad"'/'NEGATIVE' (0.6663)</span></span></code></pre></div>
<p> </p>
<hr>
</div>
</div>
<div class="section level2">
<h2 id="training-rnns">Training RNNs<a class="anchor" aria-label="anchor" href="#training-rnns"></a>
</h2>
<div style="text-align: justify;">
<p>Here, we train a sentiment analysis model to categorize text. In this
case, we also include a pipeline that implements the use of Recurrent
Neural Networks (RNN). This makes them particularly effective for tasks
involving sequential data. This section also show you how to implement
one of most powerful features in flaiR, stacked embeddings. You can
stack multiple embeddings with different layers and let the classifier
learn from different types of features. In Flair NLP, and with the
<strong>flaiR</strong> package, it’s very easy to accomplish this
task.</p>
</div>
<div class="section level3">
<h3 id="import-necessary-modules">Import Necessary Modules<a class="anchor" aria-label="anchor" href="#import-necessary-modules"></a>
</h3>
<div class="sourceCode" id="cb95"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://davidycliao.github.io/flaiR">flaiR</a></span><span class="op">)</span></span>
<span><span class="va">WordEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">WordEmbeddings</span></span>
<span><span class="va">FlairEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">FlairEmbeddings</span></span>
<span><span class="va">DocumentRNNEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">DocumentRNNEmbeddings</span></span>
<span><span class="va">TextClassifier</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_models.html">flair_models</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">TextClassifier</span></span>
<span><span class="va">ModelTrainer</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_trainers.html">flair_trainers</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">ModelTrainer</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="get-the-imdb-corpus">Get the IMDB Corpus<a class="anchor" aria-label="anchor" href="#get-the-imdb-corpus"></a>
</h3>
<div style="text-align: justify;">
<p>The IMDB movie review dataset is used here, which is a commonly
utilized dataset for sentiment analysis. <code>$downsample(0.1)</code>
method means only 10% of the dataset is used, allowing for a faster
demonstration.</p>
</div>
<div class="sourceCode" id="cb96"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># load the IMDB file and downsize it to 0.1</span></span>
<span><span class="va">IMDB</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_datasets.html">flair_datasets</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">IMDB</span></span>
<span><span class="va">corpus</span> <span class="op">&lt;-</span> <span class="fu">IMDB</span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="fu">downsample</span><span class="op">(</span><span class="fl">0.1</span><span class="op">)</span> </span>
<span><span class="co">#&gt; 2025-01-19 02:04:38,929 Reading data from /Users/yenchiehliao/.flair/datasets/imdb_v4-rebalanced</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:38,930 Train: /Users/yenchiehliao/.flair/datasets/imdb_v4-rebalanced/train.txt</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:38,930 Dev: None</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:38,930 Test: None</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:39,546 No test split found. Using 10% (i.e. 5000 samples) of the train split as test data</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:39,561 No dev split found. Using 10% (i.e. 4500 samples) of the train split as dev data</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:39,561 Initialized corpus /Users/yenchiehliao/.flair/datasets/imdb_v4-rebalanced (label type name is 'sentiment')</span></span>
<span><span class="co"># create the label dictionary</span></span>
<span><span class="va">lbl_type</span> <span class="op">&lt;-</span> <span class="st">'sentiment'</span></span>
<span><span class="va">label_dict</span> <span class="op">&lt;-</span> <span class="va">corpus</span><span class="op">$</span><span class="fu">make_label_dictionary</span><span class="op">(</span>label_type<span class="op">=</span><span class="va">lbl_type</span><span class="op">)</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:39,579 Computing label dictionary. Progress:</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:49,332 Dictionary created for label 'sentiment' with 2 values: POSITIVE (seen 2056 times), NEGATIVE (seen 1994 times)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="stacked-embeddings">Stacked Embeddings<a class="anchor" aria-label="anchor" href="#stacked-embeddings"></a>
</h3>
<div style="text-align: justify;">
<p>This is one of Flair’s most powerful features: it allows for the
integration of embeddings to enable the model to learn from more sparse
features. Three types of embeddings are utilized here: GloVe embeddings,
and two types of Flair embeddings (forward and backward). Word
embeddings are used to convert words into vectors.</p>
</div>
<div class="sourceCode" id="cb97"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># make a list of word embeddings</span></span>
<span><span class="va">word_embeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="fu">WordEmbeddings</span><span class="op">(</span><span class="st">'glove'</span><span class="op">)</span>,</span>
<span>                        <span class="fu">FlairEmbeddings</span><span class="op">(</span><span class="st">'news-forward-fast'</span><span class="op">)</span>,</span>
<span>                        <span class="fu">FlairEmbeddings</span><span class="op">(</span><span class="st">'news-backward-fast'</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># initialize the document embeddings</span></span>
<span><span class="va">document_embeddings</span> <span class="op">&lt;-</span> <span class="fu">DocumentRNNEmbeddings</span><span class="op">(</span><span class="va">word_embeddings</span>, </span>
<span>                                             hidden_size <span class="op">=</span> <span class="fl">512L</span>,</span>
<span>                                             reproject_words <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                                             reproject_words_dimension <span class="op">=</span> <span class="fl">256L</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb98"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># create a Text Classifier with the embeddings and label dictionary</span></span>
<span><span class="va">classifier</span> <span class="op">&lt;-</span> <span class="fu">TextClassifier</span><span class="op">(</span><span class="va">document_embeddings</span>, </span>
<span>                            label_dictionary<span class="op">=</span><span class="va">label_dict</span>, label_type<span class="op">=</span><span class="st">'class'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># initialize the text classifier trainer with our corpus</span></span>
<span><span class="va">trainer</span> <span class="op">&lt;-</span> <span class="fu">ModelTrainer</span><span class="op">(</span><span class="va">classifier</span>, <span class="va">corpus</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="start-the-training">Start the Training<a class="anchor" aria-label="anchor" href="#start-the-training"></a>
</h3>
<div style="text-align: justify;">
<p>For the sake of this example, setting max_epochs to 5. You might want
to increase this for better performance.</p>
<p>It is worth noting that the learning rate is a parameter that
determines the step size at each iteration while moving towards a
minimum of the loss function. A smaller learning rate could slow down
the learning process, but it could lead to more precise convergence.
<code>mini_batch_size</code> determines the number of samples that will
be used to compute the gradient at each step. The ‘L’ in 32L is used in
R to denote that the number is an integer.</p>
<p><code>patience</code> (aka early stop) is a hyper-parameter used in
conjunction with early stopping to avoid overfitting. It determines the
number of epochs the training process will tolerate without improvements
before stopping the training. Setting max_epochs to 5 means the
algorithm will make five passes through the dataset.</p>
</div>
<div class="sourceCode" id="cb99"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># note: the 'L' in 32L is used in R to denote that the number is an integer.</span></span>
<span><span class="va">trainer</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="st">'models/sentiment'</span>,</span>
<span>              learning_rate<span class="op">=</span><span class="fl">0.1</span>,</span>
<span>              mini_batch_size<span class="op">=</span><span class="fl">32L</span>,</span>
<span>              patience<span class="op">=</span><span class="fl">5L</span>,</span>
<span>              max_epochs<span class="op">=</span><span class="fl">5L</span><span class="op">)</span>  </span></code></pre></div>
</div>
<div class="section level3">
<h3 id="to-apply-the-trained-model-for-prediction">To Apply the Trained Model for Prediction<a class="anchor" aria-label="anchor" href="#to-apply-the-trained-model-for-prediction"></a>
</h3>
<div class="sourceCode" id="cb100"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="st">"This movie was really exciting!"</span></span>
<span><span class="va">classifier</span><span class="op">$</span><span class="fu">predict</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence.labels</span><span class="op">)</span></span></code></pre></div>
<hr>
</div>
</div>
<div class="section level2">
<h2 id="finetune-transformers">Finetune Transformers<a class="anchor" aria-label="anchor" href="#finetune-transformers"></a>
</h2>
<div style="text-align: justify;">
<p>We use data from <em>The Temporal Focus of Campaign Communication
(2020 JOP)</em> as an example. Let’s assume we receive the data for
training from different times. First, suppose you have a dataset of 1000
entries called <code>cc_muller_old</code>. On another day, with the help
of nice friends, you receive another set of data, adding 2000 entries in
a dataset called <code>cc_muller_new</code>. Both subsets are from
<code>data(cc_muller)</code>. We will show how to fine-tune a
transformer model with <code>cc_muller_old</code>, and then continue
with another round of fine-tuning using <code>cc_muller_new</code>.</p>
</div>
<div class="sourceCode" id="cb101"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://davidycliao.github.io/flaiR">flaiR</a></span><span class="op">)</span></span></code></pre></div>
<div class="section level3">
<h3 id="fine-tuning-a-transformers-model">Fine-tuning a Transformers Model<a class="anchor" aria-label="anchor" href="#fine-tuning-a-transformers-model"></a>
</h3>
<p><u><strong>Step 1</strong></u> Load Necessary Modules from Flair</p>
<p>Load necessary classes from <code>flair</code> package.</p>
<div class="sourceCode" id="cb102"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Sentence is a class for holding a text sentence</span></span>
<span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span>
<span></span>
<span><span class="co"># Corpus is a class for text corpora</span></span>
<span><span class="va">Corpus</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Corpus</span></span>
<span></span>
<span><span class="co"># TransformerDocumentEmbeddings is a class for loading transformer </span></span>
<span><span class="va">TransformerDocumentEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">TransformerDocumentEmbeddings</span></span>
<span></span>
<span><span class="co"># TextClassifier is a class for text classification</span></span>
<span><span class="va">TextClassifier</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_models.html">flair_models</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">TextClassifier</span></span>
<span></span>
<span><span class="co"># ModelTrainer is a class for training and evaluating models</span></span>
<span><span class="va">ModelTrainer</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_trainers.html">flair_trainers</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">ModelTrainer</span></span></code></pre></div>
<div style="text-align: justify;">
<p>We use purrr to help us split sentences using Sentence from
<code><a href="../reference/flair_data.html">flair_data()</a></code>, then use map2 to add labels, and finally use
<code>Corpus</code> to segment the data.</p>
</div>
<div class="sourceCode" id="cb103"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://purrr.tidyverse.org/" class="external-link">purrr</a></span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="va">cc_muller</span><span class="op">)</span></span>
<span><span class="va">cc_muller_old</span> <span class="op">&lt;-</span> <span class="va">cc_muller</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">1000</span>,<span class="op">]</span></span>
<span></span>
<span><span class="va">old_text</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://purrr.tidyverse.org/reference/map.html" class="external-link">map</a></span><span class="op">(</span><span class="va">cc_muller_old</span><span class="op">$</span><span class="va">text</span>, <span class="va">Sentence</span><span class="op">)</span></span>
<span><span class="va">old_labels</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/character.html" class="external-link">as.character</a></span><span class="op">(</span><span class="va">cc_muller_old</span><span class="op">$</span><span class="va">class</span><span class="op">)</span></span>
<span></span>
<span><span class="va">old_text</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://purrr.tidyverse.org/reference/map2.html" class="external-link">map2</a></span><span class="op">(</span><span class="va">old_text</span>, <span class="va">old_labels</span>, <span class="op">~</span> <span class="op">{</span></span>
<span>  <span class="va">.x</span><span class="op">$</span><span class="fu">add_label</span><span class="op">(</span><span class="st">"classification"</span>, <span class="va">.y</span><span class="op">)</span></span>
<span>  <span class="va">.x</span></span>
<span><span class="op">}</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb104"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">old_text</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 1000</span></span></code></pre></div>
<div class="sourceCode" id="cb105"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">2046</span><span class="op">)</span></span>
<span><span class="va">sample</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html" class="external-link">sample</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="cn">TRUE</span>, <span class="cn">FALSE</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">old_text</span><span class="op">)</span>, replace<span class="op">=</span><span class="cn">TRUE</span>, prob<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0.8</span>, <span class="fl">0.2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">old_train</span>  <span class="op">&lt;-</span> <span class="va">old_text</span><span class="op">[</span><span class="va">sample</span><span class="op">]</span></span>
<span><span class="va">old_test</span>   <span class="op">&lt;-</span> <span class="va">old_text</span><span class="op">[</span><span class="op">!</span><span class="va">sample</span><span class="op">]</span></span>
<span></span>
<span><span class="va">test_id</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html" class="external-link">sample</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="cn">TRUE</span>, <span class="cn">FALSE</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">old_test</span><span class="op">)</span>, replace<span class="op">=</span><span class="cn">TRUE</span>, prob<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0.5</span>, <span class="fl">0.5</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">old_test</span>   <span class="op">&lt;-</span> <span class="va">old_test</span><span class="op">[</span><span class="va">test_id</span><span class="op">]</span></span>
<span><span class="va">old_dev</span>   <span class="op">&lt;-</span> <span class="va">old_test</span><span class="op">[</span><span class="op">!</span><span class="va">test_id</span><span class="op">]</span></span></code></pre></div>
<div style="text-align: justify;">
<p>If you do not provide a development set (dev set) while using Flair,
it will automatically split the training data into training and
development datasets. The test set is used for training the model and
evaluating its final performance, whereas the development set is used
for adjusting model parameters and preventing overfitting, or in other
words, for early stopping of the model.</p>
</div>
<div class="sourceCode" id="cb106"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">old_corpus</span> <span class="op">&lt;-</span> <span class="fu">Corpus</span><span class="op">(</span>train <span class="op">=</span> <span class="va">old_train</span>, test <span class="op">=</span> <span class="va">old_test</span><span class="op">)</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:52,042 No dev split found. Using 10% (i.e. 80 samples) of the train split as dev data</span></span></code></pre></div>
<p><u><strong>Step 3</strong></u> Load <code>distilbert</code>
Transformer</p>
<div class="sourceCode" id="cb107"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">document_embeddings</span> <span class="op">&lt;-</span> <span class="fu">TransformerDocumentEmbeddings</span><span class="op">(</span><span class="st">'distilbert-base-uncased'</span>, fine_tune<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<div style="text-align: justify;">
<p>First, the <code>$make_label_dictionary</code> function is used to
automatically create a label dictionary for the classification task. The
label dictionary is a mapping from label to index, which is used to map
the labels to a tensor of label indices. Besides classification tasks,
flaiR also supports other label types for training custom model. From
the cc_muller dataset: Future (seen 423 times), Present (seen 262
times), Past (seen 131 times).</p>
</div>
<div class="sourceCode" id="cb108"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">old_label_dict</span> <span class="op">&lt;-</span> <span class="va">old_corpus</span><span class="op">$</span><span class="fu">make_label_dictionary</span><span class="op">(</span>label_type<span class="op">=</span><span class="st">"classification"</span><span class="op">)</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:53,249 Computing label dictionary. Progress:</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:53,258 Dictionary created for label 'classification' with 3 values: Future (seen 380 times), Present (seen 232 times), Past (seen 111 times)</span></span></code></pre></div>
<div style="text-align: justify;">
<p><code>TextClassifier</code> is used to create a text classifier. The
classifier takes the document embeddings (importing from
<code>'distilbert-base-uncased'</code> from Hugging Face) and the label
dictionary as input. The label type is also specified as
classification.</p>
</div>
<div class="sourceCode" id="cb109"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">old_classifier</span> <span class="op">&lt;-</span> <span class="fu">TextClassifier</span><span class="op">(</span><span class="va">document_embeddings</span>,</span>
<span>                                 label_dictionary <span class="op">=</span> <span class="va">old_label_dict</span>, </span>
<span>                                 label_type<span class="op">=</span><span class="st">'classification'</span><span class="op">)</span></span></code></pre></div>
<p><u><strong>Step 4</strong></u> Start Training</p>
<p><code>ModelTrainer</code> is used to train the model.</p>
<div class="sourceCode" id="cb110"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">old_trainer</span> <span class="op">&lt;-</span> <span class="fu">ModelTrainer</span><span class="op">(</span>model <span class="op">=</span> <span class="va">old_classifier</span>, corpus <span class="op">=</span> <span class="va">old_corpus</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb111"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">old_trainer</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="st">"vignettes/inst/muller-campaign-communication"</span>,  </span>
<span>                  learning_rate<span class="op">=</span><span class="fl">0.02</span>,              </span>
<span>                  mini_batch_size<span class="op">=</span><span class="fl">8L</span>,              </span>
<span>                  anneal_with_restarts <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                  save_final_model<span class="op">=</span><span class="cn">TRUE</span>,</span>
<span>                  max_epochs<span class="op">=</span><span class="fl">1L</span><span class="op">)</span>   </span>
<span><span class="co">#&gt; 2025-01-19 02:04:53,390 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:53,390 Model: "TextClassifier(</span></span>
<span><span class="co">#&gt;   (embeddings): TransformerDocumentEmbeddings(</span></span>
<span><span class="co">#&gt;     (model): DistilBertModel(</span></span>
<span><span class="co">#&gt;       (embeddings): Embeddings(</span></span>
<span><span class="co">#&gt;         (word_embeddings): Embedding(30523, 768, padding_idx=0)</span></span>
<span><span class="co">#&gt;         (position_embeddings): Embedding(512, 768)</span></span>
<span><span class="co">#&gt;         (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)</span></span>
<span><span class="co">#&gt;         (dropout): Dropout(p=0.1, inplace=False)</span></span>
<span><span class="co">#&gt;       )</span></span>
<span><span class="co">#&gt;       (transformer): Transformer(</span></span>
<span><span class="co">#&gt;         (layer): ModuleList(</span></span>
<span><span class="co">#&gt;           (0-5): 6 x TransformerBlock(</span></span>
<span><span class="co">#&gt;             (attention): DistilBertSdpaAttention(</span></span>
<span><span class="co">#&gt;               (dropout): Dropout(p=0.1, inplace=False)</span></span>
<span><span class="co">#&gt;               (q_lin): Linear(in_features=768, out_features=768, bias=True)</span></span>
<span><span class="co">#&gt;               (k_lin): Linear(in_features=768, out_features=768, bias=True)</span></span>
<span><span class="co">#&gt;               (v_lin): Linear(in_features=768, out_features=768, bias=True)</span></span>
<span><span class="co">#&gt;               (out_lin): Linear(in_features=768, out_features=768, bias=True)</span></span>
<span><span class="co">#&gt;             )</span></span>
<span><span class="co">#&gt;             (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)</span></span>
<span><span class="co">#&gt;             (ffn): FFN(</span></span>
<span><span class="co">#&gt;               (dropout): Dropout(p=0.1, inplace=False)</span></span>
<span><span class="co">#&gt;               (lin1): Linear(in_features=768, out_features=3072, bias=True)</span></span>
<span><span class="co">#&gt;               (lin2): Linear(in_features=3072, out_features=768, bias=True)</span></span>
<span><span class="co">#&gt;               (activation): GELUActivation()</span></span>
<span><span class="co">#&gt;             )</span></span>
<span><span class="co">#&gt;             (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)</span></span>
<span><span class="co">#&gt;           )</span></span>
<span><span class="co">#&gt;         )</span></span>
<span><span class="co">#&gt;       )</span></span>
<span><span class="co">#&gt;     )</span></span>
<span><span class="co">#&gt;   )</span></span>
<span><span class="co">#&gt;   (decoder): Linear(in_features=768, out_features=3, bias=True)</span></span>
<span><span class="co">#&gt;   (dropout): Dropout(p=0.0, inplace=False)</span></span>
<span><span class="co">#&gt;   (locked_dropout): LockedDropout(p=0.0)</span></span>
<span><span class="co">#&gt;   (word_dropout): WordDropout(p=0.0)</span></span>
<span><span class="co">#&gt;   (loss_function): CrossEntropyLoss()</span></span>
<span><span class="co">#&gt;   (weights): None</span></span>
<span><span class="co">#&gt;   (weight_tensor) None</span></span>
<span><span class="co">#&gt; )"</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:53,390 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:53,390 Corpus: 723 train + 80 dev + 85 test sentences</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:53,390 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:53,390 Train:  723 sentences</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:53,390         (train_with_dev=False, train_with_test=False)</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:53,390 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:53,390 Training Params:</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:53,390  - learning_rate: "0.02" </span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:53,390  - mini_batch_size: "8"</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:53,391  - max_epochs: "1"</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:53,391  - shuffle: "True"</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:53,391 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:53,391 Plugins:</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:53,391  - AnnealOnPlateau | patience: '3', anneal_factor: '0.5', min_learning_rate: '0.0001'</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:53,391 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:53,391 Final evaluation on model from best epoch (best-model.pt)</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:53,391  - metric: "('micro avg', 'f1-score')"</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:53,391 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:53,391 Computation:</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:53,391  - compute on device: cpu</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:53,391  - embedding storage: cpu</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:53,391 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:53,391 Model training base path: "vignettes/inst/muller-campaign-communication"</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:53,391 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:53,391 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:56,642 epoch 1 - iter 9/91 - loss 1.15913804 - time (sec): 3.25 - samples/sec: 22.15 - lr: 0.020000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:04:59,345 epoch 1 - iter 18/91 - loss 1.09388583 - time (sec): 5.95 - samples/sec: 24.19 - lr: 0.020000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:02,125 epoch 1 - iter 27/91 - loss 0.98490597 - time (sec): 8.73 - samples/sec: 24.73 - lr: 0.020000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:04,561 epoch 1 - iter 36/91 - loss 0.90666179 - time (sec): 11.17 - samples/sec: 25.79 - lr: 0.020000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:06,768 epoch 1 - iter 45/91 - loss 0.86723306 - time (sec): 13.38 - samples/sec: 26.91 - lr: 0.020000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:08,690 epoch 1 - iter 54/91 - loss 0.82877369 - time (sec): 15.30 - samples/sec: 28.24 - lr: 0.020000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:10,977 epoch 1 - iter 63/91 - loss 0.82633138 - time (sec): 17.59 - samples/sec: 28.66 - lr: 0.020000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:13,127 epoch 1 - iter 72/91 - loss 0.77545721 - time (sec): 19.74 - samples/sec: 29.19 - lr: 0.020000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:15,355 epoch 1 - iter 81/91 - loss 0.75503454 - time (sec): 21.96 - samples/sec: 29.50 - lr: 0.020000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:17,573 epoch 1 - iter 90/91 - loss 0.73309229 - time (sec): 24.18 - samples/sec: 29.78 - lr: 0.020000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:17,685 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:17,685 EPOCH 1 done: loss 0.7313 - lr: 0.020000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:18,438 DEV : loss 0.4210357666015625 - f1-score (micro avg)  0.8375</span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:18,440  - 0 epochs without improvement</span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:18,442 saving best model</span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:19,321 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:19,324 Loading model from best epoch ...</span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:21,320 </span></span>
<span><span class="co">#&gt; Results:</span></span>
<span><span class="co">#&gt; - F-score (micro) 0.8235</span></span>
<span><span class="co">#&gt; - F-score (macro) 0.8246</span></span>
<span><span class="co">#&gt; - Accuracy 0.8235</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; By class:</span></span>
<span><span class="co">#&gt;               precision    recall  f1-score   support</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;       Future     0.8125    0.9070    0.8571        43</span></span>
<span><span class="co">#&gt;      Present     0.7826    0.6667    0.7200        27</span></span>
<span><span class="co">#&gt;         Past     0.9286    0.8667    0.8966        15</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     accuracy                         0.8235        85</span></span>
<span><span class="co">#&gt;    macro avg     0.8412    0.8134    0.8246        85</span></span>
<span><span class="co">#&gt; weighted avg     0.8235    0.8235    0.8205        85</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:21,320 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; $test_score</span></span>
<span><span class="co">#&gt; [1] 0.8235294</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="continue-fine-tuning-with-new-dataset">Continue Fine-tuning with New Dataset<a class="anchor" aria-label="anchor" href="#continue-fine-tuning-with-new-dataset"></a>
</h3>
<div style="text-align: justify;">
<p>Now, we can continue to fine tune the already fine tuned model with
an additional 2000 pieces of data. First, let’s say we have another 2000
entries called <code>cc_muller_new</code>. We can fine-tune the previous
model with these 2000 entries. The steps are the same as before. For
this case, we don’t need to split the dataset again. We can use the
entire 2000 entries as the training set and use the
<code>old_test</code> set to evaluate how well our refined model
performs.</p>
</div>
<p><u><strong>Step 1</strong></u> Load the
<code>muller-campaign-communication</code> Model</p>
<p>Load the model (<code>old_model</code>) you have already fine tuned
from previous stage and let’s fine tune it with the new data,
<code>new_corpus</code>.</p>
<div class="sourceCode" id="cb112"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">old_model</span> <span class="op">&lt;-</span> <span class="va">TextClassifier</span><span class="op">$</span><span class="fu">load</span><span class="op">(</span><span class="st">"vignettes/inst/muller-campaign-communication/best-model.pt"</span><span class="op">)</span></span></code></pre></div>
<p><u><strong>Step 2</strong></u> Convert the New Data to Sentence and
Corpus</p>
<div class="sourceCode" id="cb113"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://purrr.tidyverse.org/" class="external-link">purrr</a></span><span class="op">)</span></span>
<span><span class="va">cc_muller_new</span> <span class="op">&lt;-</span> <span class="va">cc_muller</span><span class="op">[</span><span class="fl">1001</span><span class="op">:</span><span class="fl">3000</span>,<span class="op">]</span></span>
<span><span class="va">new_text</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://purrr.tidyverse.org/reference/map.html" class="external-link">map</a></span><span class="op">(</span><span class="va">cc_muller_new</span><span class="op">$</span><span class="va">text</span>, <span class="va">Sentence</span><span class="op">)</span></span>
<span><span class="va">new_labels</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/character.html" class="external-link">as.character</a></span><span class="op">(</span><span class="va">cc_muller_new</span><span class="op">$</span><span class="va">class</span><span class="op">)</span></span>
<span></span>
<span><span class="va">new_text</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://purrr.tidyverse.org/reference/map2.html" class="external-link">map2</a></span><span class="op">(</span><span class="va">new_text</span>, <span class="va">new_labels</span>, <span class="op">~</span> <span class="op">{</span></span>
<span>  <span class="va">.x</span><span class="op">$</span><span class="fu">add_label</span><span class="op">(</span><span class="st">"classification"</span>, <span class="va">.y</span><span class="op">)</span></span>
<span>  <span class="va">.x</span></span>
<span><span class="op">}</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb114"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">new_corpus</span> <span class="op">&lt;-</span> <span class="fu">Corpus</span><span class="op">(</span>train<span class="op">=</span><span class="va">new_text</span>, test<span class="op">=</span><span class="va">old_test</span><span class="op">)</span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:23,437 No dev split found. Using 10% (i.e. 200 samples) of the train split as dev data</span></span></code></pre></div>
<p><u><strong>Step 3</strong></u> Create a New Model Trainer with the
Old Model and New Corpus</p>
<div class="sourceCode" id="cb115"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">new_trainer</span> <span class="op">&lt;-</span> <span class="fu">ModelTrainer</span><span class="op">(</span><span class="va">old_model</span>, <span class="va">new_corpus</span><span class="op">)</span></span></code></pre></div>
<p><u><strong>Step 4</strong></u> Train the New Model</p>
<div class="sourceCode" id="cb116"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">new_trainer</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="st">"vignettes/inst/new-muller-campaign-communication"</span>,</span>
<span>                  learning_rate<span class="op">=</span><span class="fl">0.002</span>, </span>
<span>                  mini_batch_size<span class="op">=</span><span class="fl">8L</span>,  </span>
<span>                  max_epochs<span class="op">=</span><span class="fl">1L</span><span class="op">)</span>    </span>
<span><span class="co">#&gt; 2025-01-19 02:05:23,525 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:23,526 Model: "TextClassifier(</span></span>
<span><span class="co">#&gt;   (embeddings): TransformerDocumentEmbeddings(</span></span>
<span><span class="co">#&gt;     (model): DistilBertModel(</span></span>
<span><span class="co">#&gt;       (embeddings): Embeddings(</span></span>
<span><span class="co">#&gt;         (word_embeddings): Embedding(30523, 768, padding_idx=0)</span></span>
<span><span class="co">#&gt;         (position_embeddings): Embedding(512, 768)</span></span>
<span><span class="co">#&gt;         (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)</span></span>
<span><span class="co">#&gt;         (dropout): Dropout(p=0.1, inplace=False)</span></span>
<span><span class="co">#&gt;       )</span></span>
<span><span class="co">#&gt;       (transformer): Transformer(</span></span>
<span><span class="co">#&gt;         (layer): ModuleList(</span></span>
<span><span class="co">#&gt;           (0-5): 6 x TransformerBlock(</span></span>
<span><span class="co">#&gt;             (attention): DistilBertSdpaAttention(</span></span>
<span><span class="co">#&gt;               (dropout): Dropout(p=0.1, inplace=False)</span></span>
<span><span class="co">#&gt;               (q_lin): Linear(in_features=768, out_features=768, bias=True)</span></span>
<span><span class="co">#&gt;               (k_lin): Linear(in_features=768, out_features=768, bias=True)</span></span>
<span><span class="co">#&gt;               (v_lin): Linear(in_features=768, out_features=768, bias=True)</span></span>
<span><span class="co">#&gt;               (out_lin): Linear(in_features=768, out_features=768, bias=True)</span></span>
<span><span class="co">#&gt;             )</span></span>
<span><span class="co">#&gt;             (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)</span></span>
<span><span class="co">#&gt;             (ffn): FFN(</span></span>
<span><span class="co">#&gt;               (dropout): Dropout(p=0.1, inplace=False)</span></span>
<span><span class="co">#&gt;               (lin1): Linear(in_features=768, out_features=3072, bias=True)</span></span>
<span><span class="co">#&gt;               (lin2): Linear(in_features=3072, out_features=768, bias=True)</span></span>
<span><span class="co">#&gt;               (activation): GELUActivation()</span></span>
<span><span class="co">#&gt;             )</span></span>
<span><span class="co">#&gt;             (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)</span></span>
<span><span class="co">#&gt;           )</span></span>
<span><span class="co">#&gt;         )</span></span>
<span><span class="co">#&gt;       )</span></span>
<span><span class="co">#&gt;     )</span></span>
<span><span class="co">#&gt;   )</span></span>
<span><span class="co">#&gt;   (decoder): Linear(in_features=768, out_features=3, bias=True)</span></span>
<span><span class="co">#&gt;   (dropout): Dropout(p=0.0, inplace=False)</span></span>
<span><span class="co">#&gt;   (locked_dropout): LockedDropout(p=0.0)</span></span>
<span><span class="co">#&gt;   (word_dropout): WordDropout(p=0.0)</span></span>
<span><span class="co">#&gt;   (loss_function): CrossEntropyLoss()</span></span>
<span><span class="co">#&gt;   (weights): None</span></span>
<span><span class="co">#&gt;   (weight_tensor) None</span></span>
<span><span class="co">#&gt; )"</span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:23,526 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:23,526 Corpus: 1800 train + 200 dev + 85 test sentences</span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:23,526 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:23,526 Train:  1800 sentences</span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:23,526         (train_with_dev=False, train_with_test=False)</span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:23,526 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:23,526 Training Params:</span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:23,526  - learning_rate: "0.002" </span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:23,526  - mini_batch_size: "8"</span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:23,526  - max_epochs: "1"</span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:23,526  - shuffle: "True"</span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:23,526 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:23,526 Plugins:</span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:23,526  - AnnealOnPlateau | patience: '3', anneal_factor: '0.5', min_learning_rate: '0.0001'</span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:23,526 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:23,527 Final evaluation on model from best epoch (best-model.pt)</span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:23,527  - metric: "('micro avg', 'f1-score')"</span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:23,527 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:23,527 Computation:</span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:23,527  - compute on device: cpu</span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:23,527  - embedding storage: cpu</span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:23,527 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:23,527 Model training base path: "vignettes/inst/new-muller-campaign-communication"</span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:23,527 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:23,527 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:29,093 epoch 1 - iter 22/225 - loss 0.47686889 - time (sec): 5.57 - samples/sec: 31.62 - lr: 0.002000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:35,058 epoch 1 - iter 44/225 - loss 0.47116109 - time (sec): 11.53 - samples/sec: 30.53 - lr: 0.002000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:40,434 epoch 1 - iter 66/225 - loss 0.45405770 - time (sec): 16.91 - samples/sec: 31.23 - lr: 0.002000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:46,206 epoch 1 - iter 88/225 - loss 0.42943539 - time (sec): 22.68 - samples/sec: 31.04 - lr: 0.002000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:51,013 epoch 1 - iter 110/225 - loss 0.41669404 - time (sec): 27.49 - samples/sec: 32.02 - lr: 0.002000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:05:56,094 epoch 1 - iter 132/225 - loss 0.41539682 - time (sec): 32.57 - samples/sec: 32.43 - lr: 0.002000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:06:01,879 epoch 1 - iter 154/225 - loss 0.41510456 - time (sec): 38.35 - samples/sec: 32.12 - lr: 0.002000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:06:07,257 epoch 1 - iter 176/225 - loss 0.41127963 - time (sec): 43.73 - samples/sec: 32.20 - lr: 0.002000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:06:12,124 epoch 1 - iter 198/225 - loss 0.40708528 - time (sec): 48.60 - samples/sec: 32.59 - lr: 0.002000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:06:17,665 epoch 1 - iter 220/225 - loss 0.39587183 - time (sec): 54.14 - samples/sec: 32.51 - lr: 0.002000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:06:19,026 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:06:19,027 EPOCH 1 done: loss 0.3941 - lr: 0.002000</span></span>
<span><span class="co">#&gt; 2025-01-19 02:06:21,162 DEV : loss 0.40370461344718933 - f1-score (micro avg)  0.85</span></span>
<span><span class="co">#&gt; 2025-01-19 02:06:21,167  - 0 epochs without improvement</span></span>
<span><span class="co">#&gt; 2025-01-19 02:06:21,168 saving best model</span></span>
<span><span class="co">#&gt; 2025-01-19 02:06:22,009 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2025-01-19 02:06:22,009 Loading model from best epoch ...</span></span>
<span><span class="co">#&gt; 2025-01-19 02:06:23,943 </span></span>
<span><span class="co">#&gt; Results:</span></span>
<span><span class="co">#&gt; - F-score (micro) 0.8706</span></span>
<span><span class="co">#&gt; - F-score (macro) 0.8779</span></span>
<span><span class="co">#&gt; - Accuracy 0.8706</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; By class:</span></span>
<span><span class="co">#&gt;               precision    recall  f1-score   support</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;       Future     0.8837    0.8837    0.8837        43</span></span>
<span><span class="co">#&gt;      Present     0.7931    0.8519    0.8214        27</span></span>
<span><span class="co">#&gt;         Past     1.0000    0.8667    0.9286        15</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     accuracy                         0.8706        85</span></span>
<span><span class="co">#&gt;    macro avg     0.8923    0.8674    0.8779        85</span></span>
<span><span class="co">#&gt; weighted avg     0.8755    0.8706    0.8718        85</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; 2025-01-19 02:06:23,943 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; $test_score</span></span>
<span><span class="co">#&gt; [1] 0.8705882</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="model-performance-metrics-pre-and-post-fine-tuning">Model Performance Metrics: Pre and Post Fine-tuning<a class="anchor" aria-label="anchor" href="#model-performance-metrics-pre-and-post-fine-tuning"></a>
</h3>
<p>After fine-tuning for 1 epoch, the model showed improved performance
on the same test set.</p>
<table class="table">
<thead><tr class="header">
<th>Evaluation Metric</th>
<th>Pre-finetune</th>
<th>Post-finetune</th>
<th>Improvement</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>F-score (micro)</td>
<td>0.7294</td>
<td>0.8471</td>
<td>+0.1177</td>
</tr>
<tr class="even">
<td>F-score (macro)</td>
<td>0.7689</td>
<td>0.8583</td>
<td>+0.0894</td>
</tr>
<tr class="odd">
<td>Accuracy</td>
<td>0.7294</td>
<td>0.8471</td>
<td>+0.1177</td>
</tr>
</tbody>
</table>
<p>More R tutorial and documentation see <a href="https://github.com/davidycliao/flaiR" class="external-link">here</a>.</p>
</div>
<div class="section level3">
<h3 id="using-your-own-fine-tuned-model-in-flair">Using Your Own Fine-tuned Model in flaiR<a class="anchor" aria-label="anchor" href="#using-your-own-fine-tuned-model-in-flair"></a>
</h3>
<p>This seciton demonstrates how to utilize your custom fine-tuned model
in flaiR for text classification tasks. Let’s explore this process step
by step.</p>
<p><u><strong>Setting Up Your Environment</strong></u></p>
<p>First, we need to load the flaiR package and prepare our model:</p>
<div class="sourceCode" id="cb117"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://davidycliao.github.io/flaiR">flaiR</a></span><span class="op">)</span></span>
<span><span class="va">classifier</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_models.html">flair_models</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">TextClassifier</span><span class="op">$</span><span class="fu">load</span><span class="op">(</span><span class="st">'vignettes/inst/new-muller-campaign-communication/best-model.pt'</span><span class="op">)</span></span></code></pre></div>
<p>It’s important to verify your model’s compatibility with
<code>$model_card</code>. You can check this by examining the version
requirements:</p>
<div class="sourceCode" id="cb118"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">classifier</span><span class="op">$</span><span class="va">model_card</span><span class="op">)</span></span>
<span><span class="co">#&gt; $flair_version</span></span>
<span><span class="co">#&gt; [1] "0.15.0"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $pytorch_version</span></span>
<span><span class="co">#&gt; [1] "2.5.1"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $transformers_version</span></span>
<span><span class="co">#&gt; [1] "4.48.0"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $training_parameters</span></span>
<span><span class="co">#&gt; $training_parameters$base_path</span></span>
<span><span class="co">#&gt; PosixPath('vignettes/inst/new-muller-campaign-communication')</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $training_parameters$learning_rate</span></span>
<span><span class="co">#&gt; [1] 0.002</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $training_parameters$decoder_learning_rate</span></span>
<span><span class="co">#&gt; NULL</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $training_parameters$mini_batch_size</span></span>
<span><span class="co">#&gt; [1] 8</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $training_parameters$eval_batch_size</span></span>
<span><span class="co">#&gt; [1] 64</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $training_parameters$mini_batch_chunk_size</span></span>
<span><span class="co">#&gt; NULL</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $training_parameters$max_epochs</span></span>
<span><span class="co">#&gt; [1] 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $training_parameters$optimizer</span></span>
<span><span class="co">#&gt; [1] "torch.optim.sgd.SGD"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $training_parameters$train_with_dev</span></span>
<span><span class="co">#&gt; [1] FALSE</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $training_parameters$train_with_test</span></span>
<span><span class="co">#&gt; [1] FALSE</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $training_parameters$max_grad_norm</span></span>
<span><span class="co">#&gt; [1] 5</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $training_parameters$reduce_transformer_vocab</span></span>
<span><span class="co">#&gt; [1] FALSE</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $training_parameters$main_evaluation_metric</span></span>
<span><span class="co">#&gt; $training_parameters$main_evaluation_metric[[1]]</span></span>
<span><span class="co">#&gt; [1] "micro avg"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $training_parameters$main_evaluation_metric[[2]]</span></span>
<span><span class="co">#&gt; [1] "f1-score"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $training_parameters$monitor_test</span></span>
<span><span class="co">#&gt; [1] FALSE</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $training_parameters$monitor_train_sample</span></span>
<span><span class="co">#&gt; [1] 0</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $training_parameters$use_final_model_for_eval</span></span>
<span><span class="co">#&gt; [1] FALSE</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $training_parameters$gold_label_dictionary_for_eval</span></span>
<span><span class="co">#&gt; NULL</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $training_parameters$exclude_labels</span></span>
<span><span class="co">#&gt; list()</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $training_parameters$sampler</span></span>
<span><span class="co">#&gt; NULL</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $training_parameters$shuffle</span></span>
<span><span class="co">#&gt; [1] TRUE</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $training_parameters$shuffle_first_epoch</span></span>
<span><span class="co">#&gt; [1] TRUE</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $training_parameters$embeddings_storage_mode</span></span>
<span><span class="co">#&gt; [1] "cpu"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $training_parameters$epoch</span></span>
<span><span class="co">#&gt; [1] 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $training_parameters$save_final_model</span></span>
<span><span class="co">#&gt; [1] TRUE</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $training_parameters$save_optimizer_state</span></span>
<span><span class="co">#&gt; [1] FALSE</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $training_parameters$save_model_each_k_epochs</span></span>
<span><span class="co">#&gt; [1] 0</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $training_parameters$create_file_logs</span></span>
<span><span class="co">#&gt; [1] TRUE</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $training_parameters$create_loss_file</span></span>
<span><span class="co">#&gt; [1] TRUE</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $training_parameters$write_weights</span></span>
<span><span class="co">#&gt; [1] FALSE</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $training_parameters$use_amp</span></span>
<span><span class="co">#&gt; [1] FALSE</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $training_parameters$multi_gpu</span></span>
<span><span class="co">#&gt; [1] FALSE</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $training_parameters$plugins</span></span>
<span><span class="co">#&gt; $training_parameters$plugins[[1]]</span></span>
<span><span class="co">#&gt; $training_parameters$plugins[[1]]$`__cls__`</span></span>
<span><span class="co">#&gt; [1] "flair.trainers.plugins.functional.anneal_on_plateau.AnnealingPlugin"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $training_parameters$plugins[[1]]$base_path</span></span>
<span><span class="co">#&gt; [1] "vignettes/inst/new-muller-campaign-communication"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $training_parameters$plugins[[1]]$min_learning_rate</span></span>
<span><span class="co">#&gt; [1] 1e-04</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $training_parameters$plugins[[1]]$anneal_factor</span></span>
<span><span class="co">#&gt; [1] 0.5</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $training_parameters$plugins[[1]]$patience</span></span>
<span><span class="co">#&gt; [1] 3</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $training_parameters$plugins[[1]]$initial_extra_patience</span></span>
<span><span class="co">#&gt; [1] 0</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $training_parameters$plugins[[1]]$anneal_with_restarts</span></span>
<span><span class="co">#&gt; [1] FALSE</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $training_parameters$kwargs</span></span>
<span><span class="co">#&gt; $training_parameters$kwargs$lr</span></span>
<span><span class="co">#&gt; [1] 0.002</span></span></code></pre></div>
<div class="sourceCode" id="cb119"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Check required versions</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">classifier</span><span class="op">$</span><span class="va">model_card</span><span class="op">$</span><span class="va">transformers_version</span><span class="op">)</span>  <span class="co"># Required transformers version</span></span>
<span><span class="co">#&gt; [1] "4.48.0"</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">classifier</span><span class="op">$</span><span class="va">model_card</span><span class="op">$</span><span class="va">flair_version</span><span class="op">)</span>         <span class="co"># Required Flair version</span></span>
<span><span class="co">#&gt; [1] "0.15.0"</span></span></code></pre></div>
<p><u><strong>Making Predictions</strong></u></p>
<p>To make predictions, we first need to prepare our text by creating a
Sentence object. This is a key component in Flair’s architecture that
handles text processing:</p>
<div class="sourceCode" id="cb120"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Get the Sentence class from flaiR</span></span>
<span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span>
<span></span>
<span><span class="co"># Create a Sentence object with your text</span></span>
<span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">"And to boost the housing we need, we will start to build a new generation of garden cities."</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Make prediction</span></span>
<span><span class="va">classifier</span><span class="op">$</span><span class="fu">predict</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span></code></pre></div>
<p><u><strong>Access Prediction Results</strong></u></p>
<p>Get predicted label</p>
<div class="sourceCode" id="cb121"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">prediction</span> <span class="op">&lt;-</span> <span class="va">sentence</span><span class="op">$</span><span class="va">labels</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">$</span><span class="va">value</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">prediction</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "Future"</span></span></code></pre></div>
<p>Get confidence score</p>
<div class="sourceCode" id="cb122"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">confidence</span> <span class="op">&lt;-</span> <span class="va">sentence</span><span class="op">$</span><span class="va">labels</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">$</span><span class="va">score</span> </span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">confidence</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.9971271</span></span></code></pre></div>
<p> </p>
<hr>
</div>
</div>
<div class="section level2">
<h2 id="extending-contexts-embedding-regression">Extending conText’s Embedding Regression<a class="anchor" aria-label="anchor" href="#extending-contexts-embedding-regression"></a>
</h2>
<div style="text-align: justify;">
<p><code>ConText</code> is a fast, flexible, and transparent framework
for estimating context-specific word and short document embeddings using
<a href="https://github.com/prodriguezsosa/EmbeddingRegression" class="external-link">the ‘a
la carte’ embeddings regression</a>, implemented by <a href="https://www.cambridge.org/core/journals/american-political-science-review/article/embedding-regression-models-for-contextspecific-description-and-inference/4C90013E5C714C8483ED95CC699022FB" class="external-link">Rodriguez
et al (2022)</a> and <a href="https://www.cambridge.org/core/journals/political-analysis/article/multilanguage-word-embeddings-for-social-scientists-estimation-inference-and-validation-resources-for-157-languages/F5DE16FA784CA81481150715967BD9FC" class="external-link">Rodriguez
et al (2024)</a>.</p>
<p>In this case study, we will demonstrate how to use the conText
package alongside other embedding frameworks by working through the
example provided in Rodriguez et al.’s <a href="https://github.com/prodriguezsosa/conText/blob/master/vignettes/quickstart.md" class="external-link">Quick
Start Guide</a>. While ConText includes its own cross-lingual ALC
Embeddings, this tutorial extends its capabilities by integrating it
with flaiR. Through this tutorial integration, this tutorial shows how
to:</p>
</div>
<ul>
<li><p>Access flaiR’s powerful embedding models</p></li>
<li><p>Connect with any transformer-based embedding models from
HuggingFace via FlaiR</p></li>
</ul>
<div style="text-align: justify;">
<p>We will be following the example directly from <a href="">Rodriguez
et al.’s Quick Start Guide</a> as this case study. It’s important to
note that results obtained using alternative embedding frameworks may
deviate from the original implementation, and should be interpreted with
caution. These comparative results are primarily intended for reference
and educational use.</p>
<p>First of all, when loading the conText package, you’ll find three
pre-loaded datasets: <code>cr_sample_corpus</code>,
<code>cr_glove_subset</code>, and <code>cr_transform.</code> These
datasets are used in the package’s tutorial to demonstrate preprocessing
steps. For this exercise, we only use <code>cr_sample_corpus</code> to
explore other embedding frameworks, including:</p>
</div>
<ul>
<li><p><code>en-crawl</code> embedding</p></li>
<li><p>Flair NLP contextual embeddings (as described in <a href="https://flairnlp.github.io/docs/tutorial-embeddings/flair-embeddings" class="external-link">Akbik
et al., COLING 2018 paper</a>)</p></li>
<li><p>Integrated embeddings extracted from transformers like
BERT.</p></li>
</ul>
<div class="section level3">
<h3 id="build-document-embedding-matrix-with-other-embedding-frameworks">Build Document-Embedding-Matrix with Other Embedding Frameworks<a class="anchor" aria-label="anchor" href="#build-document-embedding-matrix-with-other-embedding-frameworks"></a>
</h3>
<p><u><strong>Step 1</strong></u> Tokenize Text with
<code>quanteda</code> and <code>conText</code></p>
<p>First, let’s tokenize <code>cr_sample_corpus</code> using the
tokens_context function from the conText package.</p>
<div class="sourceCode" id="cb123"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># tokenize corpus removing unnecessary (i.e. semantically uninformative) elements</span></span>
<span><span class="va">toks</span> <span class="op">&lt;-</span> <span class="fu">tokens</span><span class="op">(</span><span class="va">cr_sample_corpus</span>, remove_punct<span class="op">=</span><span class="cn">T</span>, remove_symbols<span class="op">=</span><span class="cn">T</span>, remove_numbers<span class="op">=</span><span class="cn">T</span>, remove_separators<span class="op">=</span><span class="cn">T</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># clean out stopwords and words with 2 or fewer characters</span></span>
<span><span class="va">toks_nostop</span> <span class="op">&lt;-</span> <span class="fu">tokens_select</span><span class="op">(</span><span class="va">toks</span>, pattern <span class="op">=</span> <span class="fu">stopwords</span><span class="op">(</span><span class="st">"en"</span><span class="op">)</span>, selection <span class="op">=</span> <span class="st">"remove"</span>, min_nchar<span class="op">=</span><span class="fl">3</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># only use features that appear at least 5 times in the corpus</span></span>
<span><span class="va">feats</span> <span class="op">&lt;-</span> <span class="fu">dfm</span><span class="op">(</span><span class="va">toks_nostop</span>, tolower<span class="op">=</span><span class="cn">T</span>, verbose <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html" class="external-link">%&gt;%</a></span> <span class="fu">dfm_trim</span><span class="op">(</span>min_termfreq <span class="op">=</span> <span class="fl">5</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html" class="external-link">%&gt;%</a></span> <span class="fu">featnames</span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># leave the pads so that non-adjacent words will not become adjacent</span></span>
<span><span class="va">toks_nostop_feats</span> <span class="op">&lt;-</span> <span class="fu">tokens_select</span><span class="op">(</span><span class="va">toks_nostop</span>, <span class="va">feats</span>, padding <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># build a tokenized corpus of contexts surrounding the target term "immigration"</span></span>
<span><span class="va">immig_toks</span> <span class="op">&lt;-</span> <span class="fu">tokens_context</span><span class="op">(</span>x <span class="op">=</span> <span class="va">toks_nostop_feats</span>, pattern <span class="op">=</span> <span class="st">"immigr*"</span>, window <span class="op">=</span> <span class="fl">6L</span><span class="op">)</span></span>
<span><span class="co">#&gt; 125 instances of "immigrant" found.</span></span>
<span><span class="co">#&gt; 288 instances of "immigrants" found.</span></span>
<span><span class="co">#&gt; 924 instances of "immigration" found.</span></span>
<span></span>
<span><span class="co"># build document-feature matrix</span></span>
<span><span class="va">immig_dfm</span> <span class="op">&lt;-</span> <span class="fu">dfm</span><span class="op">(</span><span class="va">immig_toks</span><span class="op">)</span></span></code></pre></div>
<p><u><strong>Step 2</strong></u> Import Embedding Tools</p>
<div style="text-align: justify;">
<p>To facilitate the loading of different embedding types, I’ll import
the following <strong>classes</strong> and <strong>functions</strong>
from flaiR: <code>WordEmbeddings</code>, <code>FlairEmbeddings</code>,
<code>TransformerWordEmbeddings</code>, <code>StackedEmbeddings</code>,
and <code>Sentence.</code> These components enable us to work with GloVe
embeddings, Flair’s contextual embeddings, and transformer-based
embeddings from the HuggingFace library.</p>
</div>
<div style="text-align: justify;">
<p>Initialize a Flair Sentence object by concatenating cr_glove_subset
row names. The collapse parameter ensures proper tokenization by adding
space delimiters. Then, embed the sentence text using the loaded
fasttext embeddings.</p>
</div>
<div class="sourceCode" id="cb124"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Combine all text into a single string and create a Flair sentence</span></span>
<span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html" class="external-link">rownames</a></span><span class="op">(</span><span class="va">cr_glove_subset</span><span class="op">)</span>, collapse <span class="op">=</span> <span class="st">" "</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Apply FastText embeddings to the sentence</span></span>
<span><span class="va">fasttext_embeddings</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Sentence[500]: "people bill immigration president can american country law states one united speaker time going just now senate years want congress work get border know said make security many think act need children today come house also americans like year dont say america new state take way jobs senator federal government first back even amendment important well legislation support right women reform colleagues million committee laws system percent vote floor last every nation program made issue care immigrants see department national health world good republican families court workers executive administration much help illegal enforcement republicans thank fact members working number opportunity thats believe pass great never legal obama homeland money family across two put look process came may violence order give rule job done able pay must day economy home community budget tax public without passed things better since majority texas let justice another hope policy lets provide debate history ago sure presidents action week young actually nations amendments bipartisan says long issues comprehensive next something place life doesnt thing lot point chairman economic business leadership service constitution tell status dream cant yield future office billion visa find military still member talk together school rights continue part madam amnesty days secure bring best yet lives democrats broken times side nothing ask protect allow problem around making clear body foreign communities including coming use rise simply heard forward end programs leader already saying individuals general power funding case whether debt course keep question worked needs washington war change got address secretary parents little least hard three citizens different education judiciary ever strong immigrant always means current report child less really services political given increase might enough create countries domestic human supreme authority brought reason kind past become aliens understand millions judge enforce benefits constitutional high stand others criminal serve stop middle real matter talking makes someone proud month safety line used college live didnt class called call far borders victims ensure district thousands california taxes rules obamacare deal goes representatives took insurance spending companies actions small instead read resources friends mexico seen illegally policies trying labor party example crisis bills votes ice special wage went taken men asked told comes serious maybe students businesses getting words group chamber senators taking leaders wish dollars citizenship sense problems close information social friend stay offer record away months set urge person minimum democratic cost local voted numbers join energy served happen aisle found congressional wanted according plan kids try chance major almost powers trade undocumented critical poverty start article single left certainly cases access patrol individual honor remember income attorney within officers wrong speak company second hundreds impact created hours respect arizona though move hearing employees gentleman anyone along known meet chair full southern big trillion level four ability experience face provisions man provides fix open theyre efforts asian university decision groups industry allowed higher often agree white visas held upon path certain share isnt looking employers consider colleague average woman anything however balance former city interest control crime weeks free gone unemployment idea true"</span></span></code></pre></div>
<div style="text-align: justify;">
<p>The <code>process_embeddings</code> function from flaiR extracts
pre-embedded GloVe vectors from a sentence object and arranges them into
a structured matrix. In this matrix, tokens are represented as rows,
embedding dimensions as columns, and each row is labeled with its
corresponding token text.</p>
</div>
<div class="sourceCode" id="cb125"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fasttext_subset</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/process_embeddings.html">process_embeddings</a></span><span class="op">(</span><span class="va">sentence</span>, verbose <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="co">#&gt; Extracting token embeddings...</span></span>
<span><span class="co">#&gt; Converting embeddings to matrix format...Processing completed in 3.729 seconds</span></span>
<span><span class="co">#&gt; Generated embedding matrix with 500 tokens and 300 dimensions</span></span></code></pre></div>
<p><u><strong>Step 3</strong></u> Computing Context-Specific Word
Embeddings Using FastText</p>
<div style="text-align: justify;">
<p>Create a feature co-occurrence matrix (FCM) from tokenized text and
transform pre-trained FastText embeddings using co-occurrence
information.</p>
</div>
<div class="sourceCode" id="cb126"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Create a feature co-occurrence matrix (FCM) from tokenized text</span></span>
<span><span class="va">toks_fcm</span> <span class="op">&lt;-</span> <span class="fu">fcm</span><span class="op">(</span><span class="va">toks_nostop_feats</span>, </span>
<span>                context <span class="op">=</span> <span class="st">"window"</span>,    </span>
<span>                window <span class="op">=</span> <span class="fl">6</span>,            </span>
<span>                count <span class="op">=</span> <span class="st">"frequency"</span>,   </span>
<span>                tri <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span>           </span>
<span></span>
<span><span class="co"># Transform pre-trained Glove embeddings using co-occurrence information</span></span>
<span><span class="va">ft_transform</span> <span class="op">&lt;-</span> <span class="fu">compute_transform</span><span class="op">(</span></span>
<span>    x <span class="op">=</span> <span class="va">toks_fcm</span>,                    </span>
<span>    pre_trained <span class="op">=</span> <span class="va">fasttext_subset</span>,        </span>
<span>    weighting <span class="op">=</span> <span class="st">'log'</span>                </span>
<span><span class="op">)</span></span></code></pre></div>
<p>Calculate Document Embedding Matrix (DEM) using transformed FastText
embeddings.</p>
<div class="sourceCode" id="cb127"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Calculate Document Embedding Matrix (DEM) using transformed FastText embeddings</span></span>
<span><span class="va">immig_dem_ft</span> <span class="op">&lt;-</span> <span class="fu">dem</span><span class="op">(</span>x <span class="op">=</span> <span class="va">immig_dfm</span>, </span>
<span>                    pre_trained <span class="op">=</span> <span class="va">fasttext_subset</span>, </span>
<span>                    transform <span class="op">=</span> <span class="cn">TRUE</span>, </span>
<span>                    transform_matrix <span class="op">=</span> <span class="va">ft_transform</span>, </span>
<span>                    verbose <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<p>Show each document inherits its corresponding docvars.</p>
<div class="sourceCode" id="cb128"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">immig_dem_ft</span><span class="op">@</span><span class="va">docvars</span><span class="op">)</span></span>
<span><span class="co">#&gt;       pattern party gender nominate_dim1</span></span>
<span><span class="co">#&gt; 1  immigrants     D      F        -0.759</span></span>
<span><span class="co">#&gt; 2 immigration     D      F        -0.402</span></span>
<span><span class="co">#&gt; 3   immigrant     D      F        -0.402</span></span>
<span><span class="co">#&gt; 4   immigrant     D      F        -0.402</span></span>
<span><span class="co">#&gt; 5   immigrant     D      F        -0.402</span></span>
<span><span class="co">#&gt; 6 immigration     D      F        -0.402</span></span></code></pre></div>
<p><u><strong>Step 4</strong></u> Embedding Eegression</p>
<div class="sourceCode" id="cb129"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">2021L</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/prodriguezsosa/EmbeddingRegression" class="external-link">conText</a></span><span class="op">)</span></span>
<span><span class="va">ft_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/conText/man/conText.html" class="external-link">conText</a></span><span class="op">(</span>formula <span class="op">=</span> <span class="va">immigration</span> <span class="op">~</span> <span class="va">party</span> <span class="op">+</span> <span class="va">gender</span>,</span>
<span>                    data <span class="op">=</span> <span class="va">toks_nostop_feats</span>,</span>
<span>                    pre_trained <span class="op">=</span> <span class="va">fasttext_subset</span>,</span>
<span>                    transform <span class="op">=</span> <span class="cn">TRUE</span>, </span>
<span>                    transform_matrix <span class="op">=</span> <span class="va">ft_transform</span>, </span>
<span>                    confidence_level <span class="op">=</span> <span class="fl">0.95</span>,</span>
<span>                    permute <span class="op">=</span> <span class="cn">TRUE</span>, </span>
<span>                    jackknife <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                    num_permutations <span class="op">=</span> <span class="fl">100</span>,</span>
<span>                    window <span class="op">=</span> <span class="fl">6</span>, case_insensitive <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                    verbose <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="co">#&gt; Note: These values are not regression coefficients. Check out the Quick Start Guide for help with interpretation: </span></span>
<span><span class="co">#&gt; https://github.com/prodriguezsosa/conText/blob/master/vignettes/quickstart.md</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;   coefficient normed.estimate std.error  lower.ci upper.ci p.value</span></span>
<span><span class="co">#&gt; 1     party_R        1.169415 0.1589605 0.8574491 1.481381       0</span></span>
<span><span class="co">#&gt; 2    gender_M        0.837367 0.1433172 0.5561016 1.118632       0</span></span></code></pre></div>
<p>extract D-dimensional beta coefficients.</p>
<div class="sourceCode" id="cb130"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># The intercept in this case is the fastext embedding for female Democrats</span></span>
<span><span class="co"># beta coefficients can be combined to get each group's fastext embedding</span></span>
<span><span class="va">DF_wv</span> <span class="op">&lt;-</span> <span class="va">ft_model</span><span class="op">[</span><span class="st">'(Intercept)'</span>,<span class="op">]</span>  <span class="co"># (D)emocrat - (F)emale </span></span>
<span><span class="va">DM_wv</span> <span class="op">&lt;-</span> <span class="va">ft_model</span><span class="op">[</span><span class="st">'(Intercept)'</span>,<span class="op">]</span> <span class="op">+</span> <span class="va">ft_model</span><span class="op">[</span><span class="st">'gender_M'</span>,<span class="op">]</span> <span class="co"># (D)emocrat - (M)ale </span></span>
<span><span class="va">RF_wv</span> <span class="op">&lt;-</span> <span class="va">ft_model</span><span class="op">[</span><span class="st">'(Intercept)'</span>,<span class="op">]</span> <span class="op">+</span> <span class="va">ft_model</span><span class="op">[</span><span class="st">'party_R'</span>,<span class="op">]</span>  <span class="co"># (R)epublican - (F)emale </span></span>
<span><span class="va">RM_wv</span> <span class="op">&lt;-</span> <span class="va">ft_model</span><span class="op">[</span><span class="st">'(Intercept)'</span>,<span class="op">]</span> <span class="op">+</span> <span class="va">ft_model</span><span class="op">[</span><span class="st">'party_R'</span>,<span class="op">]</span> <span class="op">+</span> <span class="va">ft_model</span><span class="op">[</span><span class="st">'gender_M'</span>,<span class="op">]</span> <span class="co"># (R)epublican - (M)ale</span></span></code></pre></div>
<p>nearest neighbors</p>
<div class="sourceCode" id="cb131"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/conText/man/nns.html" class="external-link">nns</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html" class="external-link">rbind</a></span><span class="op">(</span><span class="va">DF_wv</span>,<span class="va">DM_wv</span><span class="op">)</span>, </span>
<span>    N <span class="op">=</span> <span class="fl">10</span>, </span>
<span>    pre_trained <span class="op">=</span> <span class="va">fasttext_subset</span>, </span>
<span>    candidates <span class="op">=</span> <span class="va">ft_model</span><span class="op">@</span><span class="va">features</span><span class="op">)</span></span>
<span><span class="co">#&gt; $DM_wv</span></span>
<span><span class="co">#&gt; <span style="color: #949494;"># A tibble: 10 × 4</span></span></span>
<span><span class="co">#&gt;    target feature      rank value</span></span>
<span><span class="co">#&gt;    <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>  <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>       <span style="color: #949494; font-style: italic;">&lt;int&gt;</span> <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 1</span> DM_wv  immigration     1 0.850</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 2</span> DM_wv  immigrants      2 0.746</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 3</span> DM_wv  immigrant       3 0.698</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 4</span> DM_wv  education       4 0.589</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 5</span> DM_wv  government      5 0.587</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 6</span> DM_wv  legislation     6 0.579</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 7</span> DM_wv  city            7 0.569</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 8</span> DM_wv  law             8 0.566</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 9</span> DM_wv  reform          9 0.559</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">10</span> DM_wv  citizenship    10 0.557</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $DF_wv</span></span>
<span><span class="co">#&gt; <span style="color: #949494;"># A tibble: 10 × 4</span></span></span>
<span><span class="co">#&gt;    target feature      rank value</span></span>
<span><span class="co">#&gt;    <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>  <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>       <span style="color: #949494; font-style: italic;">&lt;int&gt;</span> <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 1</span> DF_wv  immigration     1 0.779</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 2</span> DF_wv  immigrants      2 0.667</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 3</span> DF_wv  immigrant       3 0.620</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 4</span> DF_wv  education       4 0.574</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 5</span> DF_wv  government      5 0.556</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 6</span> DF_wv  economic        6 0.551</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 7</span> DF_wv  citizenship     7 0.549</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 8</span> DF_wv  legislation     8 0.540</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 9</span> DF_wv  issues          9 0.531</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">10</span> DF_wv  laws           10 0.530</span></span></code></pre></div>
<div class="sourceCode" id="cb132"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org" class="external-link">ggplot2</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html" class="external-link">ggplot</a></span><span class="op">(</span><span class="va">ft_model</span><span class="op">@</span><span class="va">normed_coefficients</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html" class="external-link">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">coefficient</span>, y <span class="op">=</span> <span class="va">normed.estimate</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_linerange.html" class="external-link">geom_errorbar</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html" class="external-link">aes</a></span><span class="op">(</span>ymin <span class="op">=</span> <span class="va">lower.ci</span>, ymax <span class="op">=</span> <span class="va">upper.ci</span><span class="op">)</span>, width <span class="op">=</span> <span class="fl">0.2</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html" class="external-link">geom_point</a></span><span class="op">(</span>size <span class="op">=</span> <span class="fl">3</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html" class="external-link">geom_hline</a></span><span class="op">(</span>yintercept <span class="op">=</span> <span class="fl">0</span>, linetype <span class="op">=</span> <span class="st">"dashed"</span>, color <span class="op">=</span> <span class="st">"gray50"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html" class="external-link">theme_minimal</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html" class="external-link">labs</a></span><span class="op">(</span></span>
<span>    title <span class="op">=</span> <span class="st">"Estimated Coefficients with 95% CIs"</span>,</span>
<span>    x <span class="op">=</span> <span class="st">"Variables"</span>,</span>
<span>    y <span class="op">=</span> <span class="st">"Normalized Estimate"</span></span>
<span>  <span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/theme.html" class="external-link">theme</a></span><span class="op">(</span>axis.text.x <span class="op">=</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/element.html" class="external-link">element_text</a></span><span class="op">(</span>angle <span class="op">=</span> <span class="fl">45</span>, hjust <span class="op">=</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p><img src="tutorial_files/figure-html/unnamed-chunk-139-1.png" width="700"></p>
</div>
<div class="section level3">
<h3 id="exploring-document-embedding-matrix-with-context-functions">Exploring Document-Embedding Matrix with conText Functions<a class="anchor" aria-label="anchor" href="#exploring-document-embedding-matrix-with-context-functions"></a>
</h3>
<p>Check dimensions of the resulting matrix.</p>
<div class="sourceCode" id="cb133"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Calculate average document embeddings for immigration-related texts</span></span>
<span><span class="va">immig_wv_ft</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/colSums.html" class="external-link">colMeans</a></span><span class="op">(</span><span class="va">immig_dem_ft</span><span class="op">)</span>, </span>
<span>                   ncol <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html" class="external-link">ncol</a></span><span class="op">(</span><span class="va">immig_dem_ft</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html" class="external-link">%&gt;%</a></span>  <span class="fu">`rownames&lt;-`</span><span class="op">(</span><span class="st">"immigration"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/dim.html" class="external-link">dim</a></span><span class="op">(</span><span class="va">immig_wv_ft</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1]   1 300</span></span></code></pre></div>
<p>to get group-specific embeddings, average within party</p>
<div class="sourceCode" id="cb134"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">immig_wv_ft_party</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/conText/man/dem_group.html" class="external-link">dem_group</a></span><span class="op">(</span><span class="va">immig_dem_ft</span>, </span>
<span>                               groups <span class="op">=</span> <span class="va">immig_dem_ft</span><span class="op">@</span><span class="va">docvars</span><span class="op">$</span><span class="va">party</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/dim.html" class="external-link">dim</a></span><span class="op">(</span><span class="va">immig_wv_ft_party</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1]   2 300</span></span></code></pre></div>
<p>Find nearest neighbors by party</p>
<div class="sourceCode" id="cb135"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># find nearest neighbors by party</span></span>
<span><span class="co"># setting as_list = FALSE combines each group's results into a single tibble (useful for joint plotting)</span></span>
<span><span class="va">immig_nns_ft</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/conText/man/nns.html" class="external-link">nns</a></span><span class="op">(</span><span class="va">immig_wv_ft_party</span>, </span>
<span>                    pre_trained <span class="op">=</span> <span class="va">fasttext_subset</span>, </span>
<span>                    N <span class="op">=</span> <span class="fl">5</span>, </span>
<span>                    candidates <span class="op">=</span> <span class="va">immig_wv_ft_party</span><span class="op">@</span><span class="va">features</span>, </span>
<span>                    as_list <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<p>check out results for Republican.</p>
<div class="sourceCode" id="cb136"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">immig_nns_ft</span><span class="op">[[</span><span class="st">"R"</span><span class="op">]</span><span class="op">]</span></span>
<span><span class="co">#&gt; <span style="color: #949494;"># A tibble: 5 × 4</span></span></span>
<span><span class="co">#&gt;   target feature      rank value</span></span>
<span><span class="co">#&gt;   <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>  <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>       <span style="color: #949494; font-style: italic;">&lt;int&gt;</span> <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">1</span> R      immigration     1 0.837</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">2</span> R      immigrants      2 0.779</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">3</span> R      immigrant       3 0.721</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">4</span> R      government      4 0.590</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">5</span> R      laws            5 0.588</span></span></code></pre></div>
<p>check out results for Democrat</p>
<div class="sourceCode" id="cb137"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">immig_nns_ft</span><span class="op">[[</span><span class="st">"D"</span><span class="op">]</span><span class="op">]</span></span>
<span><span class="co">#&gt; <span style="color: #949494;"># A tibble: 5 × 4</span></span></span>
<span><span class="co">#&gt;   target feature      rank value</span></span>
<span><span class="co">#&gt;   <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>  <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>       <span style="color: #949494; font-style: italic;">&lt;int&gt;</span> <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">1</span> D      immigration     1 0.849</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">2</span> D      immigrants      2 0.817</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">3</span> D      immigrant       3 0.764</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">4</span> D      education       4 0.621</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">5</span> D      families        5 0.612</span></span></code></pre></div>
<p>compute the cosine similarity between each party’s embedding and a
specific set of features</p>
<div class="sourceCode" id="cb138"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/conText/man/cos_sim.html" class="external-link">cos_sim</a></span><span class="op">(</span><span class="va">immig_wv_ft_party</span>, </span>
<span>        pre_trained <span class="op">=</span> <span class="va">fasttext_subset</span>, </span>
<span>        features <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">'reform'</span>, <span class="st">'enforcement'</span><span class="op">)</span>, as_list <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="co">#&gt;   target     feature     value</span></span>
<span><span class="co">#&gt; 1      D      reform 0.5306484</span></span>
<span><span class="co">#&gt; 2      R      reform 0.5425264</span></span>
<span><span class="co">#&gt; 3      D enforcement 0.5102908</span></span>
<span><span class="co">#&gt; 4      R enforcement 0.5647015</span></span></code></pre></div>
<p>compute the cosine similarity between each party’s embedding and a
specific set of features.</p>
<div class="sourceCode" id="cb139"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Republican</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/conText/man/nns_ratio.html" class="external-link">nns_ratio</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">immig_wv_ft_party</span>, </span>
<span>          N <span class="op">=</span> <span class="fl">15</span>, </span>
<span>          numerator <span class="op">=</span> <span class="st">"R"</span>, </span>
<span>          candidates <span class="op">=</span> <span class="va">immig_wv_ft_party</span><span class="op">@</span><span class="va">features</span>, </span>
<span>          pre_trained <span class="op">=</span> <span class="va">fasttext_subset</span>, </span>
<span>          verbose <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="co">#&gt;        feature     value</span></span>
<span><span class="co">#&gt; 1       coming 1.3142709</span></span>
<span><span class="co">#&gt; 2       things 1.2022012</span></span>
<span><span class="co">#&gt; 3    political 1.1506919</span></span>
<span><span class="co">#&gt; 4  enforcement 1.1066269</span></span>
<span><span class="co">#&gt; 5       aliens 1.0194435</span></span>
<span><span class="co">#&gt; 6       issues 1.0124816</span></span>
<span><span class="co">#&gt; 7   government 0.9908574</span></span>
<span><span class="co">#&gt; 8        visas 0.9904320</span></span>
<span><span class="co">#&gt; 9         laws 0.9874748</span></span>
<span><span class="co">#&gt; 10 immigration 0.9849872</span></span>
<span><span class="co">#&gt; 11 legislation 0.9666258</span></span>
<span><span class="co">#&gt; 12  immigrants 0.9526017</span></span>
<span><span class="co">#&gt; 13 citizenship 0.9524804</span></span>
<span><span class="co">#&gt; 14   education 0.9448045</span></span>
<span><span class="co">#&gt; 15   immigrant 0.9435372</span></span>
<span><span class="co">#&gt; 16         law 0.9222851</span></span>
<span><span class="co">#&gt; 17   community 0.8816272</span></span>
<span><span class="co">#&gt; 18    citizens 0.8722014</span></span>
<span><span class="co">#&gt; 19        city 0.8544540</span></span>
<span><span class="co">#&gt; 20    families 0.7845902</span></span>
<span><span class="co">#&gt; 21 communities 0.7798629</span></span></code></pre></div>
<div class="sourceCode" id="cb140"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Democrat</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/conText/man/nns_ratio.html" class="external-link">nns_ratio</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">immig_wv_ft_party</span>, </span>
<span>          N <span class="op">=</span> <span class="fl">15</span>, </span>
<span>          numerator <span class="op">=</span> <span class="st">"D"</span>, </span>
<span>          candidates <span class="op">=</span> <span class="va">immig_wv_ft_party</span><span class="op">@</span><span class="va">features</span>, </span>
<span>          pre_trained <span class="op">=</span> <span class="va">fasttext_subset</span>, </span>
<span>          verbose <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="co">#&gt;        feature     value</span></span>
<span><span class="co">#&gt; 1  communities 1.2822766</span></span>
<span><span class="co">#&gt; 2     families 1.2745507</span></span>
<span><span class="co">#&gt; 3         city 1.1703380</span></span>
<span><span class="co">#&gt; 4     citizens 1.1465242</span></span>
<span><span class="co">#&gt; 5    community 1.1342663</span></span>
<span><span class="co">#&gt; 6          law 1.0842634</span></span>
<span><span class="co">#&gt; 7    immigrant 1.0598416</span></span>
<span><span class="co">#&gt; 8    education 1.0584201</span></span>
<span><span class="co">#&gt; 9  citizenship 1.0498903</span></span>
<span><span class="co">#&gt; 10  immigrants 1.0497567</span></span>
<span><span class="co">#&gt; 11 legislation 1.0345265</span></span>
<span><span class="co">#&gt; 12 immigration 1.0152416</span></span>
<span><span class="co">#&gt; 13        laws 1.0126841</span></span>
<span><span class="co">#&gt; 14       visas 1.0096605</span></span>
<span><span class="co">#&gt; 15  government 1.0092270</span></span>
<span><span class="co">#&gt; 16      issues 0.9876722</span></span>
<span><span class="co">#&gt; 17      aliens 0.9809273</span></span>
<span><span class="co">#&gt; 18 enforcement 0.9036469</span></span>
<span><span class="co">#&gt; 19   political 0.8690423</span></span>
<span><span class="co">#&gt; 20      things 0.8318075</span></span>
<span><span class="co">#&gt; 21      coming 0.7608781</span></span></code></pre></div>
<p>compute the cosine similarity between each party’s embedding and a
set of tokenized contexts</p>
<div class="sourceCode" id="cb141"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">immig_ncs</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/conText/man/ncs.html" class="external-link">ncs</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">immig_wv_ft_party</span>, </span>
<span>                 contexts_dem <span class="op">=</span> <span class="va">immig_dem_ft</span>, </span>
<span>                 contexts <span class="op">=</span> <span class="va">immig_toks</span>, </span>
<span>                 N <span class="op">=</span> <span class="fl">5</span>, </span>
<span>                 as_list <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># nearest contexts to Republican embedding of target term</span></span>
<span><span class="co"># note, these may included contexts originating from Democrat speakers</span></span>
<span><span class="va">immig_ncs</span><span class="op">[[</span><span class="st">"R"</span><span class="op">]</span><span class="op">]</span></span>
<span><span class="co">#&gt; <span style="color: #949494;"># A tibble: 5 × 4</span></span></span>
<span><span class="co">#&gt;   target context                                                rank value</span></span>
<span><span class="co">#&gt;   <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>  <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>                                                 <span style="color: #949494; font-style: italic;">&lt;int&gt;</span> <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">1</span> R      reminded difficult achieve consensus issues reform s…     1 0.536</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">2</span> R      laws last years eight states adopted enforcement mea…     2 0.496</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">3</span> R      well exactly right way stop illegal reducing demand …     3 0.492</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">4</span> R      speeches border enforcement speeches stopping illega…     4 0.482</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">5</span> R      welcome legal immigrants legal process illegal simpl…     5 0.472</span></span></code></pre></div>
<div class="sourceCode" id="cb142"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">immig_ncs</span><span class="op">[[</span><span class="st">"D"</span><span class="op">]</span><span class="op">]</span></span>
<span><span class="co">#&gt; <span style="color: #949494;"># A tibble: 5 × 4</span></span></span>
<span><span class="co">#&gt;   target context                                                rank value</span></span>
<span><span class="co">#&gt;   <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>  <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>                                                 <span style="color: #949494; font-style: italic;">&lt;int&gt;</span> <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">1</span> D      heritage principles demand courage reform broken sys…     1 0.571</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">2</span> D      speaker almost year since twothirds comprehensive re…     2 0.562</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">3</span> D      senate accepted indeed need thorough comprehensive r…     3 0.562</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">4</span> D      state union consideration bill provide comprehensive…     4 0.536</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">5</span> D      cover house republican failure bring comprehensive r…     5 0.534</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="comparative-analysis-of-a-la-carte-flair-stacked-and-bert-embeddings">Comparative Analysis of A La Carte, Flair Stacked, and BERT
Embeddings<a class="anchor" aria-label="anchor" href="#comparative-analysis-of-a-la-carte-flair-stacked-and-bert-embeddings"></a>
</h3>
<div class="section level4">
<h4 id="build-a-la-carte-document-embedding-matrix">Build A La Carte Document-Embedding-Matrix<a class="anchor" aria-label="anchor" href="#build-a-la-carte-document-embedding-matrix"></a>
</h4>
<div class="sourceCode" id="cb143"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># build a document-embedding-matrix</span></span>
<span><span class="va">immig_dem</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/conText/man/dem.html" class="external-link">dem</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">immig_dfm</span>, pre_trained <span class="op">=</span> <span class="va">cr_glove_subset</span>, transform <span class="op">=</span> <span class="cn">TRUE</span>, transform_matrix <span class="op">=</span> <span class="va">cr_transform</span>, verbose <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">2021L</span><span class="op">)</span></span>
<span><span class="va">alc_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/conText/man/conText.html" class="external-link">conText</a></span><span class="op">(</span>formula <span class="op">=</span> <span class="va">immigration</span> <span class="op">~</span> <span class="va">party</span> <span class="op">+</span> <span class="va">gender</span>,</span>
<span>                     data <span class="op">=</span> <span class="va">toks_nostop_feats</span>,</span>
<span>                     pre_trained <span class="op">=</span> <span class="va">cr_glove_subset</span>,</span>
<span>                     transform <span class="op">=</span> <span class="cn">TRUE</span>, </span>
<span>                     transform_matrix <span class="op">=</span> <span class="va">cr_transform</span>,</span>
<span>                     jackknife <span class="op">=</span> <span class="cn">TRUE</span>, </span>
<span>                     confidence_level <span class="op">=</span> <span class="fl">0.95</span>,</span>
<span>                     permute <span class="op">=</span> <span class="cn">TRUE</span>, </span>
<span>                     num_permutations <span class="op">=</span> <span class="fl">100</span>,</span>
<span>                     window <span class="op">=</span> <span class="fl">6</span>, </span>
<span>                     case_insensitive <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                     verbose <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="co">#&gt; Note: These values are not regression coefficients. Check out the Quick Start Guide for help with interpretation: </span></span>
<span><span class="co">#&gt; https://github.com/prodriguezsosa/conText/blob/master/vignettes/quickstart.md</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;   coefficient normed.estimate std.error lower.ci upper.ci p.value</span></span>
<span><span class="co">#&gt; 1     party_R        2.960860 0.2398964 2.490054 3.431665       0</span></span>
<span><span class="co">#&gt; 2    gender_M        2.303401 0.2290150 1.853950 2.752851       0</span></span></code></pre></div>
</div>
<div class="section level4">
<h4 id="document-embedding-matrix-construction-using-flair-contextual-stacked-embeddings">Document Embedding Matrix Construction Using Flair Contextual
Stacked Embeddings<a class="anchor" aria-label="anchor" href="#document-embedding-matrix-construction-using-flair-contextual-stacked-embeddings"></a>
</h4>
<div style="text-align: justify;">
<p>To facilitate the loading of different embedding types, I’ll import
the following <strong>classes</strong> and <strong>functions</strong>
from flaiF: <code>WordEmbeddings</code>, <code>FlairEmbeddings</code>,
<code>TransformerWordEmbeddings</code>, <code>StackedEmbeddings</code>,
and <code>Sentence</code>. These components enable us to work with GloVe
embeddings, Flair’s contextual embeddings, and transformer-based
embeddings from the HuggingFace library.</p>
</div>
<p>Combine three different types of embeddings into a stacked embedding
model.</p>
<p>This creates a stacked embedding model that combines:</p>
<ul>
<li>
<code>FastText embeddings</code>: Captures general word
semantics</li>
<li>
<code>Forward Flair</code>: Captures contextual information reading
text left-to-right</li>
<li>
<code>Backward Flair</code>: Captures contextual information reading
text right-to-left</li>
</ul>
<div class="sourceCode" id="cb144"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">stacked_embeddings</span>  <span class="op">&lt;-</span> <span class="fu">StackedEmbeddings</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>  <span class="va">fasttext_embeddings</span>,    </span>
<span>  <span class="va">flair_forward</span>,          </span>
<span>  <span class="va">flair_backward</span>        </span>
<span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb145"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Step 1: Create a Flair Sentence object from the text</span></span>
<span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html" class="external-link">rownames</a></span><span class="op">(</span><span class="va">cr_glove_subset</span><span class="op">)</span>, collapse <span class="op">=</span> <span class="st">" "</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Step 2: Generate embeddings using our stacked model</span></span>
<span><span class="va">stacked_embeddings</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Step 3: Extract and store embeddings for each token</span></span>
<span><span class="va">stacked_subset</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/process_embeddings.html">process_embeddings</a></span><span class="op">(</span><span class="va">sentence</span>, verbose <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="co">#&gt; Extracting token embeddings...</span></span>
<span><span class="co">#&gt; Converting embeddings to matrix format...Processing completed in 3.539 seconds</span></span>
<span><span class="co">#&gt; Generated embedding matrix with 500 tokens and 4396 dimensions</span></span>
<span></span>
<span><span class="co"># Step 4: Compute transformation matrix</span></span>
<span></span>
<span><span class="va">st_transform</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/conText/man/compute_transform.html" class="external-link">compute_transform</a></span><span class="op">(</span></span>
<span>   x <span class="op">=</span> <span class="va">toks_fcm</span>,               </span>
<span>   pre_trained <span class="op">=</span> <span class="va">stacked_subset</span>,</span>
<span>   weighting <span class="op">=</span> <span class="st">'log'</span>          </span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Step 5: Generate document embeddings matrix</span></span>
<span><span class="va">immig_dem_st</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/conText/man/dem.html" class="external-link">dem</a></span><span class="op">(</span></span>
<span>   x <span class="op">=</span> <span class="va">immig_dfm</span>,              </span>
<span>   pre_trained <span class="op">=</span> <span class="va">stacked_subset</span>,</span>
<span>   transform <span class="op">=</span> <span class="cn">TRUE</span>,          </span>
<span>   transform_matrix <span class="op">=</span> <span class="va">st_transform</span>,</span>
<span>   verbose <span class="op">=</span> <span class="cn">TRUE</span>             </span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Step 6: Fit conText model for analysis</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">2021L</span><span class="op">)</span>                 </span>
<span><span class="va">st_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/conText/man/conText.html" class="external-link">conText</a></span><span class="op">(</span>formula <span class="op">=</span> <span class="va">immigration</span> <span class="op">~</span> <span class="va">party</span> <span class="op">+</span> <span class="va">gender</span>,  </span>
<span>                    data <span class="op">=</span> <span class="va">toks_nostop_feats</span>,                </span>
<span>                    pre_trained <span class="op">=</span> <span class="va">stacked_subset</span>,          </span>
<span>                    transform <span class="op">=</span> <span class="cn">TRUE</span>,                      </span>
<span>                    transform_matrix <span class="op">=</span> <span class="va">st_transform</span>,       </span>
<span>                    jackknife <span class="op">=</span> <span class="cn">TRUE</span>,                      </span>
<span>                    confidence_level <span class="op">=</span> <span class="fl">0.95</span>,             </span>
<span>                    permute <span class="op">=</span> <span class="cn">TRUE</span>,                      </span>
<span>                    num_permutations <span class="op">=</span> <span class="fl">100</span>,               </span>
<span>                    window <span class="op">=</span> <span class="fl">6</span>,                          </span>
<span>                    case_insensitive <span class="op">=</span> <span class="cn">TRUE</span>,              </span>
<span>                    verbose <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="co">#&gt; Note: These values are not regression coefficients. Check out the Quick Start Guide for help with interpretation: </span></span>
<span><span class="co">#&gt; https://github.com/prodriguezsosa/conText/blob/master/vignettes/quickstart.md</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;   coefficient normed.estimate std.error  lower.ci upper.ci p.value</span></span>
<span><span class="co">#&gt; 1     party_R        121.4567  96.63111 -68.18549 311.0988    0.32</span></span>
<span><span class="co">#&gt; 2    gender_M        226.2376 102.94598  24.20230 428.2730    0.07</span></span></code></pre></div>
</div>
<div class="section level4">
<h4 id="document-embedding-matrix-construction-with-bert">Document Embedding Matrix Construction with BERT<a class="anchor" aria-label="anchor" href="#document-embedding-matrix-construction-with-bert"></a>
</h4>
<div style="text-align: justify;">
<p>BERT embeddings provide powerful contextual representations through
their bidirectional transformer architecture. These embeddings are good
at understanding context from both directions within text, generating
deep contextual representations through multiple transformer layers, and
leveraging pre-training on large text corpora to achieve strong
performance across NLP tasks. The classic BERT base model generates
768-dimensional embeddings for each token, providing rich semantic
representations.</p>
</div>
<p>By utilizing the Flair framework, we also can seamlessly
integrate:</p>
<ul>
<li>Multiple BERT variants like RoBERTa and DistilBERT</li>
<li>Cross-lingual models such as XLM-RoBERTa<br>
</li>
<li>Domain-adapted BERT models</li>
<li>Any transformer model available on HuggingFace</li>
</ul>
<div class="sourceCode" id="cb146"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Initialize BERT base uncased model embeddings from HuggingFace</span></span>
<span><span class="va">bert_embeddings</span> <span class="op">&lt;-</span> <span class="fu">TransformerWordEmbeddings</span><span class="op">(</span><span class="st">'bert-base-uncased'</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb147"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Step 1: Create a Flair Sentence object from the text</span></span>
<span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html" class="external-link">rownames</a></span><span class="op">(</span><span class="va">cr_glove_subset</span><span class="op">)</span>, collapse <span class="op">=</span> <span class="st">" "</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Step 2: Generate embeddings using BERT model from HugginFace</span></span>
<span><span class="va">bert_embeddings</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Sentence[500]: "people bill immigration president can american country law states one united speaker time going just now senate years want congress work get border know said make security many think act need children today come house also americans like year dont say america new state take way jobs senator federal government first back even amendment important well legislation support right women reform colleagues million committee laws system percent vote floor last every nation program made issue care immigrants see department national health world good republican families court workers executive administration much help illegal enforcement republicans thank fact members working number opportunity thats believe pass great never legal obama homeland money family across two put look process came may violence order give rule job done able pay must day economy home community budget tax public without passed things better since majority texas let justice another hope policy lets provide debate history ago sure presidents action week young actually nations amendments bipartisan says long issues comprehensive next something place life doesnt thing lot point chairman economic business leadership service constitution tell status dream cant yield future office billion visa find military still member talk together school rights continue part madam amnesty days secure bring best yet lives democrats broken times side nothing ask protect allow problem around making clear body foreign communities including coming use rise simply heard forward end programs leader already saying individuals general power funding case whether debt course keep question worked needs washington war change got address secretary parents little least hard three citizens different education judiciary ever strong immigrant always means current report child less really services political given increase might enough create countries domestic human supreme authority brought reason kind past become aliens understand millions judge enforce benefits constitutional high stand others criminal serve stop middle real matter talking makes someone proud month safety line used college live didnt class called call far borders victims ensure district thousands california taxes rules obamacare deal goes representatives took insurance spending companies actions small instead read resources friends mexico seen illegally policies trying labor party example crisis bills votes ice special wage went taken men asked told comes serious maybe students businesses getting words group chamber senators taking leaders wish dollars citizenship sense problems close information social friend stay offer record away months set urge person minimum democratic cost local voted numbers join energy served happen aisle found congressional wanted according plan kids try chance major almost powers trade undocumented critical poverty start article single left certainly cases access patrol individual honor remember income attorney within officers wrong speak company second hundreds impact created hours respect arizona though move hearing employees gentleman anyone along known meet chair full southern big trillion level four ability experience face provisions man provides fix open theyre efforts asian university decision groups industry allowed higher often agree white visas held upon path certain share isnt looking employers consider colleague average woman anything however balance former city interest control crime weeks free gone unemployment idea true"</span></span>
<span></span>
<span><span class="co"># Step 3: Extract and store embeddings for each token</span></span>
<span><span class="va">bert_subset</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/process_embeddings.html">process_embeddings</a></span><span class="op">(</span><span class="va">sentence</span>, verbose <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="co">#&gt; Extracting token embeddings...</span></span>
<span><span class="co">#&gt; Converting embeddings to matrix format...Processing completed in 3.027 seconds</span></span>
<span><span class="co">#&gt; Generated embedding matrix with 500 tokens and 768 dimensions</span></span>
<span></span>
<span><span class="co"># Step 4: Compute transformation matrix </span></span>
<span><span class="va">bt_transform</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/conText/man/compute_transform.html" class="external-link">compute_transform</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">toks_fcm</span>,     </span>
<span>                                  pre_trained <span class="op">=</span> <span class="va">bert_subset</span>,</span>
<span>                                  weighting <span class="op">=</span> <span class="st">'log'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Step 5: Generate document embeddings matrix</span></span>
<span><span class="va">immig_dem_bt</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/conText/man/dem.html" class="external-link">dem</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">immig_dfm</span>,</span>
<span>                    pre_trained <span class="op">=</span> <span class="va">bert_subset</span>,</span>
<span>                    transform <span class="op">=</span> <span class="cn">TRUE</span>,          </span>
<span>                    transform_matrix <span class="op">=</span> <span class="va">bt_transform</span>,</span>
<span>                    verbose <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Step 6: Fit conText model for analysis</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">2021L</span><span class="op">)</span>                 </span>
<span><span class="va">bt_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/conText/man/conText.html" class="external-link">conText</a></span><span class="op">(</span>formula <span class="op">=</span> <span class="va">immigration</span> <span class="op">~</span> <span class="va">party</span> <span class="op">+</span> <span class="va">gender</span>,  </span>
<span>                    data <span class="op">=</span> <span class="va">toks_nostop_feats</span>,                </span>
<span>                    pre_trained <span class="op">=</span> <span class="va">bert_subset</span>,          </span>
<span>                    transform <span class="op">=</span> <span class="cn">TRUE</span>,                      </span>
<span>                    transform_matrix <span class="op">=</span> <span class="va">bt_transform</span>,       </span>
<span>                    jackknife <span class="op">=</span> <span class="cn">TRUE</span>,                      </span>
<span>                    confidence_level <span class="op">=</span> <span class="fl">0.95</span>,             </span>
<span>                    permute <span class="op">=</span> <span class="cn">TRUE</span>,                      </span>
<span>                    num_permutations <span class="op">=</span> <span class="fl">100</span>,               </span>
<span>                    window <span class="op">=</span> <span class="fl">6</span>,                          </span>
<span>                    case_insensitive <span class="op">=</span> <span class="cn">TRUE</span>,              </span>
<span>                    verbose <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="co">#&gt; Note: These values are not regression coefficients. Check out the Quick Start Guide for help with interpretation: </span></span>
<span><span class="co">#&gt; https://github.com/prodriguezsosa/conText/blob/master/vignettes/quickstart.md</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;   coefficient normed.estimate std.error   lower.ci  upper.ci p.value</span></span>
<span><span class="co">#&gt; 1     party_R        356.6358  260.9004 -155.39102  868.6627    0.20</span></span>
<span><span class="co">#&gt; 2    gender_M        571.7140  276.0197   30.01485 1113.4131    0.07</span></span></code></pre></div>
</div>
<div class="section level4">
<h4 id="comparision-of-different-embedding-approaches">Comparision of Different Embedding Approaches<a class="anchor" aria-label="anchor" href="#comparision-of-different-embedding-approaches"></a>
</h4>
<div style="text-align: justify;">
<p>While this tutorial doesn’t determine a definitive best approach,
it’s important to understand the key distinctions between word embedding
methods. BERT, FastText, Flair Stacked Embeddings, and GloVe can be
categorized into two groups: dynamic and static embeddings.</p>
<p>Dynamic embeddings, particularly BERT and Flair, adapt their word
representations based on context using high-dimensional vector spaces
(BERT uses 768 dimensions in its base model). BERT employs
self-attention mechanisms and subword tokenization, while Flair uses
character-level modeling. Both effectively handle out-of-vocabulary
words through these mechanisms.</p>
<p>However, there is a notable difference between their case study and
here. While they provide selected words, we directly extract individual
word vectors from BERT and Flair (forward/backward) embeddings using the
same set of words. This doesn’t truly utilize BERT and Flair embeddings’
capability of modeling context. A more meaningful approach would be to
extract embeddings at the quasi-sentence or paragraph level, or
alternatively, to pool the entire document before extracting
embeddings.</p>
<p>These context-based approaches stand in stark contrast to GloVe’s
methodology, which relies on pre-computed global word-word co-occurrence
statistics to generate static word vectors.</p>
</div>
<p><img src="tutorial_files/figure-html/unnamed-chunk-156-1.png" width="95%"></p>
<p> </p>
<hr>
</div>
</div>
</div>
<div class="section level2">
<h2 id="cite">Cite<a class="anchor" aria-label="anchor" href="#cite"></a>
</h2>
<pre><code>@Manual{,
  title = {Flair NLP and flaiR for Social Science},
  author = {Yen-Chieh Liao, Sohini Timbadia and Stefan Müller},
  year = {2024},
  url = {https://davidycliao.github.io/flaiR/articles/tutorial.html}
}</code></pre>
</div>

  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



   </div>
  <footer><div class="container">
  <div class="pkgdown-footer-left">
  <p>Developed by <a href="https://davidycliao.github.io" class="external-link">Yen-Chieh Liao</a>, <a href="https://muellerstefan.net" class="external-link">Stefan Müller</a>, Akbik Alan, Blythe Duncan, Vollgraf Roland.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

  </div></footer>
</body>
</html>
