---
title: "Tutorial in R"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Tutorial in R}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r, include = FALSE}
library(flaiR)
library(reticulate)
# system(paste(reticulate::py_config()$python, "-m pip install flair"))
reticulate::py_install("flair")
```

# Introduction 

<div style="text-align: justify">

__Flair NLP__ is an open-source library for Natural Language Processing (NLP) developed by [Zalando Research](https://github.com/zalandoresearch/). Known for its state-of-the-art solutions for NLP tasks like Named Entity Recognition (NER), Part-of-Speech tagging (POS), and more, it has garnered attention in the NLP community for its ease of use and powerful functionalities. Developed in Python, and built on the PyTorch framework, it offers a flexible and dynamic approach to deal with textual data. On the other hand, {flaiR} in R aims to continue the framework established by Flair in Python by creating a framework for R, thereby extending Flair’s capabilities to the R programming environment.

One of the hallmark features of Flair is its __contextual string embeddings__, which is crucial in discerning the meaning of words in different contextual usages. Traditional embeddings assign a fixed vector to a word, without considering its context, which can be a limitation when trying to understand the nuances in the word’s usage across different sentences. On the contrary, Flair’s contextual embeddings generate word vectors by considering the surrounding text, thus capturing the word's context and semantics more accurately. This is particularly impactful in scenarios where a word can have different meanings based on its usage.

Flair offers pre-trained models for various languages and tasks, providing a solid foundation for various NLP applications such as text classification, sentiment analysis, and entity recognition, etc. For instance, if you're involved in a project that requires identifying persons, organizations, or locations from text, Flair has pre-trained NER models that can simplify this task.

</div>

## OOP in R when Introducing Python

<div style="text-align: justify">

Object-Oriented Programming (OOP) is a programming paradigm that uses objects, which contain both data (attributes) and functions (methods), to design applications and software. The idea is to bind the data and the methods that operate on that data into one single unit, an object. Before the advent of R6, OOP was not very common in the early stages of R. To my knowledge, R6 is relatively rare; aside from {[mlr3](https://mlr3.mlr-org.com)}, which is written in R6, most packages are accomplished in S4 and S3 (to my personal experience), which, of course, may be greatly related to the habits and tasks of R users. However, the purpose of 'flaiR' is to standardize wrapping the '{flair NLP}' Python functionality in R and to provide more convenient access for R users to utilize flair NLP features. Most usage of Flair NLP within the {flaiR} employs concepts of objects and classes, which are similar to R6. However, these features are packaged in {reticulate} from Python. In other words, some functionalities imported into R essentially belong to Python classes or modules.

In addition, tensors serve as a fundamental building block for creating and training neural networks and for conducting various numerical computations in Python. For all of Flair's NLP tasks in Python on PyTorch, there are numerous extensive functionalities for tensor operations, including element-wise operations, matrix multiplications, and reshaping. In this tutorial, we also cover how to work with tensors in R and how to convert tensors into matrices in the R environment. This is particularly important when using Flair word embeddings.

</div>

&nbsp;

## The Overview

<div style="text-align: justify">

The following tutorial is mainly based on Tadej Magajna's '[__Natural Language Processing with Flair: A Practical Guide to Understanding and Solving NLP Problems__](https://github.com/PacktPublishing/Natural-Language-Processing-with-Flair)', as well as the official [Flair NLP](https://flairnlp.github.io/docs/intro) Python tutorial and blog. both are written in Python. If you utilize the examples from {flaiR} in R , I welcome you to cite this R repository, but you should also cite their works.  Except when necessary, everything will be accomplished within the R environment, utilizing several important R packages, such as {[quanteda](https://quanteda.org)}, {[`udpipe`](https://bnosac.github.io/udpipe/en/)}, and {[mlr3](https://mlr3.mlr-org.com)}, to complete the following topics:


- __[Sentence and Token Object](#articles/sentence_token.html)__

- __[Flair Embedding in R](#/articles/flair_embeddings.html)__

- __Sequence Taggings__ 

- __Text Classification__

- __Training the Model in fliaR in R__

- __Featured Function in fliaR__

</div>

&nbsp;

-----

# Sentence and Token

<div style="text-align: justify">

Sentence and Token are fundamental classes.

##  __Sentence__

A Sentence in Flair is an object that contains a sequence of Token objects, and it can be annotated with labels, such as named entities, part-of-speech tags, and more. It also can store embeddings for the sentence as a whole and different kinds of linguistic annotations.

Here’s a simple example of how you create a Sentence:

```{r}
# Creating a Sentence object
library(flaiR)
string <- "What I see in UCD today, what I have seen of UCD in its impact on my own life and the life of Ireland."
sentence <- flair_data.sentence(string)
```


`Sentence[26]` means that there are a total of 26 tokens in the sentence.

```{r}
print(sentence)
```

##  __Token__

When you use Flair to handle text data,[^1] `Sentence` and `Token` objects often play central roles in many use cases. When you create a Sentence object, it usually automatically decomposes the internal raw text into multiple Token objects. In other words, the Sentence object automatically handles the text tokenization work, so you usually don’t need to create Token objects manually. 

Unlike R, which indexes from 1, Python indexes from 0. Therefore, when I use a for loop, I use `seq_along(sentence) - 1`. The output should be something like:
```{r}
# The Sentence object has automatically created and contains multiple Token objects
# We can iterate through the Sentence object to view each Token.

for (i in seq_along(sentence)-1) {
  print(sentence[[i]])
}
```

Or you can directly use `$tokens` method to print all tokens.

```{r}
print(sentence$tokens)
```

[^1]: Flair is built on PyTorch, which is a library in Python. 


__Retrieve the Token__


To comprehend the string representation format of the Sentence object, tagging at least one token is adequate. The `get_token(n)` method, a Python method, allows us to retrieve the Token object for a particular token. Additionally, we can use __`[]`__ to index a specific token. It is noteworthy that Python indexes from 0, whereas R starts indexing from 1.

```{r}
# method in Python
sentence$get_token(5)
```


```{r}
# indexing in R 
sentence[4]
```

Each word (and punctuation) in the sentence is treated as an individual Token object. These Token objects store text information and other possible linguistic information (such as part-of-speech tags or named entity tags) and embeddings (if you used a model to generate them).

Even though in most cases you do not need to create Token objects manually, understanding how to manage these objects manually is still useful in some situations, such as when you want fine-grained control over the tokenization process. For example, you can control the exactness of tokenization by adding manually created Token objects to a Sentence object.

This design pattern in Flair allows users to handle text data in a very flexible way. Users can use the automatic tokenization feature for rapid development, and also perform finer-grained control to accommodate more use cases.

__Annotate POS tag and NER tag__

The `add_label(label_type, value)` method can be employed to assign a label to the token. We manually add a tag in this preliminary tutorial, so usually, in Universal POS tags, if `sentence[10]` is 'see', 'seen' might be tagged as `VERB`, indicating it is a past participle form of a verb.


```{r}
sentence[10]$add_label('manual-pos', 'VERB')
```


```{r}
print(sentence[10])
```

We can also add a NER (Named Entity Recognition) tag to `sentence[4]`, "UCD", identifying it as a university in Dublin.

```{r}
sentence[4]$add_label('ner', 'ORG')
```

```{r}
print(sentence[4])
```

If we print the sentence object, `Sentence[50]` provides information for 50 tokens → ['in'/ORG, 'seen'/VERB], thus displaying two tagging pieces of information.

```{r}
print(sentence)
```

</div>


# Flair Embedding 
<div style="text-align: justify">

Flair is a very popular natural language processing library, providing a variety of embedding methods for text representation through Flair.  Flair Embeddings is a word embedding framowork in Natural Language Processing, developed by the [Zalando](https://engineering.zalando.com/posts/2018/11/zalando-research-releases-flair.html). Flair  focuses on word-level representation and can capture contextual information of words, meaning that the same word can have different embeddings in different contexts. Unlike traditional word embeddings (such as Word2Vec or GloVe), Flair can dynamically generate word embeddings based on context and has achieved excellent results in various NLP tasks. Below are some key points about Flair Embeddings:

__Context-Aware__

Flair can understand the context of a word in a sentence and dynamically generate word embeddings based on this context. This is different from static embeddings, where the embedding of a word does not consider its context in a sentence.

__Character-Based__

Flair uses a character-level language model, meaning it can generate embeddings for rare words or even misspelled words. This is an important feature because it allows the model to understand and process words that have never appeared in the training data.

__Multilingual Support__

Flair provides various pre-trained character-level language models, supporting contextual word embeddings for multiple languages.

__Combinability__

Flair allows you to easily combine different word embeddings (e.g., Flair Embeddings, Word2Vec, GloVe, etc.) to create powerful stacked embeddings.

</div>


## Classic Wordembeddings

<div style="text-align: justify">

In Flair, the simplest form of embedding that still contains semantic information about the word is called classic word embeddings. These embeddings are pre-trained and non-contextual. Let's retrieve a few word embeddings. Then, we can utilize FastText embeddings with the following code. To use them, we simply instantiate a WordEmbeddings class by passing in the ID of the embedding of our choice. Then, we simply wrap our text into a Sentence object, and call the embed(sentence) method on our WordEmbeddings class.


```{r}
embedding = flair_embeddings.WordEmbeddings('crawl') 
sentence = flair_data.sentence("one two three one") 
embedding$embed(sentence) 

for (i in seq_along(sentence$tokens)) {
  print(head(sentence$tokens[[i]]$embedding), n =5)
}

```

Flair supports a range of classic word embeddings, each offering unique features and application scopes. Below is an overview, detailing the ID required to load each embedding and its corresponding language.


| Embedding Type                 | ID     | Language   |
|--------------------------------|--------|------------|
| GloVe                          | glove  | English    |
| Komninos                       | extvec | English    |
| Twitter                        | twitter| English    |
| Turian (small)                 | turian | English    |
| FastText (crawl)               | crawl  | English    |
| FastText (news & Wikipedia)    | ar     | Arabic     |
| FastText (news & Wikipedia)    | bg     | Bulgarian  |
| FastText (news & Wikipedia)    | ca     | Catalan    |
| FastText (news & Wikipedia)    | cz     | Czech      |
| FastText (news & Wikipedia)    | da     | Danish     |
| FastText (news & Wikipedia)    | de     | German     |
| FastText (news & Wikipedia)    | es     | Spanish    |
| FastText (news & Wikipedia)    | en     | English    |
| FastText (news & Wikipedia)    | eu     | Basque     |
| FastText (news & Wikipedia)    | fa     | Persian    |
| FastText (news & Wikipedia)    | fi     | Finnish    |
| FastText (news & Wikipedia)    | fr     | French     |
| FastText (news & Wikipedia)    | he     | Hebrew     |
| FastText (news & Wikipedia)    | hi     | Hindi      |
| FastText (news & Wikipedia)    | hr     | Croatian   |
| FastText (news & Wikipedia)    | id     | Indonesian |
| FastText (news & Wikipedia)    | it     | Italian    |
| FastText (news & Wikipedia)    | ja     | Japanese   |
| FastText (news & Wikipedia)    | ko     | Korean     |
| FastText (news & Wikipedia)    | nl     | Dutch      |
| FastText (news & Wikipedia)    | no     | Norwegian  |
| FastText (news & Wikipedia)    | pl     | Polish     |
| FastText (news & Wikipedia)    | pt     | Portuguese |
| FastText (news & Wikipedia)    | ro     | Romanian   |
| FastText (news & Wikipedia)    | ru     | Russian    |
| FastText (news & Wikipedia)    | si     | Slovenian  |
| FastText (news & Wikipedia)    | sk     | Slovak     |
| FastText (news & Wikipedia)    | sr     | Serbian    |
| FastText (news & Wikipedia)    | sv     | Swedish    |
| FastText (news & Wikipedia)    | tr     | Turkish    |
| FastText (news & Wikipedia)    | zh     | Chinese    |

</div>

&nbsp;

--- 

## Contexual Embeddings

<div style="text-align: justify">

Understanding the contextuality of Flair embeddings The idea behind contextual string embeddings is that each word embedding should be defined by not only its syntactic-semantic meaning but also the context it appears in. What this means is that each word will have a different embedding for every context it appears in. Each pre-trained Flair model offers a **forward** version and a **backward** version. Let's assume you are processing a language that, just like this book, uses the left-to-right script. The forward version takes into account the context that happens before the word – on the left-hand side. The backward version works in the opposite direction. It takes into account the context after the word – on the right-hand side of the word. If this is true, then two same words that appear at the beginning of two different sentences should have identical forward embeddings, because their context is null. Let's test this out:


Because we are using a forward model, it only takes into account the context that occurs before a word. Additionally, since our word has no context on the left-hand side of its position in the sentence, the two embeddings are identical, and the code assumes they are identical, indeed output is __True__.


```{r}
embedding <- flair_embeddings.FlairEmbeddings('news-forward')
s1 <- flair_data.sentence("nice shirt") 
s2 <- flair_data.sentence("nice pants") 

embedding$embed(s1) 
embedding$embed(s2) 
cat(" s1 sentence:", paste(s1[0], sep = ""), "\n", "s2 sentence:", paste(s2[0], sep = ""))
```


We test whether the sum of the two 2048 embeddings of 'nice' is equal to 2048. If it is true, it indicates that the embedding results are consistent, which should theoretically be the case.

```{r}
length(s1[0]$embedding$numpy()) == sum(s1[0]$embedding$numpy() ==  s2[0]$embedding$numpy())
```


Now we separately add a few more words, `very` and `pretty`, into two sentence objects.

```{r}
embedding <- flair_embeddings.FlairEmbeddings('news-forward')
s1 <- flair_data.sentence("nice shirt") 
s2 <- flair_data.sentence("nice pants") 
```

```{r}
embedding <- flair_embeddings.FlairEmbeddings('news-forward')
s1 <- flair_data.sentence("very nice shirt") 
s2 <- flair_data.sentence("pretty nice pants") 

embedding$embed(s1) 
embedding$embed(s2) 
```

The two sets of embeddings are not identical because the words are different, so it returns __False__.

```{r}
length(s1[0]$embedding$numpy()) == sum(s1[0]$embedding$numpy() ==  s2[0]$embedding$numpy())
```

The measure of similarity between two vectors in an inner product space is known as cosine similarity. The formula for calculating cosine similarity between two vectors, such as vectors A and B, is as follows:

$Cosine Similarity = \frac{\sum_{i} (A_i \cdot B_i)}{\sqrt{\sum_{i} (A_i^2)} \cdot \sqrt{\sum_{i} (B_i^2)}}$


```{r}
library(lsa)
vector1 <- as.numeric(s1[0]$embedding$numpy())
vector2 <- as.numeric(s2[0]$embedding$numpy())
```


We can observe that the similarity between the two words is 0.55.
```{r}
cosine_similarity <- cosine(vector1, vector2)
print(cosine_similarity)
```


</div>

&nbsp;

-----

## Extracting Embeddings from BERT

<div style="text-align: justify">

First, we utilize the flair.embeddings.TransformerWordEmbeddings function to download BERT, and more transformer models can also be found on [Flair NLP's Hugging Face](https://huggingface.co/flair).

```{r}
TransformerWordEmbeddings <- flair_embeddings.TransformerWordEmbeddings("bert-base-uncased")
```


```{r}
embedding <- TransformerWordEmbeddings$embed(sentence)
```

Traverse each token in the sentence and print them. To view each token, it's necessary to use` reticulate::py_str(token)` since the sentence is a Python object.

```{r}
# Iterate through each token in the sentence, printing them. 
# Utilize reticulate::py_str(token) to view each token, given that the sentence is a Python object.
for (i in seq_along(sentence$tokens)) {
  cat("Token: ", reticulate::py_str(sentence$tokens[[i]]), "\n")
  # Access the embedding of the token, converting it to an R object, 
  # and print the first 10 elements of the vector.
  token_embedding <- sentence$tokens[[i]]$embedding
  print(head(token_embedding, 10))
}
```


</div>


# Training a Binary Classifier in flaiR

<div style="text-align: justify">


In this section, we'll train a sentiment analysis model that can categorize text as either positive or negative. This case study is adapted from pages 116 to 130 of Tadej Magajna's book, '[Natural Language Processing with Flair](https://www.packtpub.com/product/natural-language-processing-with-flair/9781801072311)'. The process for training text classifiers in Flair mirrors the sequence followed for sequence labeling models. Specifically, the steps to train text classifiers are:

- Load a tagged corpus and compute the label dictionary map.
- Prepare the document embeddings.
- Initialize the `TextClassifier` class.
- Train the model.

</div>

## Loading a Tagged Corpus

<div style="text-align: justify">

Training text classification models requires a set of text documents (typically, sentences or paragraphs) where each document is associated with one or more classification labels. To train our sentiment analysis text classification model, we will be using the famous Internet Movie Database (IMDb) dataset, which contains 50,000 movie reviews from IMDB, where each review is labeled as either positive or negative. References to this dataset are already baked into Flair, so loading the dataset couldn't be easier:

```{r}
library(flaiR)
Corpus <- flair_data()$Corpus
IMDB <- flair_datasets()$IMDB

corpus = IMDB()
corpus$downsample(0.05)
lbl_type = 'sentiment'
label_dict = corpus$make_label_dictionary(label_type=lbl_type)

```

</div>

## Loading the Embeddings
<div style="text-align: justify">

In flair, iit covers all the different types of document embeddings that we can use. Here, we simply use `DocumentPoolEmbeddings`. They require no training prior to training the classification model itself:

```{r}
DocumentPoolEmbeddings <- flair_embeddings()$DocumentPoolEmbeddings
WordEmbeddings <- flair_embeddings()$WordEmbeddings
glove = WordEmbeddings('glove')
document_embeddings = DocumentPoolEmbeddings(glove)
```

</div>

## Initializing the TextClassifier Class

```{r}
# initiate TextClassifier
TextClassifier <- flair_models()$TextClassifier
classifier <- TextClassifier(document_embeddings,
                             label_dictionary = label_dict,
                             label_type = lbl_type)
```

## Training the Model

<div style="text-align: justify">


Training the text classifier model involves two simple steps:

- Defining the model trainer class by passing in the classifier model and the corpus
- Setting off the training process passing in the required training hyperparameters.

```{r}
# initiate ModelTrainer
ModelTrainer <- flair_trainers()$ModelTrainer

# fit the model
trainer <- ModelTrainer(classifier, corpus)

# start to train
trainer$train('classifier',
              learning_rate=0.1,
              mini_batch_size=32L,
              max_epochs=10L)
```
</div>

## Loading and Using the Classifiers
<div style="text-align: justify">

After training the text classification model, the resulting classifier will already be stored in memory as part of the classifier variable. It is possible, however, that your Python session exited after training. If so, you'll need to load the model into memory with the following:
```{r}
TextClassifier <- flair_models()$TextClassifier
classifier <- TextClassifier$load('classifier/best-model.pt')
```


We import the Sentence object. Now, we can generate predictions on some example text inputs.

```{r}
Sentence <- flair_data()$Sentence
```

```{r}
sentence <- Sentence("great")
classifier$predict(sentence)
print(sentence$labels)
```

```{r}
sentence <- Sentence("sad")
classifier$predict(sentence)
print(sentence$labels)
```

</div>

# Training a RNN with FlaiR
<div style="text-align: justify">

Here, we train a sentiment analysis model to categorize text. In this case, we also include a pipeline that implements the use of Recurrent Neural Networks (RNN). This makes them particularly effective for tasks involving sequential data.

</div>

## Import Necessary Libraries

```{r}
library(flaiR)
IMDB <- flair_datasets()$IMDB
WordEmbeddings <- flair_embeddings()$WordEmbeddings
FlairEmbeddings <- flair_embeddings()$FlairEmbeddings
DocumentRNNEmbeddings <- flair_embeddings()$DocumentRNNEmbeddings
TextClassifier <- flair_models()$TextClassifier
ModelTrainer <- flair_trainers()$ModelTrainer
```


## Get the IMDB Corpus
<div style="text-align: justify">

The IMDB movie review dataset is used here, which is a commonly utilized dataset for sentiment analysis. `$downsample(0.1)` method means only 10% of the dataset is used, allowing for a faster demonstration
```{r}
corpus <- IMDB()$downsample(0.1) 

# Create the label dictionary
lbl_type <- 'sentiment'
label_dict <- corpus$make_label_dictionary(label_type=lbl_type)
```

</div>

## Stacked Embeddings in flaiR 
<div style="text-align: justify">

This is one of Flair's most powerful features: it allows for the integration of embeddings to enable the model to learn from more sparse features. Three types of embeddings are utilized here: GloVe embeddings, and two types of Flair embeddings (forward and backward). Word embeddings are used to convert words into vectors.
```{r}
# Make a list of word embeddings
word_embeddings <- list(WordEmbeddings('glove'),
                        FlairEmbeddings('news-forward-fast'),
                        FlairEmbeddings('news-backward-fast'))

# Initialize the document embeddings

document_embeddings <- DocumentRNNEmbeddings(word_embeddings, 
                                             hidden_size = 512L,
                                             reproject_words = TRUE,
                                             reproject_words_dimension = 256L)

```

```{r}
# Create a Text Classifier with the embeddings and label dictionary
classifier = TextClassifier(document_embeddings, 
                            label_dictionary=label_dict, label_type='class')

# Initialize the text classifier trainer with our corpus
trainer = ModelTrainer(classifier, corpus)
```
</div>


## Start the Training
<div style="text-align: justify">

For the sake of this example, setting max_epochs to 5. You might want to increase this for better performance.

It is worth noting that  thelearning rate is a parameter that determines the step size at each iteration while moving towards a minimum of the loss function. A smaller learning rate could slow down the learning process, but it could lead to more precise convergence. `mini_batch_size` determines the number of samples that will be used to compute the gradient at each step. The 'L' in 32L is used in R to denote that the number is an integer.

patience (aka early stop) is a hyperparameter used in conjunction with early stopping to avoid overfitting. It determines the number of epochs the training process will tolerate without improvements before stopping the training.  Setting max_epochs to 5 means the algorithm will make five passes through the dataset.

```{r eval=FALSE, include=TRUE}
# The 'L' in 32L is used in R to denote that the number is an integer.
trainer$train('models/sentiment',
              learning_rate=0.1,
              mini_batch_size=32L,
              patience=5L,
              max_epochs=5L)  
```


</div>


## To Apply the Trained Model for Prediction

<div style="text-align: justify">

```{r eval=FALSE, include=TRUE}
sentence <- "This movie was really exciting!"
classifier$predict(sentence)
print(sentence.labels)
```

</div>
