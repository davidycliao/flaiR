<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="flaiR">
<title>Tutorial in R • flaiR</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.2.2/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.2.2/bootstrap.bundle.min.js"></script><link href="../deps/Source_Sans_Pro-0.4.7/font.css" rel="stylesheet">
<link href="../deps/Source_Code_Pro-0.4.7/font.css" rel="stylesheet">
<!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Tutorial in R">
<meta property="og:description" content="flaiR">
<meta property="og:image" content="https://davidycliao.github.io/flaiR/logo.png">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]--><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=G-3ZG40PPG98"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-3ZG40PPG98');
</script><script defer data-domain="{YOUR DOMAIN},all.tidyverse.org" src="https://plausible.io/js/plausible.js"></script>
</head>
<body>
    <a href="#container" class="visually-hidden-focusable">Skip to content</a>
    

    <nav class="navbar fixed-top navbar-inverse navbar-expand-lg bg-none"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">flaiR</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Released version">0.0.5</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../index.html">
    <span class="fa fa-home"></span>
     
  </a>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown--quick-start">
    <span class="fa fa-rocket"></span>
     
    Quick Start
  </a>
  <div class="dropdown-menu" aria-labelledby="dropdown--quick-start">
    <a class="dropdown-item" href="../articles/quickstart.html#why-write-%7BflaiR%7D-to-access-fair-nlp-in-python">Why {flaiR}</a>
    <a class="dropdown-item" href="../articles/quickstart.html#install-flair-with-using-remotes">Install flaiR via GitHub</a>
    <a class="dropdown-item" href="../articles/quickstart.html#wrapped-functions">Wrapped Functions</a>
    <a class="dropdown-item" href="../articles/quickstart.html#embeddings">FlaiR Embeddings</a>
    <a class="dropdown-item" href="../articles/quickstart.html#featured-functions-for-nlp-tasks-with-data-table-output">Features in flaiR</a>
    <a class="dropdown-item" href="../articles/quickstart.html#how-to-contribute">How to Contribute</a>
  </div>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown--tutorial-in-r">
    <span class="fa fa-project-diagram"></span>
     
    Tutorial in R
  </a>
  <div class="dropdown-menu" aria-labelledby="dropdown--tutorial-in-r">
    <a class="dropdown-item" href="../articles/tutorial.html#introduction">Introduction</a>
    <a class="dropdown-item" href="../articles/tutorial.html#sentence-and-token">Sentence and Token</a>
    <a class="dropdown-item" href="../articles/tutorial.html#sequence-taggings">Sequence Taggings</a>
    <a class="dropdown-item" href="../articles/tutorial.html#flair-embedding">Flair Embeddings</a>
    <a class="dropdown-item" href="../articles/tutorial.html#training-a-binary-classifier-in-flair">Training a Binary Classifier in flaiR</a>
    <a class="dropdown-item" href="../articles/tutorial.html#training-a-rnn-with-flair">Training a RNN with FlaiR</a>
  </div>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown--toolbox">
    <span class="fa fa-newspaper-o"></span>
     
    ToolBox
  </a>
  <div class="dropdown-menu" aria-labelledby="dropdown--toolbox">
    <a class="dropdown-item" href="../articles/flair_models.html">Flar Pre-trianed Models</a>
    <a class="dropdown-item" href="../articles/transformer_wordembeddings.html">Wordembeddings in Flair</a>
    <a class="dropdown-item" href="../articles/highlight_text.html">Coloring Entities</a>
    <h6 class="dropdown-header" data-toc-skip>Visualizing the Sentiments (Comming Soon)</h6>
  </div>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown--reference">
    <span class="fa fa-file-code-o"></span>
     
    Reference
  </a>
  <div class="dropdown-menu" aria-labelledby="dropdown--reference">
    <a class="dropdown-item" href="../reference/index.html#wrapper-functions">Wrapper Functions</a>
    <a class="dropdown-item" href="../reference/index.html#designed-flair-functions">Designed flaiR Functions</a>
    <a class="dropdown-item" href="../reference/index.html#tool-box">ToolBox</a>
    <a class="dropdown-item" href="../reference/index.html#utility-functions">Utility</a>
    <a class="dropdown-item" href="../reference/index.html#example-dataset">Dataset</a>
  </div>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown--news">
    <span class="fa fa-newspaper-o"></span>
     
    News
  </a>
  <div class="dropdown-menu" aria-labelledby="dropdown--news">
    <a class="dropdown-item" href="../news/index.html#flair-005-development-version">0.0.5</a>
    <a class="dropdown-item" href="../news/index.html#flair-004-development-version">0.0.4</a>
    <a class="dropdown-item" href="../news/index.html#flair-003-development-version">0.0.3</a>
    <a class="dropdown-item" href="../news/index.html#flair-001-development-version">0.0.1</a>
  </div>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/davidycliao/flaiR">
    <span class="fa fa-github fa-lg"></span>
     
    GitHub
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article" id="container">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>Tutorial in R</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/davidycliao/flaiR/blob/HEAD/vignettes/tutorial.Rmd" class="external-link"><code>vignettes/tutorial.Rmd</code></a></small>
      <div class="d-none name"><code>tutorial.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<div style="text-align: justify">
<p><strong>Flair NLP</strong> is an open-source library for Natural
Language Processing (NLP) developed by <a href="https://github.com/zalandoresearch/" class="external-link">Zalando Research</a>. Known
for its state-of-the-art solutions for NLP tasks like Named Entity
Recognition (NER), Part-of-Speech tagging (POS), and more, it has
garnered attention in the NLP community for its ease of use and powerful
functionalities. Developed in Python, and built on the PyTorch
framework, it offers a flexible and dynamic approach to deal with
textual data. On the other hand, {flaiR} in R aims to continue the
framework established by Flair in Python by creating a framework for R,
thereby extending Flair’s capabilities to the R programming
environment.</p>
<p>One of the hallmark features of Flair is its <strong>contextual
string embeddings</strong>, which is crucial in discerning the meaning
of words in different contextual usages. Traditional embeddings assign a
fixed vector to a word, without considering its context, which can be a
limitation when trying to understand the nuances in the word’s usage
across different sentences. On the contrary, Flair’s contextual
embeddings generate word vectors by considering the surrounding text,
thus capturing the word’s context and semantics more accurately. This is
particularly impactful in scenarios where a word can have different
meanings based on its usage.</p>
<p>Flair offers pre-trained models for various languages and tasks,
providing a solid foundation for various NLP applications such as text
classification, sentiment analysis, and entity recognition, etc. For
instance, if you’re involved in a project that requires identifying
persons, organizations, or locations from text, Flair has pre-trained
NER models that can simplify this task.</p>
</div>
<div class="section level3">
<h3 id="oop-in-r-when-introducing-python">OOP in R when Introducing Python<a class="anchor" aria-label="anchor" href="#oop-in-r-when-introducing-python"></a>
</h3>
<div style="text-align: justify">
<p>Object-Oriented Programming (OOP) is a programming paradigm that uses
objects, which contain both data (attributes) and functions (methods),
to design applications and software. The idea is to bind the data and
the methods that operate on that data into one single unit, an object.
Before the advent of R6, OOP was not very common in the early stages of
R. To my knowledge, R6 is relatively rare; aside from {<a href="https://mlr3.mlr-org.com" class="external-link">mlr3</a>}, which is written in R6, most
packages are accomplished in S4 and S3 (to my personal experience),
which, of course, may be greatly related to the habits and tasks of R
users. However, the purpose of {<code>flaiR</code>} is to standardize
wrapping the ‘{flair NLP}’ Python functionality in R and to provide more
convenient access for R users to utilize flair NLP features. Most usage
of Flair NLP within the {flaiR} employs concepts of objects and classes,
which are similar to R6. However, these features are packaged in
{reticulate} from Python. In other words, some functionalities imported
into R essentially belong to Python classes or modules.</p>
<p>In addition, tensors serve as a fundamental building block for
creating and training neural networks and for conducting various
numerical computations in Python. For all of Flair’s NLP tasks in Python
on PyTorch, there are numerous extensive functionalities for tensor
operations, including element-wise operations, matrix multiplications,
and reshaping. In this tutorial, we also cover how to work with tensors
in R and how to convert tensors into matrices in the R environment. This
is particularly important when using Flair word embeddings in R
environment.</p>
</div>
<p> </p>
</div>
<div class="section level3">
<h3 id="the-overview">The Overview<a class="anchor" aria-label="anchor" href="#the-overview"></a>
</h3>
<div style="text-align: justify">
<p>The following tutorial is mainly based on Tadej Magajna’s ‘<a href="https://github.com/PacktPublishing/Natural-Language-Processing-with-Flair" class="external-link"><strong>Natural
Language Processing with Flair: A Practical Guide to Understanding and
Solving NLP Problems</strong></a>’, as well as the official <a href="https://flairnlp.github.io/docs/intro" class="external-link">Flair NLP</a> Python
tutorial and blog. both are written in Python. If you utilize the
examples from {flaiR} in R , I welcome you to cite this R repository,
but you should also cite their works. Except when necessary, everything
will be accomplished within the R environment, utilizing several
important R packages, such as {<a href="https://quanteda.org" class="external-link">quanteda</a>}, {<a href="https://bnosac.github.io/udpipe/en/" class="external-link"><code>udpipe</code></a>}, and
{<a href="https://mlr3.mlr-org.com" class="external-link">mlr3</a>}, to complete the following
topics:</p>
<ul>
<li><p><strong><a href="#tutorial.html#sentence-and-token">Sentence and
Token Object</a></strong></p></li>
<li><p><strong><a href="#tutorial.html#sequence-taggings">Sequence
Taggings</a></strong></p></li>
<li><p><strong><a href="#tutorial.html#flair-embedding">Embedding in
flaiR</a></strong></p></li>
<li><p><strong><a href="#tutorial.html#training-a-binary-classifier-in-flair">Training a
Binary Classifier in flaiR</a></strong></p></li>
<li><p><strong><a href="#tutorial.html#training-a-rnn-with-flair">Training a RNN with
FlaiR</a></strong></p></li>
<li><p><strong><a href="">Finetune BERT in FlaiR (Comming
Soon)</a></strong></p></li>
</ul>
</div>
<p> </p>
<hr>
</div>
</div>
<div class="section level2">
<h2 id="sentence-and-token">Sentence and Token<a class="anchor" aria-label="anchor" href="#sentence-and-token"></a>
</h2>
<div style="text-align: justify">
<p>Sentence and Token are fundamental classes.</p>
<div class="section level3">
<h3 id="sentence">
<strong>Sentence</strong><a class="anchor" aria-label="anchor" href="#sentence"></a>
</h3>
<p>A Sentence in Flair is an object that contains a sequence of Token
objects, and it can be annotated with labels, such as named entities,
part-of-speech tags, and more. It also can store embeddings for the
sentence as a whole and different kinds of linguistic annotations.</p>
<p>Here’s a simple example of how you create a Sentence:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Creating a Sentence object</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va">flaiR</span><span class="op">)</span></span>
<span><span class="va">string</span> <span class="op">&lt;-</span> <span class="st">"What I see in UCD today, what I have seen of UCD in its impact on my own life and the life of Ireland."</span></span>
<span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.sentence.html">flair_data.sentence</a></span><span class="op">(</span><span class="va">string</span><span class="op">)</span></span></code></pre></div>
<p><code>Sentence[26]</code> means that there are a total of 26 tokens
in the sentence.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="co">#&gt; Sentence[26]: "What I see in UCD today, what I have seen of UCD in its impact on my own life and the life of Ireland."</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="token">
<strong>Token</strong><a class="anchor" aria-label="anchor" href="#token"></a>
</h3>
<p>When you use Flair to handle text data,<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content="&lt;p&gt;Flair is built on PyTorch, which is a library in
Python.&lt;/p&gt;"><sup>1</sup></a> <code>Sentence</code>
and <code>Token</code> objects often play central roles in many use
cases. When you create a Sentence object, it usually automatically
decomposes the internal raw text into multiple Token objects. In other
words, the Sentence object automatically handles the text tokenization
work, so you usually don’t need to create Token objects manually.</p>
<p>Unlike R, which indexes from 1, Python indexes from 0. Therefore,
when I use a for loop, I use <code>seq_along(sentence) - 1</code>. The
output should be something like:</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># The Sentence object has automatically created and contains multiple Token objects</span></span>
<span><span class="co"># We can iterate through the Sentence object to view each Token.</span></span>
<span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq_along</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="co">#&gt; Token[0]: "What"</span></span>
<span><span class="co">#&gt; Token[1]: "I"</span></span>
<span><span class="co">#&gt; Token[2]: "see"</span></span>
<span><span class="co">#&gt; Token[3]: "in"</span></span>
<span><span class="co">#&gt; Token[4]: "UCD"</span></span>
<span><span class="co">#&gt; Token[5]: "today"</span></span>
<span><span class="co">#&gt; Token[6]: ","</span></span>
<span><span class="co">#&gt; Token[7]: "what"</span></span>
<span><span class="co">#&gt; Token[8]: "I"</span></span>
<span><span class="co">#&gt; Token[9]: "have"</span></span>
<span><span class="co">#&gt; Token[10]: "seen"</span></span>
<span><span class="co">#&gt; Token[11]: "of"</span></span>
<span><span class="co">#&gt; Token[12]: "UCD"</span></span>
<span><span class="co">#&gt; Token[13]: "in"</span></span>
<span><span class="co">#&gt; Token[14]: "its"</span></span>
<span><span class="co">#&gt; Token[15]: "impact"</span></span>
<span><span class="co">#&gt; Token[16]: "on"</span></span>
<span><span class="co">#&gt; Token[17]: "my"</span></span>
<span><span class="co">#&gt; Token[18]: "own"</span></span>
<span><span class="co">#&gt; Token[19]: "life"</span></span>
<span><span class="co">#&gt; Token[20]: "and"</span></span>
<span><span class="co">#&gt; Token[21]: "the"</span></span>
<span><span class="co">#&gt; Token[22]: "life"</span></span>
<span><span class="co">#&gt; Token[23]: "of"</span></span>
<span><span class="co">#&gt; Token[24]: "Ireland"</span></span>
<span><span class="co">#&gt; Token[25]: "."</span></span></code></pre></div>
<p>Or you can directly use <code>$tokens</code> method to print all
tokens.</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="va">tokens</span><span class="op">)</span></span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Token[0]: "What"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[2]]</span></span>
<span><span class="co">#&gt; Token[1]: "I"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[3]]</span></span>
<span><span class="co">#&gt; Token[2]: "see"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[4]]</span></span>
<span><span class="co">#&gt; Token[3]: "in"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[5]]</span></span>
<span><span class="co">#&gt; Token[4]: "UCD"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[6]]</span></span>
<span><span class="co">#&gt; Token[5]: "today"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[7]]</span></span>
<span><span class="co">#&gt; Token[6]: ","</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[8]]</span></span>
<span><span class="co">#&gt; Token[7]: "what"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[9]]</span></span>
<span><span class="co">#&gt; Token[8]: "I"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[10]]</span></span>
<span><span class="co">#&gt; Token[9]: "have"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[11]]</span></span>
<span><span class="co">#&gt; Token[10]: "seen"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[12]]</span></span>
<span><span class="co">#&gt; Token[11]: "of"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[13]]</span></span>
<span><span class="co">#&gt; Token[12]: "UCD"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[14]]</span></span>
<span><span class="co">#&gt; Token[13]: "in"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[15]]</span></span>
<span><span class="co">#&gt; Token[14]: "its"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[16]]</span></span>
<span><span class="co">#&gt; Token[15]: "impact"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[17]]</span></span>
<span><span class="co">#&gt; Token[16]: "on"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[18]]</span></span>
<span><span class="co">#&gt; Token[17]: "my"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[19]]</span></span>
<span><span class="co">#&gt; Token[18]: "own"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[20]]</span></span>
<span><span class="co">#&gt; Token[19]: "life"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[21]]</span></span>
<span><span class="co">#&gt; Token[20]: "and"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[22]]</span></span>
<span><span class="co">#&gt; Token[21]: "the"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[23]]</span></span>
<span><span class="co">#&gt; Token[22]: "life"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[24]]</span></span>
<span><span class="co">#&gt; Token[23]: "of"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[25]]</span></span>
<span><span class="co">#&gt; Token[24]: "Ireland"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[26]]</span></span>
<span><span class="co">#&gt; Token[25]: "."</span></span></code></pre></div>
<p><strong>Retrieve the Token</strong></p>
<p>To comprehend the string representation format of the Sentence
object, tagging at least one token is adequate. The
<code>get_token(n)</code> method, a Python method, allows us to retrieve
the Token object for a particular token. Additionally, we can use
<strong><code>[]</code></strong> to index a specific token. It is
noteworthy that Python indexes from 0, whereas R starts indexing from
1.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># method in Python</span></span>
<span><span class="va">sentence</span><span class="op">$</span><span class="fu">get_token</span><span class="op">(</span><span class="fl">5</span><span class="op">)</span></span>
<span><span class="co">#&gt; Token[4]: "UCD"</span></span></code></pre></div>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># indexing in R </span></span>
<span><span class="va">sentence</span><span class="op">[</span><span class="fl">4</span><span class="op">]</span></span>
<span><span class="co">#&gt; Token[4]: "UCD"</span></span></code></pre></div>
<p>Each word (and punctuation) in the sentence is treated as an
individual Token object. These Token objects store text information and
other possible linguistic information (such as part-of-speech tags or
named entity tags) and embeddings (if you used a model to generate
them).</p>
<p>Even though in most cases you do not need to create Token objects
manually, understanding how to manage these objects manually is still
useful in some situations, such as when you want fine-grained control
over the tokenization process. For example, you can control the
exactness of tokenization by adding manually created Token objects to a
Sentence object.</p>
<p>This design pattern in Flair allows users to handle text data in a
very flexible way. Users can use the automatic tokenization feature for
rapid development, and also perform finer-grained control to accommodate
more use cases.</p>
<p><strong>Annotate POS tag and NER tag</strong></p>
<p>The <code>add_label(label_type, value)</code> method can be employed
to assign a label to the token. We manually add a tag in this
preliminary tutorial, so usually, in Universal POS tags, if
<code>sentence[10]</code> is ‘see’, ‘seen’ might be tagged as
<code>VERB</code>, indicating it is a past participle form of a
verb.</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">sentence</span><span class="op">[</span><span class="fl">10</span><span class="op">]</span><span class="op">$</span><span class="fu">add_label</span><span class="op">(</span><span class="st">'manual-pos'</span>, <span class="st">'VERB'</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">[</span><span class="fl">10</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="co">#&gt; Token[10]: "seen" → VERB (1.0)</span></span></code></pre></div>
<p>We can also add a NER (Named Entity Recognition) tag to
<code>sentence[4]</code>, “UCD”, identifying it as a university in
Dublin.</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">sentence</span><span class="op">[</span><span class="fl">4</span><span class="op">]</span><span class="op">$</span><span class="fu">add_label</span><span class="op">(</span><span class="st">'ner'</span>, <span class="st">'ORG'</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">[</span><span class="fl">4</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="co">#&gt; Token[4]: "UCD" → ORG (1.0)</span></span></code></pre></div>
<p>If we print the sentence object, <code>Sentence[50]</code> provides
information for 50 tokens → [‘in’/ORG, ‘seen’/VERB], thus displaying two
tagging pieces of information.</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="co">#&gt; Sentence[26]: "What I see in UCD today, what I have seen of UCD in its impact on my own life and the life of Ireland." → ["UCD"/ORG, "seen"/VERB]</span></span></code></pre></div>
</div>
</div>
<p> </p>
<hr>
</div>
<div class="section level2">
<h2 id="sequence-taggings">Sequence Taggings<a class="anchor" aria-label="anchor" href="#sequence-taggings"></a>
</h2>
<div class="section level3">
<h3 id="tag-entities-in-text">
<strong>Tag Entities in Text</strong><a class="anchor" aria-label="anchor" href="#tag-entities-in-text"></a>
</h3>
<div style="text-align: justify">
<p>Let’s run named entity recognition over the following example
sentence: “I love Berlin and New York. To do this, all you need is to
make a Sentence for this text, load a pre-trained model and use it to
predict tags for the sentence object.</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># attach flaiR in R</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va">flaiR</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># make a sentence</span></span>
<span></span>
<span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span>
<span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">'I love Berlin and New York.'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># load the NER tagger</span></span>
<span><span class="va">Classifier</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_nn.html">flair_nn</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Classifier</span></span>
<span><span class="va">tagger</span> <span class="op">&lt;-</span> <span class="va">Classifier</span><span class="op">$</span><span class="fu">load</span><span class="op">(</span><span class="st">'ner'</span><span class="op">)</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:12,188 SequenceTagger predicts: Dictionary with 20 tags: &lt;unk&gt;, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, &lt;START&gt;, &lt;STOP&gt;</span></span>
<span></span>
<span><span class="co"># run NER over sentence</span></span>
<span><span class="va">tagger</span><span class="op">$</span><span class="fu">predict</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span></code></pre></div>
<p>This should print:</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># print the sentence with all annotations</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="co">#&gt; Sentence[7]: "I love Berlin and New York." → ["Berlin"/LOC, "New York"/LOC]</span></span></code></pre></div>
<p>Use a for loop to print out each POS tag. It’s important to note that
Python is indexed from 0. Therefore, in an R environment, we must use
<code>seq_along(sentence$get_labels()) - 1</code>.</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq_along</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="fu">get_labels</span><span class="op">(</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="fu">get_labels</span><span class="op">(</span><span class="op">)</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span><span class="co">#&gt; 'Span[2:3]: "Berlin"'/'LOC' (0.9812)</span></span>
<span><span class="co">#&gt; 'Span[4:6]: "New York"'/'LOC' (0.9957)</span></span></code></pre></div>
</div>
<p> </p>
</div>
<div class="section level3">
<h3 id="tag-part-of-speech-in-text">
<strong>Tag Part-of-Speech in Text</strong><a class="anchor" aria-label="anchor" href="#tag-part-of-speech-in-text"></a>
</h3>
<div style="text-align: justify">
<p>We use flair/pos-english for POS tagging in the standard models on
Hugging Face.</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># attach flaiR in R</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va">flaiR</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># make a sentence</span></span>
<span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span>
<span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">'I love Berlin and New York.'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># load the NER tagger</span></span>
<span><span class="va">Classifier</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_nn.html">flair_nn</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Classifier</span></span>
<span><span class="va">tagger</span> <span class="op">&lt;-</span> <span class="va">Classifier</span><span class="op">$</span><span class="fu">load</span><span class="op">(</span><span class="st">'pos'</span><span class="op">)</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:13,052 SequenceTagger predicts: Dictionary with 53 tags: &lt;unk&gt;, O, UH, ,, VBD, PRP, VB, PRP$, NN, RB, ., DT, JJ, VBP, VBG, IN, CD, NNS, NNP, WRB, VBZ, WDT, CC, TO, MD, VBN, WP, :, RP, EX, JJR, FW, XX, HYPH, POS, RBR, JJS, PDT, NNPS, RBS, AFX, WP$, -LRB-, -RRB-, ``, '', LS, $, SYM, ADD</span></span>
<span></span>
<span><span class="co"># run NER over sentence</span></span>
<span><span class="va">tagger</span><span class="op">$</span><span class="fu">predict</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span></code></pre></div>
<p>This should print:</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># print the sentence with all annotations</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="co">#&gt; Sentence[7]: "I love Berlin and New York." → ["I"/PRP, "love"/VBP, "Berlin"/NNP, "and"/CC, "New"/NNP, "York"/NNP, "."/.]</span></span></code></pre></div>
<p>Use a for loop to print out each pos tag.</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq_along</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="fu">get_labels</span><span class="op">(</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="fu">get_labels</span><span class="op">(</span><span class="op">)</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span><span class="co">#&gt; 'Token[0]: "I"'/'PRP' (1.0)</span></span>
<span><span class="co">#&gt; 'Token[1]: "love"'/'VBP' (1.0)</span></span>
<span><span class="co">#&gt; 'Token[2]: "Berlin"'/'NNP' (0.9999)</span></span>
<span><span class="co">#&gt; 'Token[3]: "and"'/'CC' (1.0)</span></span>
<span><span class="co">#&gt; 'Token[4]: "New"'/'NNP' (1.0)</span></span>
<span><span class="co">#&gt; 'Token[5]: "York"'/'NNP' (1.0)</span></span>
<span><span class="co">#&gt; 'Token[6]: "."'/'.' (1.0)</span></span></code></pre></div>
</div>
<p> </p>
</div>
<div class="section level3">
<h3 id="detect-sentiment">
<strong>Detect Sentiment</strong><a class="anchor" aria-label="anchor" href="#detect-sentiment"></a>
</h3>
<div style="text-align: justify">
<p>Let’s run sentiment analysis over the same sentence to determine
whether it is POSITIVE or NEGATIVE.</p>
<p>You can do this with essentially the same code as above. Just instead
of loading the ‘ner’ model, you now load the ‘sentiment’ model:</p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># attach flaiR in R</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va">flaiR</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># make a sentence</span></span>
<span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span>
<span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">'I love Berlin and New York.'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># load the flair_nn.classifier_load tagger</span></span>
<span><span class="va">Classifier</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_nn.html">flair_nn</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Classifier</span></span>
<span><span class="va">tagger</span> <span class="op">&lt;-</span> <span class="va">Classifier</span><span class="op">$</span><span class="fu">load</span><span class="op">(</span><span class="st">'sentiment'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># run sentiment analysis over sentence</span></span>
<span><span class="va">tagger</span><span class="op">$</span><span class="fu">predict</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># print the sentence with all annotations</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="co">#&gt; Sentence[7]: "I love Berlin and New York." → POSITIVE (0.9982)</span></span></code></pre></div>
</div>
<p> </p>
<p> </p>
<hr>
</div>
</div>
<div class="section level2">
<h2 id="flair-embedding">Flair Embedding<a class="anchor" aria-label="anchor" href="#flair-embedding"></a>
</h2>
<div style="text-align: justify">
<p>Flair is a very popular natural language processing library,
providing a variety of embedding methods for text representation through
Flair. Flair Embeddings is a word embedding framowork in Natural
Language Processing, developed by the <a href="https://engineering.zalando.com/posts/2018/11/zalando-research-releases-flair.html" class="external-link">Zalando</a>.
Flair focuses on word-level representation and can capture contextual
information of words, meaning that the same word can have different
embeddings in different contexts. Unlike traditional word embeddings
(such as Word2Vec or GloVe), Flair can dynamically generate word
embeddings based on context and has achieved excellent results in
various NLP tasks. Below are some key points about Flair Embeddings:</p>
<p><strong>Context-Aware</strong></p>
<p>Flair can understand the context of a word in a sentence and
dynamically generate word embeddings based on this context. This is
different from static embeddings, where the embedding of a word does not
consider its context in a sentence.</p>
<p><strong>Character-Based</strong></p>
<p>Flair uses a character-level language model, meaning it can generate
embeddings for rare words or even misspelled words. This is an important
feature because it allows the model to understand and process words that
have never appeared in the training data.</p>
<p><strong>Multilingual Support</strong></p>
<p>Flair provides various pre-trained character-level language models,
supporting contextual word embeddings for multiple languages.</p>
<p><strong>Combinability</strong></p>
<p>Flair allows you to easily combine different word embeddings (e.g.,
Flair Embeddings, Word2Vec, GloVe, etc.) to create powerful stacked
embeddings.</p>
</div>
<div class="section level3">
<h3 id="classic-wordembeddings">Classic Wordembeddings<a class="anchor" aria-label="anchor" href="#classic-wordembeddings"></a>
</h3>
<div style="text-align: justify">
<p>In Flair, the simplest form of embedding that still contains semantic
information about the word is called classic word embeddings. These
embeddings are pre-trained and non-contextual. Let’s retrieve a few word
embeddings. Then, we can utilize FastText embeddings with the following
code. To use them, we simply instantiate a WordEmbeddings class by
passing in the ID of the embedding of our choice. Then, we simply wrap
our text into a Sentence object, and call the embed(sentence) method on
our WordEmbeddings class.</p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">embedding</span> <span class="op">=</span> <span class="fu"><a href="../reference/flair_embeddings.WordEmbeddings.html">flair_embeddings.WordEmbeddings</a></span><span class="op">(</span><span class="st">'crawl'</span><span class="op">)</span> </span>
<span><span class="va">sentence</span> <span class="op">=</span> <span class="fu"><a href="../reference/flair_data.sentence.html">flair_data.sentence</a></span><span class="op">(</span><span class="st">"one two three one"</span><span class="op">)</span> </span>
<span><span class="va">embedding</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span> </span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Sentence[4]: "one two three one"</span></span>
<span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq_along</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="va">tokens</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="va">tokens</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">)</span>, n <span class="op">=</span><span class="fl">5</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="co">#&gt; tensor([-0.0535, -0.0368, -0.2851, -0.0381, -0.0486,  0.2383])</span></span>
<span><span class="co">#&gt; tensor([ 0.0282, -0.0786, -0.1236,  0.1756, -0.1199,  0.0964])</span></span>
<span><span class="co">#&gt; tensor([-0.0920, -0.0690, -0.1475,  0.2313, -0.0872,  0.0799])</span></span>
<span><span class="co">#&gt; tensor([-0.0535, -0.0368, -0.2851, -0.0381, -0.0486,  0.2383])</span></span></code></pre></div>
<p>Flair supports a range of classic word embeddings, each offering
unique features and application scopes. Below is an overview, detailing
the ID required to load each embedding and its corresponding
language.</p>
<table class="table">
<thead><tr class="header">
<th>Embedding Type</th>
<th>ID</th>
<th>Language</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>GloVe</td>
<td>glove</td>
<td>English</td>
</tr>
<tr class="even">
<td>Komninos</td>
<td>extvec</td>
<td>English</td>
</tr>
<tr class="odd">
<td>Twitter</td>
<td>twitter</td>
<td>English</td>
</tr>
<tr class="even">
<td>Turian (small)</td>
<td>turian</td>
<td>English</td>
</tr>
<tr class="odd">
<td>FastText (crawl)</td>
<td>crawl</td>
<td>English</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>ar</td>
<td>Arabic</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>bg</td>
<td>Bulgarian</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>ca</td>
<td>Catalan</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>cz</td>
<td>Czech</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>da</td>
<td>Danish</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>de</td>
<td>German</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>es</td>
<td>Spanish</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>en</td>
<td>English</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>eu</td>
<td>Basque</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>fa</td>
<td>Persian</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>fi</td>
<td>Finnish</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>fr</td>
<td>French</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>he</td>
<td>Hebrew</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>hi</td>
<td>Hindi</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>hr</td>
<td>Croatian</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>id</td>
<td>Indonesian</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>it</td>
<td>Italian</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>ja</td>
<td>Japanese</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>ko</td>
<td>Korean</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>nl</td>
<td>Dutch</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>no</td>
<td>Norwegian</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>pl</td>
<td>Polish</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>pt</td>
<td>Portuguese</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>ro</td>
<td>Romanian</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>ru</td>
<td>Russian</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>si</td>
<td>Slovenian</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>sk</td>
<td>Slovak</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>sr</td>
<td>Serbian</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>sv</td>
<td>Swedish</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>tr</td>
<td>Turkish</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>zh</td>
<td>Chinese</td>
</tr>
</tbody>
</table>
</div>
<p> </p>
<hr>
</div>
<div class="section level3">
<h3 id="contexual-embeddings">Contexual Embeddings<a class="anchor" aria-label="anchor" href="#contexual-embeddings"></a>
</h3>
<div style="text-align: justify">
<p>Understanding the contextuality of Flair embeddings The idea behind
contextual string embeddings is that each word embedding should be
defined by not only its syntactic-semantic meaning but also the context
it appears in. What this means is that each word will have a different
embedding for every context it appears in. Each pre-trained Flair model
offers a <strong>forward</strong> version and a
<strong>backward</strong> version. Let’s assume you are processing a
language that, just like this book, uses the left-to-right script. The
forward version takes into account the context that happens before the
word – on the left-hand side. The backward version works in the opposite
direction. It takes into account the context after the word – on the
right-hand side of the word. If this is true, then two same words that
appear at the beginning of two different sentences should have identical
forward embeddings, because their context is null. Let’s test this
out:</p>
<p>Because we are using a forward model, it only takes into account the
context that occurs before a word. Additionally, since our word has no
context on the left-hand side of its position in the sentence, the two
embeddings are identical, and the code assumes they are identical,
indeed output is <strong>True</strong>.</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">embedding</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.FlairEmbeddings.html">flair_embeddings.FlairEmbeddings</a></span><span class="op">(</span><span class="st">'news-forward'</span><span class="op">)</span></span>
<span><span class="co">#&gt; Initialized Flair forward embeddings</span></span>
<span><span class="va">s1</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.sentence.html">flair_data.sentence</a></span><span class="op">(</span><span class="st">"nice shirt"</span><span class="op">)</span> </span>
<span><span class="va">s2</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.sentence.html">flair_data.sentence</a></span><span class="op">(</span><span class="st">"nice pants"</span><span class="op">)</span> </span>
<span></span>
<span><span class="va">embedding</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">s1</span><span class="op">)</span> </span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Sentence[2]: "nice shirt"</span></span>
<span><span class="va">embedding</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">s2</span><span class="op">)</span> </span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Sentence[2]: "nice pants"</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">" s1 sentence:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste</a></span><span class="op">(</span><span class="va">s1</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span>, sep <span class="op">=</span> <span class="st">""</span><span class="op">)</span>, <span class="st">"\n"</span>, <span class="st">"s2 sentence:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste</a></span><span class="op">(</span><span class="va">s2</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span>, sep <span class="op">=</span> <span class="st">""</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt;  s1 sentence: Token[0]: "nice" </span></span>
<span><span class="co">#&gt;  s2 sentence: Token[0]: "nice"</span></span></code></pre></div>
<p>We test whether the sum of the two 2048 embeddings of ‘nice’ is equal
to 2048. If it is true, it indicates that the embedding results are
consistent, which should theoretically be the case.</p>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">s1</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">numpy</span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">==</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">s1</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">numpy</span><span class="op">(</span><span class="op">)</span> <span class="op">==</span>  <span class="va">s2</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">numpy</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] TRUE</span></span></code></pre></div>
<p>Now we separately add a few more words, <code>very</code> and
<code>pretty</code>, into two sentence objects.</p>
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">embedding</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.FlairEmbeddings.html">flair_embeddings.FlairEmbeddings</a></span><span class="op">(</span><span class="st">'news-forward'</span><span class="op">)</span></span>
<span><span class="co">#&gt; Initialized Flair forward embeddings</span></span>
<span><span class="va">s1</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.sentence.html">flair_data.sentence</a></span><span class="op">(</span><span class="st">"nice shirt"</span><span class="op">)</span> </span>
<span><span class="va">s2</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.sentence.html">flair_data.sentence</a></span><span class="op">(</span><span class="st">"nice pants"</span><span class="op">)</span> </span></code></pre></div>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">embedding</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.FlairEmbeddings.html">flair_embeddings.FlairEmbeddings</a></span><span class="op">(</span><span class="st">'news-forward'</span><span class="op">)</span></span>
<span><span class="co">#&gt; Initialized Flair forward embeddings</span></span>
<span><span class="va">s1</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.sentence.html">flair_data.sentence</a></span><span class="op">(</span><span class="st">"very nice shirt"</span><span class="op">)</span> </span>
<span><span class="va">s2</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.sentence.html">flair_data.sentence</a></span><span class="op">(</span><span class="st">"pretty nice pants"</span><span class="op">)</span> </span>
<span></span>
<span><span class="va">embedding</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">s1</span><span class="op">)</span> </span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Sentence[3]: "very nice shirt"</span></span>
<span><span class="va">embedding</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">s2</span><span class="op">)</span> </span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Sentence[3]: "pretty nice pants"</span></span></code></pre></div>
<p>The two sets of embeddings are not identical because the words are
different, so it returns <strong>False</strong>.</p>
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">s1</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">numpy</span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">==</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">s1</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">numpy</span><span class="op">(</span><span class="op">)</span> <span class="op">==</span>  <span class="va">s2</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">numpy</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] FALSE</span></span></code></pre></div>
<p>The measure of similarity between two vectors in an inner product
space is known as cosine similarity. The formula for calculating cosine
similarity between two vectors, such as vectors A and B, is as
follows:</p>
<p><span class="math inline">\(Cosine Similarity = \frac{\sum_{i} (A_i
\cdot B_i)}{\sqrt{\sum_{i} (A_i^2)} \cdot \sqrt{\sum_{i}
(B_i^2)}}\)</span></p>
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va">lsa</span><span class="op">)</span></span>
<span><span class="co">#&gt; Loading required package: SnowballC</span></span>
<span><span class="va">vector1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html" class="external-link">as.numeric</a></span><span class="op">(</span><span class="va">s1</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">numpy</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">vector2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html" class="external-link">as.numeric</a></span><span class="op">(</span><span class="va">s2</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">numpy</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>We can observe that the similarity between the two words is 0.55.</p>
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cosine_similarity</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/lsa/man/cosine.html" class="external-link">cosine</a></span><span class="op">(</span><span class="va">vector1</span>, <span class="va">vector2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">cosine_similarity</span><span class="op">)</span></span>
<span><span class="co">#&gt;           [,1]</span></span>
<span><span class="co">#&gt; [1,] 0.5571664</span></span></code></pre></div>
</div>
<p> </p>
<hr>
</div>
<div class="section level3">
<h3 id="extracting-embeddings-from-bert">Extracting Embeddings from BERT<a class="anchor" aria-label="anchor" href="#extracting-embeddings-from-bert"></a>
</h3>
<div style="text-align: justify">
<p>First, we utilize the flair.embeddings.TransformerWordEmbeddings
function to download BERT, and more transformer models can also be found
on <a href="https://huggingface.co/flair" class="external-link">Flair NLP’s Hugging
Face</a>.</p>
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">TransformerWordEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.TransformerWordEmbeddings.html">flair_embeddings.TransformerWordEmbeddings</a></span><span class="op">(</span><span class="st">"bert-base-uncased"</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb29"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">embedding</span> <span class="op">&lt;-</span> <span class="va">TransformerWordEmbeddings</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span></code></pre></div>
<p>Traverse each token in the sentence and print them. To view each
token, it’s necessary to use<code>reticulate::py_str(token)</code> since
the sentence is a Python object.</p>
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Iterate through each token in the sentence, printing them. </span></span>
<span><span class="co"># Utilize reticulate::py_str(token) to view each token, given that the sentence is a Python object.</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq_along</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="va">tokens</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"Token: "</span>, <span class="fu">reticulate</span><span class="fu">::</span><span class="fu"><a href="https://rstudio.github.io/reticulate/reference/py_str.html" class="external-link">py_str</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="va">tokens</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span><span class="op">)</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span>  <span class="co"># Access the embedding of the token, converting it to an R object, </span></span>
<span>  <span class="co"># and print the first 10 elements of the vector.</span></span>
<span>  <span class="va">token_embedding</span> <span class="op">&lt;-</span> <span class="va">sentence</span><span class="op">$</span><span class="va">tokens</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">token_embedding</span>, <span class="fl">10</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="co">#&gt; Token:  Token[0]: "one" </span></span>
<span><span class="co">#&gt; tensor([-0.0535, -0.0368, -0.2851, -0.0381, -0.0486,  0.2383, -0.1200,  0.2620,</span></span>
<span><span class="co">#&gt;         -0.0575,  0.0228])</span></span>
<span><span class="co">#&gt; Token:  Token[1]: "two" </span></span>
<span><span class="co">#&gt; tensor([ 0.0282, -0.0786, -0.1236,  0.1756, -0.1199,  0.0964, -0.1327,  0.4449,</span></span>
<span><span class="co">#&gt;         -0.0264, -0.1168])</span></span>
<span><span class="co">#&gt; Token:  Token[2]: "three" </span></span>
<span><span class="co">#&gt; tensor([-0.0920, -0.0690, -0.1475,  0.2313, -0.0872,  0.0799, -0.0901,  0.4403,</span></span>
<span><span class="co">#&gt;         -0.0103, -0.1494])</span></span>
<span><span class="co">#&gt; Token:  Token[3]: "one" </span></span>
<span><span class="co">#&gt; tensor([-0.0535, -0.0368, -0.2851, -0.0381, -0.0486,  0.2383, -0.1200,  0.2620,</span></span>
<span><span class="co">#&gt;         -0.0575,  0.0228])</span></span></code></pre></div>
</div>
<p> </p>
<hr>
</div>
</div>
<div class="section level2">
<h2 id="training-a-binary-classifier-in-flair">Training a Binary Classifier in flaiR<a class="anchor" aria-label="anchor" href="#training-a-binary-classifier-in-flair"></a>
</h2>
<div style="text-align: justify">
<p>In this section, we’ll train a sentiment analysis model that can
categorize text as either positive or negative. This case study is
adapted from pages 116 to 130 of Tadej Magajna’s book, ‘<a href="https://www.packtpub.com/product/natural-language-processing-with-flair/9781801072311" class="external-link">Natural
Language Processing with Flair</a>’. The process for training text
classifiers in Flair mirrors the sequence followed for sequence labeling
models. Specifically, the steps to train text classifiers are:</p>
<ul>
<li>Load a tagged corpus and compute the label dictionary map.</li>
<li>Prepare the document embeddings.</li>
<li>Initialize the <code>TextClassifier</code> class.</li>
<li>Train the model.</li>
</ul>
</div>
<div class="section level3">
<h3 id="loading-a-tagged-corpus">Loading a Tagged Corpus<a class="anchor" aria-label="anchor" href="#loading-a-tagged-corpus"></a>
</h3>
<div style="text-align: justify">
<p>Training text classification models requires a set of text documents
(typically, sentences or paragraphs) where each document is associated
with one or more classification labels. To train our sentiment analysis
text classification model, we will be using the famous Internet Movie
Database (IMDb) dataset, which contains 50,000 movie reviews from IMDB,
where each review is labeled as either positive or negative. References
to this dataset are already baked into Flair, so loading the dataset
couldn’t be easier:</p>
<div class="sourceCode" id="cb31"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va">flaiR</span><span class="op">)</span></span>
<span><span class="co"># load IMDB from flair_datasets module</span></span>
<span><span class="va">Corpus</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Corpus</span></span>
<span><span class="va">IMDB</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_datasets.html">flair_datasets</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">IMDB</span></span></code></pre></div>
<div class="sourceCode" id="cb32"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># downsize to 0.05</span></span>
<span><span class="va">corpus</span> <span class="op">=</span> <span class="fu">IMDB</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:22,649 Reading data from /Users/yenchiehliao/.flair/datasets/imdb_v4-rebalanced</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:22,649 Train: /Users/yenchiehliao/.flair/datasets/imdb_v4-rebalanced/train.txt</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:22,649 Dev: None</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:22,649 Test: None</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:23,219 Initialized corpus /Users/yenchiehliao/.flair/datasets/imdb_v4-rebalanced (label type name is 'sentiment')</span></span>
<span><span class="va">corpus</span><span class="op">$</span><span class="fu">downsample</span><span class="op">(</span><span class="fl">0.05</span><span class="op">)</span></span>
<span><span class="co">#&gt; &lt;flair.datasets.document_classification.IMDB object at 0x2dca18ed0&gt;</span></span></code></pre></div>
<p>Print the sizes in the corpus object as follows - test: %d | train:
%d | dev: %d”</p>
<div class="sourceCode" id="cb33"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">test_size</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">corpus</span><span class="op">$</span><span class="va">test</span><span class="op">)</span></span>
<span><span class="va">train_size</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">corpus</span><span class="op">$</span><span class="va">train</span><span class="op">)</span></span>
<span><span class="va">dev_size</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">corpus</span><span class="op">$</span><span class="va">dev</span><span class="op">)</span></span>
<span><span class="va">output</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sprintf.html" class="external-link">sprintf</a></span><span class="op">(</span><span class="st">"Corpus object sizes - Test: %d | Train: %d | Dev: %d"</span>, <span class="va">test_size</span>, <span class="va">train_size</span>, <span class="va">dev_size</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">output</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "Corpus object sizes - Test: 250 | Train: 2025 | Dev: 225"</span></span></code></pre></div>
<div class="sourceCode" id="cb34"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">lbl_type</span> <span class="op">=</span> <span class="st">'sentiment'</span></span>
<span><span class="va">label_dict</span> <span class="op">=</span> <span class="va">corpus</span><span class="op">$</span><span class="fu">make_label_dictionary</span><span class="op">(</span>label_type<span class="op">=</span><span class="va">lbl_type</span><span class="op">)</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:23,332 Computing label dictionary. Progress:</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:26,975 Dictionary created for label 'sentiment' with 3 values: NEGATIVE (seen 1040 times), POSITIVE (seen 985 times)</span></span></code></pre></div>
</div>
</div>
<div class="section level3">
<h3 id="loading-the-embeddings">Loading the Embeddings<a class="anchor" aria-label="anchor" href="#loading-the-embeddings"></a>
</h3>
<div style="text-align: justify">
<p>In flair, iit covers all the different types of document embeddings
that we can use. Here, we simply use
<code>DocumentPoolEmbeddings</code>. They require no training prior to
training the classification model itself:</p>
<div class="sourceCode" id="cb35"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">DocumentPoolEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">DocumentPoolEmbeddings</span></span>
<span><span class="va">WordEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">WordEmbeddings</span></span>
<span><span class="va">glove</span> <span class="op">=</span> <span class="fu">WordEmbeddings</span><span class="op">(</span><span class="st">'glove'</span><span class="op">)</span></span>
<span><span class="va">document_embeddings</span> <span class="op">=</span> <span class="fu">DocumentPoolEmbeddings</span><span class="op">(</span><span class="va">glove</span><span class="op">)</span></span></code></pre></div>
</div>
</div>
<div class="section level3">
<h3 id="initializing-the-textclassifier-class">Initializing the TextClassifier Class<a class="anchor" aria-label="anchor" href="#initializing-the-textclassifier-class"></a>
</h3>
<div style="text-align: justify">
<div class="sourceCode" id="cb36"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># initiate TextClassifier</span></span>
<span><span class="va">TextClassifier</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_models.html">flair_models</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">TextClassifier</span></span>
<span><span class="va">classifier</span> <span class="op">&lt;-</span> <span class="fu">TextClassifier</span><span class="op">(</span><span class="va">document_embeddings</span>,</span>
<span>                             label_dictionary <span class="op">=</span> <span class="va">label_dict</span>,</span>
<span>                             label_type <span class="op">=</span> <span class="va">lbl_type</span><span class="op">)</span></span></code></pre></div>
<p><code>$to</code> allows you to set the device to use CPU, GPU, or
specific MPS devices on Mac (such as mps:0, mps:1, mps:2).</p>
<div class="sourceCode" id="cb37"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">classifier</span><span class="op">$</span><span class="fu">to</span><span class="op">(</span><span class="fu"><a href="../reference/flair_device.html">flair_device</a></span><span class="op">(</span><span class="st">"mps"</span><span class="op">)</span><span class="op">)</span> </span></code></pre></div>
<pre><code><span><span class="fu">TextClassifier</span><span class="op">(</span></span>
<span>  <span class="op">(</span><span class="va">embeddings</span><span class="op">)</span><span class="op">:</span> <span class="fu">DocumentPoolEmbeddings</span><span class="op">(</span></span>
<span>    fine_tune_mode<span class="op">=</span><span class="va">none</span>, pooling<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span></span>
<span>    <span class="op">(</span><span class="va">embeddings</span><span class="op">)</span><span class="op">:</span> <span class="fu">StackedEmbeddings</span><span class="op">(</span></span>
<span>      <span class="op">(</span><span class="va">list_embedding_0</span><span class="op">)</span><span class="op">:</span> <span class="fu">WordEmbeddings</span><span class="op">(</span></span>
<span>        <span class="st">'glove'</span></span>
<span>        <span class="op">(</span><span class="va">embedding</span><span class="op">)</span><span class="op">:</span> <span class="fu">Embedding</span><span class="op">(</span><span class="fl">400001</span>, <span class="fl">100</span><span class="op">)</span></span>
<span>      <span class="op">)</span></span>
<span>    <span class="op">)</span></span>
<span>  <span class="op">)</span></span>
<span>  <span class="op">(</span><span class="va">decoder</span><span class="op">)</span><span class="op">:</span> <span class="fu">Linear</span><span class="op">(</span>in_features<span class="op">=</span><span class="fl">100</span>, out_features<span class="op">=</span><span class="fl">3</span>, bias<span class="op">=</span><span class="va">True</span><span class="op">)</span></span>
<span>  <span class="op">(</span><span class="va">dropout</span><span class="op">)</span><span class="op">:</span> <span class="fu">Dropout</span><span class="op">(</span>p<span class="op">=</span><span class="fl">0.0</span>, inplace<span class="op">=</span><span class="va">False</span><span class="op">)</span></span>
<span>  <span class="op">(</span><span class="va">locked_dropout</span><span class="op">)</span><span class="op">:</span> <span class="fu">LockedDropout</span><span class="op">(</span>p<span class="op">=</span><span class="fl">0.0</span><span class="op">)</span></span>
<span>  <span class="op">(</span><span class="va">word_dropout</span><span class="op">)</span><span class="op">:</span> <span class="fu">WordDropout</span><span class="op">(</span>p<span class="op">=</span><span class="fl">0.0</span><span class="op">)</span></span>
<span>  <span class="op">(</span><span class="va">loss_function</span><span class="op">)</span><span class="op">:</span> <span class="fu">CrossEntropyLoss</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre>
</div>
</div>
<div class="section level3">
<h3 id="training-the-model">Training the Model<a class="anchor" aria-label="anchor" href="#training-the-model"></a>
</h3>
<div style="text-align: justify">
<p>Training the text classifier model involves two simple steps: -
Defining the model trainer class by passing in the classifier model and
the corpus - Setting off the training process passing in the required
training hyperparameters.</p>
<p><strong>It is worth noting that the ‘L’ in numbers like 32L and 5L is
used in R to denote that the number is an integer. Without the ‘L’
suffix, numbers in R are treated as numeric, which are by default
double-precision floating-point numbers. In contrast, Python determines
the type based on the value of the number itself. Whole numbers (e.g., 5
or 32) are of type int, while numbers with decimal points (e.g., 5.0)
are of type float. Floating-point numbers in both languages are
representations of real numbers but can have some approximation due to
the way they are stored in memory.</strong></p>
<div class="sourceCode" id="cb39"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># initiate ModelTrainer</span></span>
<span><span class="va">ModelTrainer</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_trainers.html">flair_trainers</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">ModelTrainer</span></span>
<span></span>
<span><span class="co"># fit the model</span></span>
<span><span class="va">trainer</span> <span class="op">&lt;-</span> <span class="fu">ModelTrainer</span><span class="op">(</span><span class="va">classifier</span>, <span class="va">corpus</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># start to train</span></span>
<span><span class="co"># note: the 'L' in 32L is used in R to denote that the number is an integer.</span></span>
<span><span class="va">trainer</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="st">'classifier'</span>,</span>
<span>              learning_rate<span class="op">=</span><span class="fl">0.1</span>,</span>
<span>              mini_batch_size<span class="op">=</span><span class="fl">32L</span>,</span>
<span>              <span class="co"># specifies how embeddings are stored in RAM, ie."cpu", "cuda", "gpu", "mps".</span></span>
<span>              <span class="co"># embeddings_storage_mode = "mps",</span></span>
<span>              max_epochs<span class="op">=</span><span class="fl">10L</span><span class="op">)</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:28,868 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:28,868 Model: "TextClassifier(</span></span>
<span><span class="co">#&gt;   (embeddings): DocumentPoolEmbeddings(</span></span>
<span><span class="co">#&gt;     fine_tune_mode=none, pooling=mean</span></span>
<span><span class="co">#&gt;     (embeddings): StackedEmbeddings(</span></span>
<span><span class="co">#&gt;       (list_embedding_0): WordEmbeddings(</span></span>
<span><span class="co">#&gt;         'glove'</span></span>
<span><span class="co">#&gt;         (embedding): Embedding(400001, 100)</span></span>
<span><span class="co">#&gt;       )</span></span>
<span><span class="co">#&gt;     )</span></span>
<span><span class="co">#&gt;   )</span></span>
<span><span class="co">#&gt;   (decoder): Linear(in_features=100, out_features=3, bias=True)</span></span>
<span><span class="co">#&gt;   (dropout): Dropout(p=0.0, inplace=False)</span></span>
<span><span class="co">#&gt;   (locked_dropout): LockedDropout(p=0.0)</span></span>
<span><span class="co">#&gt;   (word_dropout): WordDropout(p=0.0)</span></span>
<span><span class="co">#&gt;   (loss_function): CrossEntropyLoss()</span></span>
<span><span class="co">#&gt;   (weights): None</span></span>
<span><span class="co">#&gt;   (weight_tensor) None</span></span>
<span><span class="co">#&gt; )"</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:28,868 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:28,868 Corpus: "Corpus: 2025 train + 225 dev + 250 test sentences"</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:28,868 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:28,868 Parameters:</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:28,868  - learning_rate: "0.100000"</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:28,868  - mini_batch_size: "32"</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:28,868  - patience: "3"</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:28,868  - anneal_factor: "0.5"</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:28,868  - max_epochs: "10"</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:28,868  - shuffle: "True"</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:28,868  - train_with_dev: "False"</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:28,868  - batch_growth_annealing: "False"</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:28,868 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:28,868 Model training base path: "classifier"</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:28,868 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:28,868 Device: cpu</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:28,868 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:28,868 Embeddings storage mode: cpu</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:28,868 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:29,546 epoch 1 - iter 6/64 - loss 0.17421939 - time (sec): 0.68 - samples/sec: 283.57 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:30,344 epoch 1 - iter 12/64 - loss 0.08795083 - time (sec): 1.48 - samples/sec: 260.25 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:31,115 epoch 1 - iter 18/64 - loss 0.05899506 - time (sec): 2.25 - samples/sec: 256.41 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:31,897 epoch 1 - iter 24/64 - loss 0.04442133 - time (sec): 3.03 - samples/sec: 253.62 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:32,499 epoch 1 - iter 30/64 - loss 0.03563552 - time (sec): 3.63 - samples/sec: 264.43 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:33,291 epoch 1 - iter 36/64 - loss 0.34862558 - time (sec): 4.42 - samples/sec: 260.47 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:34,087 epoch 1 - iter 42/64 - loss 0.29912602 - time (sec): 5.22 - samples/sec: 257.56 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:34,813 epoch 1 - iter 48/64 - loss 0.26186525 - time (sec): 5.94 - samples/sec: 258.39 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:35,565 epoch 1 - iter 54/64 - loss 0.23284634 - time (sec): 6.70 - samples/sec: 258.05 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:36,161 epoch 1 - iter 60/64 - loss 0.20961277 - time (sec): 7.29 - samples/sec: 263.28 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:36,584 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:36,584 EPOCH 1 done: loss 0.1988 - lr 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:37,702 Evaluating as a multi-label problem: False</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:37,707 DEV : loss 4.092826843261719 - f1-score (micro avg)  0.5067</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:38,095 BAD EPOCHS (no improvement): 0</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:38,096 saving best model</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:38,465 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:39,312 epoch 2 - iter 6/64 - loss 1.96033063 - time (sec): 0.85 - samples/sec: 226.74 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:40,105 epoch 2 - iter 12/64 - loss 1.46618138 - time (sec): 1.64 - samples/sec: 234.16 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:40,736 epoch 2 - iter 18/64 - loss 1.27617172 - time (sec): 2.27 - samples/sec: 253.69 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:41,564 epoch 2 - iter 24/64 - loss 1.19680186 - time (sec): 3.10 - samples/sec: 247.83 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:42,322 epoch 2 - iter 30/64 - loss 1.16214700 - time (sec): 3.86 - samples/sec: 248.94 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:43,050 epoch 2 - iter 36/64 - loss 1.13151993 - time (sec): 4.58 - samples/sec: 251.26 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:43,707 epoch 2 - iter 42/64 - loss 1.10091302 - time (sec): 5.24 - samples/sec: 256.41 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:44,439 epoch 2 - iter 48/64 - loss 1.07882052 - time (sec): 5.97 - samples/sec: 257.12 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:45,161 epoch 2 - iter 54/64 - loss 1.05865800 - time (sec): 6.70 - samples/sec: 258.06 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:45,951 epoch 2 - iter 60/64 - loss 1.02944283 - time (sec): 7.49 - samples/sec: 256.49 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:46,258 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:46,258 EPOCH 2 done: loss 1.0179 - lr 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:47,401 Evaluating as a multi-label problem: False</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:47,406 DEV : loss 0.7175589203834534 - f1-score (micro avg)  0.4933</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:47,974 BAD EPOCHS (no improvement): 1</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:47,974 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:48,738 epoch 3 - iter 6/64 - loss 0.92861721 - time (sec): 0.76 - samples/sec: 251.54 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:49,687 epoch 3 - iter 12/64 - loss 0.90488524 - time (sec): 1.71 - samples/sec: 224.23 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:50,418 epoch 3 - iter 18/64 - loss 0.88640979 - time (sec): 2.44 - samples/sec: 235.71 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:51,086 epoch 3 - iter 24/64 - loss 0.87210764 - time (sec): 3.11 - samples/sec: 246.86 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:51,880 epoch 3 - iter 30/64 - loss 0.85630859 - time (sec): 3.91 - samples/sec: 245.83 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:52,651 epoch 3 - iter 36/64 - loss 0.84800965 - time (sec): 4.68 - samples/sec: 246.36 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:53,366 epoch 3 - iter 42/64 - loss 0.83953758 - time (sec): 5.39 - samples/sec: 249.26 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:53,990 epoch 3 - iter 48/64 - loss 0.83359408 - time (sec): 6.02 - samples/sec: 255.36 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:54,777 epoch 3 - iter 54/64 - loss 0.83153667 - time (sec): 6.80 - samples/sec: 254.03 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:55,515 epoch 3 - iter 60/64 - loss 0.82868384 - time (sec): 7.54 - samples/sec: 254.61 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:55,912 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:55,912 EPOCH 3 done: loss 0.8307 - lr 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:56,967 Evaluating as a multi-label problem: False</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:56,972 DEV : loss 0.7263646125793457 - f1-score (micro avg)  0.4978</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:57,503 BAD EPOCHS (no improvement): 2</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:57,503 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:58,271 epoch 4 - iter 6/64 - loss 0.85024741 - time (sec): 0.77 - samples/sec: 250.16 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:58,918 epoch 4 - iter 12/64 - loss 0.85543307 - time (sec): 1.41 - samples/sec: 271.58 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:12:59,691 epoch 4 - iter 18/64 - loss 0.85299979 - time (sec): 2.19 - samples/sec: 263.37 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:00,473 epoch 4 - iter 24/64 - loss 0.85456784 - time (sec): 2.97 - samples/sec: 258.61 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:01,322 epoch 4 - iter 30/64 - loss 0.85155124 - time (sec): 3.82 - samples/sec: 251.43 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:01,995 epoch 4 - iter 36/64 - loss 0.85096497 - time (sec): 4.49 - samples/sec: 256.51 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:02,729 epoch 4 - iter 42/64 - loss 0.84714862 - time (sec): 5.23 - samples/sec: 257.20 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:03,524 epoch 4 - iter 48/64 - loss 0.85725941 - time (sec): 6.02 - samples/sec: 255.14 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:04,298 epoch 4 - iter 54/64 - loss 0.86073934 - time (sec): 6.79 - samples/sec: 254.34 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:04,962 epoch 4 - iter 60/64 - loss 0.85536322 - time (sec): 7.46 - samples/sec: 257.43 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:05,497 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:05,497 EPOCH 4 done: loss 0.8470 - lr 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:06,544 Evaluating as a multi-label problem: False</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:06,549 DEV : loss 0.7379472255706787 - f1-score (micro avg)  0.5067</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:07,076 BAD EPOCHS (no improvement): 0</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:07,077 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:07,764 epoch 5 - iter 6/64 - loss 0.83285094 - time (sec): 0.69 - samples/sec: 279.55 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:08,475 epoch 5 - iter 12/64 - loss 0.82839541 - time (sec): 1.40 - samples/sec: 274.75 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:09,138 epoch 5 - iter 18/64 - loss 0.82598301 - time (sec): 2.06 - samples/sec: 279.45 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:09,868 epoch 5 - iter 24/64 - loss 0.81520855 - time (sec): 2.79 - samples/sec: 275.21 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:10,675 epoch 5 - iter 30/64 - loss 0.80822741 - time (sec): 3.60 - samples/sec: 266.83 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:11,369 epoch 5 - iter 36/64 - loss 0.82178961 - time (sec): 4.29 - samples/sec: 268.40 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:12,100 epoch 5 - iter 42/64 - loss 0.82795326 - time (sec): 5.02 - samples/sec: 267.56 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:12,945 epoch 5 - iter 48/64 - loss 0.84166880 - time (sec): 5.87 - samples/sec: 261.77 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:13,753 epoch 5 - iter 54/64 - loss 0.82371768 - time (sec): 6.68 - samples/sec: 258.83 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:14,569 epoch 5 - iter 60/64 - loss 0.81841860 - time (sec): 7.49 - samples/sec: 256.28 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:14,979 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:14,979 EPOCH 5 done: loss 0.8176 - lr 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:16,024 Evaluating as a multi-label problem: False</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:16,028 DEV : loss 0.7230531573295593 - f1-score (micro avg)  0.52</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:16,521 BAD EPOCHS (no improvement): 0</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:16,522 saving best model</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:16,786 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:17,570 epoch 6 - iter 6/64 - loss 0.79619130 - time (sec): 0.78 - samples/sec: 244.74 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:18,196 epoch 6 - iter 12/64 - loss 0.80090753 - time (sec): 1.41 - samples/sec: 272.39 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:18,920 epoch 6 - iter 18/64 - loss 0.80931118 - time (sec): 2.13 - samples/sec: 269.87 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:19,635 epoch 6 - iter 24/64 - loss 0.81293423 - time (sec): 2.85 - samples/sec: 269.52 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:20,482 epoch 6 - iter 30/64 - loss 0.80778593 - time (sec): 3.70 - samples/sec: 259.72 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:21,266 epoch 6 - iter 36/64 - loss 0.79714093 - time (sec): 4.48 - samples/sec: 257.12 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:22,046 epoch 6 - iter 42/64 - loss 0.79778157 - time (sec): 5.26 - samples/sec: 255.49 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:22,701 epoch 6 - iter 48/64 - loss 0.80304135 - time (sec): 5.92 - samples/sec: 259.65 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:23,411 epoch 6 - iter 54/64 - loss 0.79755635 - time (sec): 6.63 - samples/sec: 260.81 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:24,143 epoch 6 - iter 60/64 - loss 0.80102819 - time (sec): 7.36 - samples/sec: 260.98 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:24,570 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:24,570 EPOCH 6 done: loss 0.7979 - lr 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:25,600 Evaluating as a multi-label problem: False</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:25,605 DEV : loss 0.5859230160713196 - f1-score (micro avg)  0.68</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:26,123 BAD EPOCHS (no improvement): 0</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:26,123 saving best model</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:26,404 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:27,281 epoch 7 - iter 6/64 - loss 0.88685140 - time (sec): 0.88 - samples/sec: 219.14 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:28,116 epoch 7 - iter 12/64 - loss 0.85385170 - time (sec): 1.71 - samples/sec: 224.36 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:28,916 epoch 7 - iter 18/64 - loss 0.86259840 - time (sec): 2.51 - samples/sec: 229.32 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:29,550 epoch 7 - iter 24/64 - loss 0.84591044 - time (sec): 3.15 - samples/sec: 244.20 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:30,410 epoch 7 - iter 30/64 - loss 0.85748077 - time (sec): 4.01 - samples/sec: 239.69 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:31,153 epoch 7 - iter 36/64 - loss 0.85939245 - time (sec): 4.75 - samples/sec: 242.61 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:31,763 epoch 7 - iter 42/64 - loss 0.85637574 - time (sec): 5.36 - samples/sec: 250.83 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:32,554 epoch 7 - iter 48/64 - loss 0.85708664 - time (sec): 6.15 - samples/sec: 249.80 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:33,298 epoch 7 - iter 54/64 - loss 0.85312813 - time (sec): 6.89 - samples/sec: 250.69 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:34,113 epoch 7 - iter 60/64 - loss 0.84615744 - time (sec): 7.71 - samples/sec: 249.08 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:34,391 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:34,394 EPOCH 7 done: loss 0.8403 - lr 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:35,532 Evaluating as a multi-label problem: False</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:35,537 DEV : loss 0.7124922871589661 - f1-score (micro avg)  0.56</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:36,073 BAD EPOCHS (no improvement): 1</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:36,073 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:36,776 epoch 8 - iter 6/64 - loss 0.87290491 - time (sec): 0.70 - samples/sec: 273.25 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:37,568 epoch 8 - iter 12/64 - loss 0.91864170 - time (sec): 1.49 - samples/sec: 256.95 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:38,389 epoch 8 - iter 18/64 - loss 0.93381403 - time (sec): 2.32 - samples/sec: 248.71 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:39,042 epoch 8 - iter 24/64 - loss 0.91107347 - time (sec): 2.97 - samples/sec: 258.72 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:39,794 epoch 8 - iter 30/64 - loss 0.87565225 - time (sec): 3.72 - samples/sec: 258.02 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:40,567 epoch 8 - iter 36/64 - loss 0.85483213 - time (sec): 4.49 - samples/sec: 256.39 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:41,366 epoch 8 - iter 42/64 - loss 0.83894238 - time (sec): 5.29 - samples/sec: 253.93 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:42,136 epoch 8 - iter 48/64 - loss 0.83149641 - time (sec): 6.06 - samples/sec: 253.36 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:42,764 epoch 8 - iter 54/64 - loss 0.83184350 - time (sec): 6.69 - samples/sec: 258.28 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:43,523 epoch 8 - iter 60/64 - loss 0.81742891 - time (sec): 7.45 - samples/sec: 257.73 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:43,933 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:43,933 EPOCH 8 done: loss 0.8118 - lr 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:45,092 Evaluating as a multi-label problem: False</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:45,097 DEV : loss 0.7182886004447937 - f1-score (micro avg)  0.5644</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:45,669 BAD EPOCHS (no improvement): 2</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:45,669 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:46,375 epoch 9 - iter 6/64 - loss 0.83474032 - time (sec): 0.71 - samples/sec: 271.95 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:47,158 epoch 9 - iter 12/64 - loss 0.79708511 - time (sec): 1.49 - samples/sec: 257.99 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:47,933 epoch 9 - iter 18/64 - loss 0.81765712 - time (sec): 2.26 - samples/sec: 254.47 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:48,637 epoch 9 - iter 24/64 - loss 0.80566325 - time (sec): 2.97 - samples/sec: 258.75 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:49,642 epoch 9 - iter 30/64 - loss 0.80037763 - time (sec): 3.97 - samples/sec: 241.62 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:50,276 epoch 9 - iter 36/64 - loss 0.80936289 - time (sec): 4.61 - samples/sec: 250.07 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:51,091 epoch 9 - iter 42/64 - loss 0.79895339 - time (sec): 5.42 - samples/sec: 247.87 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:51,668 epoch 9 - iter 48/64 - loss 0.80182302 - time (sec): 6.00 - samples/sec: 256.03 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:52,413 epoch 9 - iter 54/64 - loss 0.80438170 - time (sec): 6.74 - samples/sec: 256.24 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:53,188 epoch 9 - iter 60/64 - loss 0.80128219 - time (sec): 7.52 - samples/sec: 255.36 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:53,552 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:53,552 EPOCH 9 done: loss 0.7986 - lr 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:54,527 Evaluating as a multi-label problem: False</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:54,532 DEV : loss 0.7260346412658691 - f1-score (micro avg)  0.5733</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:55,031 BAD EPOCHS (no improvement): 3</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:55,032 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:55,860 epoch 10 - iter 6/64 - loss 0.77774240 - time (sec): 0.83 - samples/sec: 231.87 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:56,589 epoch 10 - iter 12/64 - loss 0.81084799 - time (sec): 1.56 - samples/sec: 246.58 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:57,358 epoch 10 - iter 18/64 - loss 0.82203421 - time (sec): 2.33 - samples/sec: 247.63 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:58,026 epoch 10 - iter 24/64 - loss 0.82163049 - time (sec): 2.99 - samples/sec: 256.54 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:58,710 epoch 10 - iter 30/64 - loss 0.80265871 - time (sec): 3.68 - samples/sec: 261.01 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:13:59,465 epoch 10 - iter 36/64 - loss 0.79418393 - time (sec): 4.43 - samples/sec: 259.86 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:14:00,097 epoch 10 - iter 42/64 - loss 0.78602786 - time (sec): 5.06 - samples/sec: 265.36 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:14:00,821 epoch 10 - iter 48/64 - loss 0.77861462 - time (sec): 5.79 - samples/sec: 265.35 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:14:01,574 epoch 10 - iter 54/64 - loss 0.78331391 - time (sec): 6.54 - samples/sec: 264.15 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:14:02,184 epoch 10 - iter 60/64 - loss 0.78270700 - time (sec): 7.15 - samples/sec: 268.45 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:14:02,596 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2023-10-20 17:14:02,596 EPOCH 10 done: loss 0.7819 - lr 0.100000</span></span>
<span><span class="co">#&gt; 2023-10-20 17:14:03,690 Evaluating as a multi-label problem: False</span></span>
<span><span class="co">#&gt; 2023-10-20 17:14:03,695 DEV : loss 0.7278090119361877 - f1-score (micro avg)  0.5778</span></span>
<span><span class="co">#&gt; 2023-10-20 17:14:04,192 Epoch    10: reducing learning rate of group 0 to 5.0000e-02.</span></span>
<span><span class="co">#&gt; 2023-10-20 17:14:04,192 BAD EPOCHS (no improvement): 4</span></span>
<span><span class="co">#&gt; 2023-10-20 17:14:04,447 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2023-10-20 17:14:05,789 Evaluating as a multi-label problem: False</span></span>
<span><span class="co">#&gt; 2023-10-20 17:14:05,793 0.648    0.648   0.648   0.648</span></span>
<span><span class="co">#&gt; 2023-10-20 17:14:05,793 </span></span>
<span><span class="co">#&gt; Results:</span></span>
<span><span class="co">#&gt; - F-score (micro) 0.648</span></span>
<span><span class="co">#&gt; - F-score (macro) 0.6367</span></span>
<span><span class="co">#&gt; - Accuracy 0.648</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; By class:</span></span>
<span><span class="co">#&gt;               precision    recall  f1-score   support</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     POSITIVE     0.6059    0.8306    0.7007       124</span></span>
<span><span class="co">#&gt;     NEGATIVE     0.7375    0.4683    0.5728       126</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     accuracy                         0.6480       250</span></span>
<span><span class="co">#&gt;    macro avg     0.6717    0.6494    0.6367       250</span></span>
<span><span class="co">#&gt; weighted avg     0.6722    0.6480    0.6362       250</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; 2023-10-20 17:14:05,793 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; $test_score</span></span>
<span><span class="co">#&gt; [1] 0.648</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $dev_score_history</span></span>
<span><span class="co">#&gt;  [1] 0.5066667 0.4933333 0.4977778 0.5066667 0.5200000 0.6800000 0.5600000</span></span>
<span><span class="co">#&gt;  [8] 0.5644444 0.5733333 0.5777778</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $train_loss_history</span></span>
<span><span class="co">#&gt;  [1] 0.1987650 1.0178708 0.8306566 0.8469646 0.8176332 0.7978882 0.8403413</span></span>
<span><span class="co">#&gt;  [8] 0.8117640 0.7986347 0.7818925</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $dev_loss_history</span></span>
<span><span class="co">#&gt;  [1] 4.0928268 0.7175589 0.7263646 0.7379472 0.7230532 0.5859230 0.7124923</span></span>
<span><span class="co">#&gt;  [8] 0.7182886 0.7260346 0.7278090</span></span></code></pre></div>
</div>
</div>
<div class="section level3">
<h3 id="loading-and-using-the-classifiers">Loading and Using the Classifiers<a class="anchor" aria-label="anchor" href="#loading-and-using-the-classifiers"></a>
</h3>
<div style="text-align: justify">
<p>After training the text classification model, the resulting
classifier will already be stored in memory as part of the classifier
variable. It is possible, however, that your Python session exited after
training. If so, you’ll need to load the model into memory with the
following:</p>
<div class="sourceCode" id="cb40"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">TextClassifier</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_models.html">flair_models</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">TextClassifier</span></span>
<span><span class="va">classifier</span> <span class="op">&lt;-</span> <span class="va">TextClassifier</span><span class="op">$</span><span class="fu">load</span><span class="op">(</span><span class="st">'classifier/best-model.pt'</span><span class="op">)</span></span></code></pre></div>
<p>We import the Sentence object. Now, we can generate predictions on
some example text inputs.</p>
<div class="sourceCode" id="cb41"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span></code></pre></div>
<div class="sourceCode" id="cb42"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">"great"</span><span class="op">)</span></span>
<span><span class="va">classifier</span><span class="op">$</span><span class="fu">predict</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="va">labels</span><span class="op">)</span></span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; 'Sentence[1]: "great"'/'POSITIVE' (0.9996)</span></span></code></pre></div>
<div class="sourceCode" id="cb43"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">"sad"</span><span class="op">)</span></span>
<span><span class="va">classifier</span><span class="op">$</span><span class="fu">predict</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="va">labels</span><span class="op">)</span></span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; 'Sentence[1]: "sad"'/'NEGATIVE' (0.5628)</span></span></code></pre></div>
</div>
<p> </p>
<hr>
</div>
</div>
<div class="section level2">
<h2 id="training-a-rnn-with-flair">Training a RNN with FlaiR<a class="anchor" aria-label="anchor" href="#training-a-rnn-with-flair"></a>
</h2>
<div style="text-align: justify">
<p>Here, we train a sentiment analysis model to categorize text. In this
case, we also include a pipeline that implements the use of Recurrent
Neural Networks (RNN). This makes them particularly effective for tasks
involving sequential data. This section also show you how to implent one
of most powerful feature in featrue, stacked Embeddings. You can stack
multiple embeddings with different layers and let the classifier learn
from different types of features. In Flair NLP, and with the
{<code>flaiR</code>} package, it’s very easy to accomplish this
task.</p>
</div>
<div class="section level3">
<h3 id="import-necessary-modules-from-flair-via-flair-in-r">Import Necessary Modules from Flair via {flaiR} in R<a class="anchor" aria-label="anchor" href="#import-necessary-modules-from-flair-via-flair-in-r"></a>
</h3>
<div class="sourceCode" id="cb44"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va">flaiR</span><span class="op">)</span></span>
<span><span class="va">WordEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">WordEmbeddings</span></span>
<span><span class="va">FlairEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">FlairEmbeddings</span></span>
<span><span class="va">DocumentRNNEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">DocumentRNNEmbeddings</span></span>
<span><span class="va">TextClassifier</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_models.html">flair_models</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">TextClassifier</span></span>
<span><span class="va">ModelTrainer</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_trainers.html">flair_trainers</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">ModelTrainer</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="get-the-imdb-corpus">Get the IMDB Corpus<a class="anchor" aria-label="anchor" href="#get-the-imdb-corpus"></a>
</h3>
<div style="text-align: justify">
<p>The IMDB movie review dataset is used here, which is a commonly
utilized dataset for sentiment analysis. <code>$downsample(0.1)</code>
method means only 10% of the dataset is used, allowing for a faster
demonstration</p>
<div class="sourceCode" id="cb45"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># load the IMDB file and downsize it to 0.1</span></span>
<span><span class="va">IMDB</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_datasets.html">flair_datasets</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">IMDB</span></span>
<span><span class="va">corpus</span> <span class="op">&lt;-</span> <span class="fu">IMDB</span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="fu">downsample</span><span class="op">(</span><span class="fl">0.1</span><span class="op">)</span> </span>
<span><span class="co">#&gt; 2023-10-20 17:14:06,275 Reading data from /Users/yenchiehliao/.flair/datasets/imdb_v4-rebalanced</span></span>
<span><span class="co">#&gt; 2023-10-20 17:14:06,275 Train: /Users/yenchiehliao/.flair/datasets/imdb_v4-rebalanced/train.txt</span></span>
<span><span class="co">#&gt; 2023-10-20 17:14:06,275 Dev: None</span></span>
<span><span class="co">#&gt; 2023-10-20 17:14:06,275 Test: None</span></span>
<span><span class="co">#&gt; 2023-10-20 17:14:06,866 Initialized corpus /Users/yenchiehliao/.flair/datasets/imdb_v4-rebalanced (label type name is 'sentiment')</span></span>
<span><span class="co"># create the label dictionary</span></span>
<span><span class="va">lbl_type</span> <span class="op">&lt;-</span> <span class="st">'sentiment'</span></span>
<span><span class="va">label_dict</span> <span class="op">&lt;-</span> <span class="va">corpus</span><span class="op">$</span><span class="fu">make_label_dictionary</span><span class="op">(</span>label_type<span class="op">=</span><span class="va">lbl_type</span><span class="op">)</span></span>
<span><span class="co">#&gt; 2023-10-20 17:14:06,884 Computing label dictionary. Progress:</span></span>
<span><span class="co">#&gt; 2023-10-20 17:14:14,242 Dictionary created for label 'sentiment' with 3 values: POSITIVE (seen 2087 times), NEGATIVE (seen 1963 times)</span></span></code></pre></div>
</div>
</div>
<div class="section level3">
<h3 id="stacked-embeddings-in-flair">Stacked Embeddings in flaiR<a class="anchor" aria-label="anchor" href="#stacked-embeddings-in-flair"></a>
</h3>
<div style="text-align: justify">
<p>This is one of Flair’s most powerful features: it allows for the
integration of embeddings to enable the model to learn from more sparse
features. Three types of embeddings are utilized here: GloVe embeddings,
and two types of Flair embeddings (forward and backward). Word
embeddings are used to convert words into vectors.</p>
<div class="sourceCode" id="cb46"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># make a list of word embeddings</span></span>
<span><span class="va">word_embeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="fu">WordEmbeddings</span><span class="op">(</span><span class="st">'glove'</span><span class="op">)</span>,</span>
<span>                        <span class="fu">FlairEmbeddings</span><span class="op">(</span><span class="st">'news-forward-fast'</span><span class="op">)</span>,</span>
<span>                        <span class="fu">FlairEmbeddings</span><span class="op">(</span><span class="st">'news-backward-fast'</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># initialize the document embeddings</span></span>
<span><span class="va">document_embeddings</span> <span class="op">&lt;-</span> <span class="fu">DocumentRNNEmbeddings</span><span class="op">(</span><span class="va">word_embeddings</span>, </span>
<span>                                             hidden_size <span class="op">=</span> <span class="fl">512L</span>,</span>
<span>                                             reproject_words <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                                             reproject_words_dimension <span class="op">=</span> <span class="fl">256L</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb47"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># create a Text Classifier with the embeddings and label dictionary</span></span>
<span><span class="va">classifier</span> <span class="op">&lt;-</span> <span class="fu">TextClassifier</span><span class="op">(</span><span class="va">document_embeddings</span>, </span>
<span>                            label_dictionary<span class="op">=</span><span class="va">label_dict</span>, label_type<span class="op">=</span><span class="st">'class'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># initialize the text classifier trainer with our corpus</span></span>
<span><span class="va">trainer</span> <span class="op">&lt;-</span> <span class="fu">ModelTrainer</span><span class="op">(</span><span class="va">classifier</span>, <span class="va">corpus</span><span class="op">)</span></span></code></pre></div>
</div>
</div>
<div class="section level3">
<h3 id="start-the-training">Start the Training<a class="anchor" aria-label="anchor" href="#start-the-training"></a>
</h3>
<div style="text-align: justify">
<p>For the sake of this example, setting max_epochs to 5. You might want
to increase this for better performance.</p>
<p>It is worth noting that thelearning rate is a parameter that
determines the step size at each iteration while moving towards a
minimum of the loss function. A smaller learning rate could slow down
the learning process, but it could lead to more precise convergence.
<code>mini_batch_size</code> determines the number of samples that will
be used to compute the gradient at each step. The ‘L’ in 32L is used in
R to denote that the number is an integer.</p>
<p><code>patience</code> (aka early stop) is a hyperparameter used in
conjunction with early stopping to avoid overfitting. It determines the
number of epochs the training process will tolerate without improvements
before stopping the training. Setting max_epochs to 5 means the
algorithm will make five passes through the dataset.</p>
<div class="sourceCode" id="cb48"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># note: the 'L' in 32L is used in R to denote that the number is an integer.</span></span>
<span><span class="va">trainer</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="st">'models/sentiment'</span>,</span>
<span>              learning_rate<span class="op">=</span><span class="fl">0.1</span>,</span>
<span>              mini_batch_size<span class="op">=</span><span class="fl">32L</span>,</span>
<span>              patience<span class="op">=</span><span class="fl">5L</span>,</span>
<span>              max_epochs<span class="op">=</span><span class="fl">5L</span><span class="op">)</span>  </span></code></pre></div>
</div>
</div>
<div class="section level3">
<h3 id="to-apply-the-trained-model-for-prediction">To Apply the Trained Model for Prediction<a class="anchor" aria-label="anchor" href="#to-apply-the-trained-model-for-prediction"></a>
</h3>
<div style="text-align: justify">
<div class="sourceCode" id="cb49"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="st">"This movie was really exciting!"</span></span>
<span><span class="va">classifier</span><span class="op">$</span><span class="fu">predict</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence.labels</span><span class="op">)</span></span></code></pre></div>
</div>
</div>
</div>

  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



   </div>
  <footer><div class="container">
  <div class="pkgdown-footer-left">
  <p></p>
<p>Developed by <a href="https://davidycliao.github.io" class="external-link">David Liao</a>, Akbik Alan, Blythe Duncan, Vollgraf Roland.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

  </div></footer>
</body>
</html>
