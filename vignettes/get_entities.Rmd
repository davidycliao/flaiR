---
title: "Tagging Named Entities with Flair Standard Models"
author: 
  - name: "David (Yen-Chieh) Liao"
    affiliation: "Postdoc at Text & Policy Research Group and SPIRe in UCD"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Tagging Named Entities with Flair Standard Models}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r, include = FALSE}
system(paste(Sys.which("python3"), "-m pip install --upgrade pip"))
system(paste(Sys.which("python3"), "-m pip install torch"))
system(paste(Sys.which("python3"), "-m pip install flair"))
library(reticulate)
# # system(paste(reticulate::py_config()$python, "-m pip install flair"))
# reticulate::py_install("flair")
library(flaiR)

```

## Generic Approach Using Pre-trained NER English Model

```{r}
library(flaiR)
data("uk_immigration")
uk_immigration <- head(uk_immigration, 10)
```

<div style="text-align: justify">

Use `load_tagger_ner` to call the NER pretrained model. The model will be downloaded from Flair's Hugging Face repo. Thus, ensure you have an internet connection. Once downloaded, the model will be stored in __.flair__ as the cache in your device. So, once you've downloaded it and it hasn't been manually removed, executing the command again will not trigger a download.


```{r}
tagger_ner <- load_tagger_ner("ner")
```

Flair NLP operates under the [PyTorch](https://pytorch.org) framework. As such, we can use the `$to` method to set the device for the Flair Python library. The flair_device("cpu")  allows you to select whether to use the CPU, CUDA devices (like cuda:0, cuda:1, cuda:2), or specific MPS devices on Mac (such as mps:0, mps:1, mps:2). For information on Accelerated PyTorch training on Mac, please refer to https://developer.apple.com/metal/pytorch/. For more about CUDA, please visit: https://developer.nvidia.com/cuda-zone 

```{r eval=FALSE, include=TRUE}
tagger_pos$to(flair_device("mps")) 
```

```
SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): FlairEmbeddings(
      (lm): LanguageModel(
        (drop): Dropout(p=0.05, inplace=False)
        (encoder): Embedding(300, 100)
        (rnn): LSTM(100, 2048)
        (decoder): Linear(in_features=2048, out_features=300, bias=True)
      )
    )
    (list_embedding_1): FlairEmbeddings(
      (lm): LanguageModel(
        (drop): Dropout(p=0.05, inplace=False)
        (encoder): Embedding(300, 100)
        (rnn): LSTM(100, 2048)
        (decoder): Linear(in_features=2048, out_features=300, bias=True)
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=4096, out_features=4096, bias=True)
  (rnn): LSTM(4096, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=53, bias=True)
  (loss_function): ViterbiLoss()
  (crf): CRF()
)
```

</div>


<div style="text-align: justify">

If you want the computation to run faster, it is recommended to keep the show.text_id set to FALSE by default. 
</div>
```{r}
results <- get_entities(uk_immigration$text,
                        uk_immigration$speaker, 
                        tagger_ner,
                        show.text_id = FALSE
                        )
print(results)
```

```{r}
print(results)
```

## Batch Processing

<div style="text-align: justify">
Processing texts individually can be both inefficient and memory-intensive. On the other hand, processing all the texts simultaneously could surpass memory constraints, especially if each document in the dataset is sizable. Parsing the documents in smaller batches may provide an optimal compromise between these two scenarios. Batch processing can enhance efficiency and aid in memory management.

By default, the batch_size parameter is set to 5. You can consider starting with this default value and then experimenting with different batch sizes to find the one that works best for your specific use case. You can monitor memory usage and processing time to help you make a decision. If you have access to a GPU, you might also try larger batch sizes to take advantage of GPU parallelism. However, be cautious not to set the batch size too large, as it can lead to out-of-memory errors. Ultimately, the choice of batch size should be based on a balance between memory constraints, processing efficiency, and the specific requirements of your entity extraction task.

</div>

```{r}
batch_process_time <- system.time({
    batch_process_results  <- get_entities_batch(uk_immigration$text,
                                                 uk_immigration$speaker, 
                                                 tagger_ner, 
                                                 show.text_id = FALSE,
                                                 batch_size = 5)
    gc()
})
print(batch_process_time)
```

```{r}
print(batch_process_results)
```
