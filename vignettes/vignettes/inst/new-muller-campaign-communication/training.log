2025-10-29 03:29:19,482 ----------------------------------------------------------------------------------------------------
2025-10-29 03:29:19,482 Model: "TextClassifier(
  (embeddings): TransformerDocumentEmbeddings(
    (model): DistilBertModel(
      (embeddings): Embeddings(
        (word_embeddings): Embedding(30523, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (transformer): Transformer(
        (layer): ModuleList(
          (0-5): 6 x TransformerBlock(
            (attention): DistilBertSdpaAttention(
              (dropout): Dropout(p=0.1, inplace=False)
              (q_lin): Linear(in_features=768, out_features=768, bias=True)
              (k_lin): Linear(in_features=768, out_features=768, bias=True)
              (v_lin): Linear(in_features=768, out_features=768, bias=True)
              (out_lin): Linear(in_features=768, out_features=768, bias=True)
            )
            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (ffn): FFN(
              (dropout): Dropout(p=0.1, inplace=False)
              (lin1): Linear(in_features=768, out_features=3072, bias=True)
              (lin2): Linear(in_features=3072, out_features=768, bias=True)
              (activation): GELUActivation()
            )
            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          )
        )
      )
    )
  )
  (decoder): Linear(in_features=768, out_features=3, bias=True)
  (dropout): Dropout(p=0.0, inplace=False)
  (locked_dropout): LockedDropout(p=0.0)
  (word_dropout): WordDropout(p=0.0)
  (loss_function): CrossEntropyLoss()
  (weights): None
  (weight_tensor) None
)"
2025-10-29 03:29:19,483 ----------------------------------------------------------------------------------------------------
2025-10-29 03:29:19,483 Corpus: 1800 train + 200 dev + 85 test sentences
2025-10-29 03:29:19,483 ----------------------------------------------------------------------------------------------------
2025-10-29 03:29:19,483 Train:  1800 sentences
2025-10-29 03:29:19,483         (train_with_dev=False, train_with_test=False)
2025-10-29 03:29:19,483 ----------------------------------------------------------------------------------------------------
2025-10-29 03:29:19,483 Training Params:
2025-10-29 03:29:19,483  - learning_rate: "0.002" 
2025-10-29 03:29:19,483  - mini_batch_size: "8"
2025-10-29 03:29:19,483  - max_epochs: "1"
2025-10-29 03:29:19,483  - shuffle: "True"
2025-10-29 03:29:19,483 ----------------------------------------------------------------------------------------------------
2025-10-29 03:29:19,483 Plugins:
2025-10-29 03:29:19,483  - AnnealOnPlateau | patience: '3', anneal_factor: '0.5', min_learning_rate: '0.0001'
2025-10-29 03:29:19,483 ----------------------------------------------------------------------------------------------------
2025-10-29 03:29:19,483 Final evaluation on model from best epoch (best-model.pt)
2025-10-29 03:29:19,483  - metric: "('micro avg', 'f1-score')"
2025-10-29 03:29:19,483 ----------------------------------------------------------------------------------------------------
2025-10-29 03:29:19,483 Computation:
2025-10-29 03:29:19,483  - compute on device: cpu
2025-10-29 03:29:19,484  - embedding storage: cpu
2025-10-29 03:29:19,484 ----------------------------------------------------------------------------------------------------
2025-10-29 03:29:19,484 Model training base path: "vignettes/inst/new-muller-campaign-communication"
2025-10-29 03:29:19,484 ----------------------------------------------------------------------------------------------------
2025-10-29 03:29:19,484 ----------------------------------------------------------------------------------------------------
2025-10-29 03:29:26,466 epoch 1 - iter 22/225 - loss 0.47686866 - time (sec): 6.98 - samples/sec: 25.21 - lr: 0.002000 - momentum: 0.000000
2025-10-29 03:29:34,375 epoch 1 - iter 44/225 - loss 0.47116069 - time (sec): 14.89 - samples/sec: 23.64 - lr: 0.002000 - momentum: 0.000000
2025-10-29 03:29:41,627 epoch 1 - iter 66/225 - loss 0.45405745 - time (sec): 22.14 - samples/sec: 23.85 - lr: 0.002000 - momentum: 0.000000
2025-10-29 03:29:49,722 epoch 1 - iter 88/225 - loss 0.42943519 - time (sec): 30.24 - samples/sec: 23.28 - lr: 0.002000 - momentum: 0.000000
2025-10-29 03:29:56,543 epoch 1 - iter 110/225 - loss 0.41669389 - time (sec): 37.06 - samples/sec: 23.75 - lr: 0.002000 - momentum: 0.000000
2025-10-29 03:30:03,828 epoch 1 - iter 132/225 - loss 0.41539676 - time (sec): 44.34 - samples/sec: 23.81 - lr: 0.002000 - momentum: 0.000000
2025-10-29 03:30:12,381 epoch 1 - iter 154/225 - loss 0.41510440 - time (sec): 52.90 - samples/sec: 23.29 - lr: 0.002000 - momentum: 0.000000
2025-10-29 03:30:19,744 epoch 1 - iter 176/225 - loss 0.41127944 - time (sec): 60.26 - samples/sec: 23.37 - lr: 0.002000 - momentum: 0.000000
2025-10-29 03:30:26,053 epoch 1 - iter 198/225 - loss 0.40708513 - time (sec): 66.57 - samples/sec: 23.79 - lr: 0.002000 - momentum: 0.000000
2025-10-29 03:30:33,543 epoch 1 - iter 220/225 - loss 0.39587166 - time (sec): 74.06 - samples/sec: 23.76 - lr: 0.002000 - momentum: 0.000000
2025-10-29 03:30:35,324 ----------------------------------------------------------------------------------------------------
2025-10-29 03:30:35,324 EPOCH 1 done: loss 0.3941 - lr: 0.002000
2025-10-29 03:30:38,754 DEV : loss 0.4037041962146759 - f1-score (micro avg)  0.85
2025-10-29 03:30:38,759  - 0 epochs without improvement
2025-10-29 03:30:38,761 saving best model
2025-10-29 03:30:39,728 ----------------------------------------------------------------------------------------------------
2025-10-29 03:30:39,728 Loading model from best epoch ...
2025-10-29 03:30:42,357 
Results:
- F-score (micro) 0.8706
- F-score (macro) 0.8779
- Accuracy 0.8706

By class:
              precision    recall  f1-score   support

      Future     0.8837    0.8837    0.8837        43
     Present     0.7931    0.8519    0.8214        27
        Past     1.0000    0.8667    0.9286        15

    accuracy                         0.8706        85
   macro avg     0.8923    0.8674    0.8779        85
weighted avg     0.8755    0.8706    0.8718        85

2025-10-29 03:30:42,358 ----------------------------------------------------------------------------------------------------
