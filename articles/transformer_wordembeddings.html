<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>WordEmbeddings Supported in Flair NLP • flaiR</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/Source_Sans_Pro-0.4.9/font.css" rel="stylesheet">
<link href="../deps/Source_Code_Pro-0.4.9/font.css" rel="stylesheet">
<link href="../deps/font-awesome-6.4.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.4.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="WordEmbeddings Supported in Flair NLP">
<!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=G-3ZG40PPG98"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-3ZG40PPG98');
</script><script defer data-domain="{YOUR DOMAIN},all.tidyverse.org" src="https://plausible.io/js/plausible.js"></script>
</head>
<body>
    <a href="#container" class="visually-hidden-focusable">Skip to content</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-none" data-bs-theme="inverse" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">flaiR</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Released version">0.0.6</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../index.html"><span class="fa fa-home"></span></a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-quick-start" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true"><span class="fa fa-rocket"></span> Quick Start</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-quick-start">
<li><a class="dropdown-item" href="../articles/quickstart.html#nlp-tasks">NLP Tasks</a></li>
    <li><a class="dropdown-item" href="../articles/quickstart.html#class-and-ojbect">Class and Ojbect</a></li>
    <li><a class="dropdown-item" href="../articles/quickstart.html#more-details-about-installation">More Details about Installation</a></li>
  </ul>
</li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-tutorial" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true"><span class="fa fa-project-diagram"></span> Tutorial</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-tutorial">
<li><a class="dropdown-item" href="../articles/tutorial.html#introduction">Introduction</a></li>
    <li><a class="dropdown-item" href="../articles/tutorial.html#sentence-and-token">Sentence and Token</a></li>
    <li><a class="dropdown-item" href="../articles/tutorial.html#sequence-taggings">Sequence Taggings</a></li>
    <li><a class="dropdown-item" href="../articles/tutorial.html#performing-ner-tasks">Performing NER Tasks</a></li>
    <li><a class="dropdown-item" href="../articles/tutorial.html#flair-embedding">Flair Embeddings</a></li>
    <li><a class="dropdown-item" href="../articles/tutorial.html#training-a-binary-classifier">Training a Binary Classifier</a></li>
    <li><a class="dropdown-item" href="../articles/tutorial.html#training-rnns">Training RNNs</a></li>
    <li><a class="dropdown-item" href="../articles/tutorial.html#finetune-transformers">Finetune Transformers </a></li>
  </ul>
</li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-expanded-feats" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true"><span class="fa fa-newspaper-o"></span> Expanded Feats</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-expanded-feats">
<li><a class="dropdown-item" href="../articles/get_pos.html">Part-of-speech Tagging</a></li>
    <li><a class="dropdown-item" href="../articles/get_entities.html">Named Entity Recognition</a></li>
    <li><a class="dropdown-item" href="../articles/get_sentiments.html">Tagging Sentiment</a></li>
    <li><a class="dropdown-item" href="../articles/highlight_text.html">The Coloring Entities</a></li>
  </ul>
</li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-reference" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true"><span class="fa fa-file-code-o"></span> Reference</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-reference">
<li><a class="dropdown-item" href="../reference/index.html">All Function Reference</a></li>
  </ul>
</li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-news" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true"><span class="fa fa-newspaper-o"></span> News</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-news">
<li><a class="dropdown-item" href="../news/index.html#flair-006-2023-10-29">0.0.6</a></li>
    <li><a class="dropdown-item" href="../news/index.html#flair-005-2023-10-01">0.0.5</a></li>
    <li><a class="dropdown-item" href="../news/index.html#flair-003-2023-09-10">0.0.3</a></li>
    <li><a class="dropdown-item" href="../news/index.html#flair-001-development-version">0.0.1</a></li>
  </ul>
</li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/davidycliao/flaiR"><span class="fa fa-github fa-lg"></span> GitHub</a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article" id="container">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>WordEmbeddings Supported in Flair NLP</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/davidycliao/flaiR/blob/HEAD/vignettes/transformer_wordembeddings.Rmd" class="external-link"><code>vignettes/transformer_wordembeddings.Rmd</code></a></small>
      <div class="d-none name"><code>transformer_wordembeddings.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="the-overview-of-embedding-in-flair-nlp">The Overview of Embedding in Flair NLP<a class="anchor" aria-label="anchor" href="#the-overview-of-embedding-in-flair-nlp"></a>
</h2>
<div style="text-align: justify">
<p>All word embedding classes inherit from the
<code>TokenEmbeddings</code> class and call the <code><a href="https://rdrr.io/r/stats/embed.html" class="external-link">embed()</a></code>
method to embed the text. In most cases when using Flair, various and
complex embedding processes are hidden behind the interface. Users
simply need to instantiate the necessary embedding class and call
<code><a href="https://rdrr.io/r/stats/embed.html" class="external-link">embed()</a></code> to embed text.</p>
<p>Here are the types of embeddings currently supported in FlairNLP:</p>
<!-- | Class | Type | Paper | -->
<!-- | ------------- | -------------  | -------------  | -->
<!-- | [`BytePairEmbeddings`](https://github.com/flairNLP/flair/tree/master/resources/docs/embeddings/BYTE_PAIR_EMBEDDINGS.md) | Subword-level word embeddings | [Heinzerling and Strube (2018)](https://www.aclweb.org/anthology/L18-1473)  | -->
<!-- | [`CharacterEmbeddings`](https://github.com/flairNLP/flair/tree/master/resources/docs/embeddings/CHARACTER_EMBEDDINGS.md) | Task-trained character-level embeddings of words | [Lample et al. (2016)](https://www.aclweb.org/anthology/N16-1030) | -->
<!-- | [`ELMoEmbeddings`](https://github.com/flairNLP/flair/tree/master/resources/docs/embeddings/ELMO_EMBEDDINGS.md) | Contextualized word-level embeddings | [Peters et al. (2018)](https://aclweb.org/anthology/N18-1202)  | -->
<!-- | [`FastTextEmbeddings`](https://github.com/flairNLP/flair/tree/master/resources/docs/embeddings/FASTTEXT_EMBEDDINGS.md) | Word embeddings with subword features | [Bojanowski et al. (2017)](https://aclweb.org/anthology/Q17-1010)  | -->
<!-- | [`FlairEmbeddings`](https://github.com/flairNLP/flair/tree/master/resources/docs/embeddings/FLAIR_EMBEDDINGS.md) | Contextualized character-level embeddings | [Akbik et al. (2018)](https://www.aclweb.org/anthology/C18-1139/)  | -->
<!-- | [`OneHotEmbeddings`](https://github.com/flairNLP/flair/tree/master/resources/docs/embeddings/ONE_HOT_EMBEDDINGS.md) | Standard one-hot embeddings of text or tags | - | -->
<!-- | [`PooledFlairEmbeddings`](https://github.com/flairNLP/flair/tree/master/resources/docs/embeddings/FLAIR_EMBEDDINGS.md) | Pooled variant of `FlairEmbeddings` |  [Akbik et al. (2019)](https://www.aclweb.org/anthology/N19-1078/)  | -->
<!-- | [`TransformerWordEmbeddings`](https://github.com/flairNLP/flair/tree/master/resources/docs/embeddings/TRANSFORMER_EMBEDDINGS.md) | Embeddings from pretrained [transformers](https://huggingface.co/transformers/pretrained_models.html) (BERT, XLM, GPT, RoBERTa, XLNet, DistilBERT etc.) | [Devlin et al. (2018)](https://www.aclweb.org/anthology/N19-1423/) [Radford et al. (2018)](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)  [Liu et al. (2019)](https://arxiv.org/abs/1907.11692) [Dai et al. (2019)](https://arxiv.org/abs/1901.02860) [Yang et al. (2019)](https://arxiv.org/abs/1906.08237) [Lample and Conneau (2019)](https://arxiv.org/abs/1901.07291) | -->
<!-- | [`WordEmbeddings`](https://github.com/flairNLP/flair/tree/master/resources/docs/embeddings/CLASSIC_WORD_EMBEDDINGS.md) | Classic word embeddings |  | -->
<table class="table">
<colgroup>
<col width="33%">
<col width="33%">
<col width="33%">
</colgroup>
<thead><tr class="header">
<th>Class</th>
<th>Type</th>
<th>Paper</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><a href="articles/transformer_wordembeddings.html#bytepairembeddings"><code>BytePairEmbeddings</code></a></td>
<td>Subword-level word embeddings</td>
<td><a href="https://www.aclweb.org/anthology/L18-1473" class="external-link">Heinzerling and
Strube (2018)</a></td>
</tr>
<tr class="even">
<td><a href=""><code>CharacterEmbeddings</code></a></td>
<td>Task-trained character-level embeddings of words</td>
<td><a href="https://www.aclweb.org/anthology/N16-1030" class="external-link">Lample et
al. (2016)</a></td>
</tr>
<tr class="odd">
<td><a href=""><code>ELMoEmbeddings</code></a></td>
<td>Contextualized word-level embeddings</td>
<td><a href="https://aclweb.org/anthology/N18-1202" class="external-link">Peters et
al. (2018)</a></td>
</tr>
<tr class="even">
<td><a href=""><code>FastTextEmbeddings</code></a></td>
<td>Word embeddings with subword features</td>
<td><a href="https://aclweb.org/anthology/Q17-1010" class="external-link">Bojanowski et
al. (2017)</a></td>
</tr>
<tr class="odd">
<td><a href="articles/transformer_wordembeddings.html#flair-embeddings"><code>FlairEmbeddings</code></a></td>
<td>Contextualized character-level embeddings</td>
<td><a href="https://www.aclweb.org/anthology/C18-1139/" class="external-link">Akbik et
al. (2018)</a></td>
</tr>
<tr class="even">
<td><a href=""><code>OneHotEmbeddings</code></a></td>
<td>Standard one-hot embeddings of text or tags</td>
<td>-</td>
</tr>
<tr class="odd">
<td><a href="articles/transformer_wordembeddings.html#pooled-flair-embeddings"><code>PooledFlairEmbeddings</code></a></td>
<td>Pooled variant of <code>FlairEmbeddings</code>
</td>
<td><a href="https://www.aclweb.org/anthology/N19-1078/" class="external-link">Akbik et
al. (2019)</a></td>
</tr>
<tr class="even">
<td>
<a href="articles/transformer_wordembeddings.html#transformer-embeddings"><code>TransformerWordEmbeddings</code></a>)</td>
<td>Embeddings from pretrained <a href="https://huggingface.co/transformers/pretrained_models.html" class="external-link">transformers</a>
(BERT, XLM, GPT, RoBERTa, XLNet, DistilBERT etc.)</td>
<td>
<a href="https://www.aclweb.org/anthology/N19-1423/" class="external-link">Devlin et
al. (2018)</a> <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" class="external-link">Radford
et al. (2018)</a> <a href="https://arxiv.org/abs/1907.11692" class="external-link">Liu et
al. (2019)</a> <a href="https://arxiv.org/abs/1901.02860" class="external-link">Dai et
al. (2019)</a> <a href="https://arxiv.org/abs/1906.08237" class="external-link">Yang et
al. (2019)</a> <a href="https://arxiv.org/abs/1901.07291" class="external-link">Lample and
Conneau (2019)</a>
</td>
</tr>
<tr class="odd">
<td><a href="https://github.com/flairNLP/flair/tree/master/resources/docs/embeddings/CLASSIC_WORD_EMBEDDINGS.md" class="external-link"><code>WordEmbeddings</code></a></td>
<td>Classic word embeddings</td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p> </p>
<hr>
</div>
<div class="section level2">
<h2 id="byte-pair-embeddings">Byte Pair Embeddings<a class="anchor" aria-label="anchor" href="#byte-pair-embeddings"></a>
</h2>
<div style="text-align: justify">
<p><em>Please note that ihis document for R is a conversion of the <a href="https://github.com/flairNLP/flair/blob/master/resources/docs/embeddings/BYTE_PAIR_EMBEDDINGS.md" class="external-link">Flair
NLP</a> document implemented in Python.</em></p>
<p><code>BytePairEmbeddings</code> are word embeddings that are
precomputed on the subword-level. This means that they are able to embed
any word by splitting words into subwords and looking up their
embeddings. <code>BytePairEmbeddings</code> were proposed and computed
by <a href="https://www.aclweb.org/anthology/L18-1473" class="external-link">Heinzerling and
Strube (2018)</a> who found that they offer nearly the same accuracy as
word embeddings, but at a fraction of the model size. So they are a
great choice if you want to train small models.</p>
<p>You initialize with a language code (275 languages supported), a
number of ‘syllables’ (one of ) and a number of dimensions (one of 50,
100, 200 or 300). The following initializes and uses byte pair
embeddings for English:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va">flaiR</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## <span style="color: #0000BB; font-weight: bold;">flaiR</span>: <span style="color: #BBBB00; font-weight: bold;">An R Wrapper for Accessing Flair NLP</span> <span style="color: #BBBB00; font-weight: bold;">0.14.0</span></span></span></code></pre>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># initialize embedding</span></span>
<span><span class="va">BytePairEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">BytePairEmbeddings</span></span>
<span><span class="va">embedding</span> <span class="op">&lt;-</span> <span class="fu">BytePairEmbeddings</span><span class="op">(</span><span class="st">'en'</span><span class="op">)</span></span>
<span><span class="co"># create a sentence</span></span>
<span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span>
<span><span class="va">sentence</span> <span class="op">=</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">'The grass is green .'</span><span class="op">)</span></span>
<span><span class="co"># embed words in sentence</span></span>
<span><span class="va">embedding</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [[1]]</span></span>
<span><span class="co">## Sentence[5]: "The grass is green ."</span></span></code></pre>
<p>More information can be found on the <a href="https://nlp.h-its.org/bpemb/" class="external-link">byte pair embeddings</a> web page.
<code>BytePairEmbeddings</code> also have a multilingual model capable
of embedding any word in any language. You can instantiate it with:</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">embedding</span> <span class="op">&lt;-</span> <span class="fu">BytePairEmbeddings</span><span class="op">(</span><span class="st">'multi'</span><span class="op">)</span></span></code></pre></div>
<p>You can also load custom <code>BytePairEmbeddings</code> by
specifying a path to model_file_path and embedding_file_path arguments.
They correspond respectively to a SentencePiece model file and to an
embedding file (Word2Vec plain text or GenSim binary). For example:</p>
</div>
</div>
<div class="section level2">
<h2 id="flair-embeddings">Flair Embeddings<a class="anchor" aria-label="anchor" href="#flair-embeddings"></a>
</h2>
<div style="text-align: justify">
<p><strong>The following example manual is translated into R from Flair
NLP by <a href="https://engineering.zalando.com/posts/2018/11/zalando-research-releases-flair.html" class="external-link">Zalando
Research</a>.</strong> In Flair, the use of embedding is very quite
straightforward. Here’s an example code snippet of how to use Flair’s
contextual string embeddings:</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va">flaiR</span><span class="op">)</span></span>
<span><span class="va">FlairEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">FlairEmbeddings</span></span>
<span><span class="co"># init embedding</span></span>
<span><span class="va">flair_embedding_forward</span> <span class="op">&lt;-</span> <span class="fu">FlairEmbeddings</span><span class="op">(</span><span class="st">'news-forward'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># create a sentence</span></span>
<span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span>
<span><span class="va">sentence</span> <span class="op">=</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">'The grass is green .'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># embed words in sentence</span></span>
<span><span class="va">flair_embedding_forward</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [[1]]</span></span>
<span><span class="co">## Sentence[5]: "The grass is green ."</span></span></code></pre>
<table class="table">
<colgroup>
<col width="33%">
<col width="33%">
<col width="33%">
</colgroup>
<thead><tr class="header">
<th>ID</th>
<th>Language</th>
<th>Embedding</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>‘multi-X’</td>
<td>300+</td>
<td>
<a href="http://opus.nlpl.eu/JW300.php" class="external-link">JW300 corpus</a>, as
proposed by <a href="https://www.aclweb.org/anthology/P19-1310/" class="external-link">Agić
and Vulić (2019)</a>. The corpus is licensed under CC-BY-NC-SA</td>
</tr>
<tr class="even">
<td>‘multi-X-fast’</td>
<td>English, German, French, Italian, Dutch, Polish</td>
<td>Mix of corpora (Web, Wikipedia, Subtitles, News), CPU-friendly</td>
</tr>
<tr class="odd">
<td>‘news-X’</td>
<td>English</td>
<td>Trained with 1 billion word corpus</td>
</tr>
<tr class="even">
<td>‘news-X-fast’</td>
<td>English</td>
<td>Trained with 1 billion word corpus, CPU-friendly</td>
</tr>
<tr class="odd">
<td>‘mix-X’</td>
<td>English</td>
<td>Trained with mixed corpus (Web, Wikipedia, Subtitles)</td>
</tr>
<tr class="even">
<td>‘ar-X’</td>
<td>Arabic</td>
<td>Added by <a href="https://github.com/zalandoresearch/flair/issues/614" class="external-link"><span class="citation">@stefan-it</span></a>: Trained with Wikipedia/OPUS</td>
</tr>
<tr class="odd">
<td>‘bg-X’</td>
<td>Bulgarian</td>
<td>Added by <a href="https://github.com/zalandoresearch/flair/issues/614" class="external-link"><span class="citation">@stefan-it</span></a>: Trained with Wikipedia/OPUS</td>
</tr>
<tr class="even">
<td>‘bg-X-fast’</td>
<td>Bulgarian</td>
<td>Added by <a href="https://github.com/stefan-it/flair-lms" class="external-link"><span class="citation">@stefan-it</span></a>: Trained with various sources
(Europarl, Wikipedia or SETimes)</td>
</tr>
<tr class="odd">
<td>‘cs-X’</td>
<td>Czech</td>
<td>Added by <a href="https://github.com/zalandoresearch/flair/issues/614" class="external-link"><span class="citation">@stefan-it</span></a>: Trained with Wikipedia/OPUS</td>
</tr>
<tr class="even">
<td>‘cs-v0-X’</td>
<td>Czech</td>
<td>Added by <a href="https://github.com/stefan-it/flair-lms" class="external-link"><span class="citation">@stefan-it</span></a>: LM embeddings (earlier
version)</td>
</tr>
<tr class="odd">
<td>‘de-X’</td>
<td>German</td>
<td>Trained with mixed corpus (Web, Wikipedia, Subtitles)</td>
</tr>
<tr class="even">
<td>‘de-historic-ha-X’</td>
<td>German (historical)</td>
<td>Added by <a href="https://github.com/stefan-it/flair-lms" class="external-link"><span class="citation">@stefan-it</span></a>: Historical German trained over
<em>Hamburger Anzeiger</em>
</td>
</tr>
<tr class="odd">
<td>‘de-historic-wz-X’</td>
<td>German (historical)</td>
<td>Added by <a href="https://github.com/stefan-it/flair-lms" class="external-link"><span class="citation">@stefan-it</span></a>: Historical German trained over
<em>Wiener Zeitung</em>
</td>
</tr>
<tr class="even">
<td>‘de-historic-rw-X’</td>
<td>German (historical)</td>
<td>Added by <a href="https://github.com/redewiedergabe" class="external-link"><span class="citation">@redewiedergabe</span></a>: Historical German trained
over 100 million tokens</td>
</tr>
<tr class="odd">
<td>‘es-X’</td>
<td>Spanish</td>
<td>Added by <a href="https://github.com/zalandoresearch/flair/issues/80" class="external-link"><span class="citation">@iamyihwa</span></a>: Trained with Wikipedia</td>
</tr>
<tr class="even">
<td>‘es-X-fast’</td>
<td>Spanish</td>
<td>Added by <a href="https://github.com/zalandoresearch/flair/issues/80" class="external-link"><span class="citation">@iamyihwa</span></a>: Trained with Wikipedia,
CPU-friendly</td>
</tr>
<tr class="odd">
<td>‘es-clinical-’</td>
<td>Spanish (clinical)</td>
<td>Added by <a href="https://github.com/flairNLP/flair/issues/2292" class="external-link"><span class="citation">@matirojasg</span></a>: Trained with Wikipedia</td>
</tr>
<tr class="even">
<td>‘eu-X’</td>
<td>Basque</td>
<td>Added by <a href="https://github.com/zalandoresearch/flair/issues/614" class="external-link"><span class="citation">@stefan-it</span></a>: Trained with Wikipedia/OPUS</td>
</tr>
<tr class="odd">
<td>‘eu-v0-X’</td>
<td>Basque</td>
<td>Added by <a href="https://github.com/stefan-it/flair-lms" class="external-link"><span class="citation">@stefan-it</span></a>: LM embeddings (earlier
version)</td>
</tr>
<tr class="even">
<td>‘fa-X’</td>
<td>Persian</td>
<td>Added by <a href="https://github.com/zalandoresearch/flair/issues/614" class="external-link"><span class="citation">@stefan-it</span></a>: Trained with Wikipedia/OPUS</td>
</tr>
<tr class="odd">
<td>‘fi-X’</td>
<td>Finnish</td>
<td>Added by <a href="https://github.com/zalandoresearch/flair/issues/614" class="external-link"><span class="citation">@stefan-it</span></a>: Trained with Wikipedia/OPUS</td>
</tr>
<tr class="even">
<td>‘fr-X’</td>
<td>French</td>
<td>Added by <a href="https://github.com/mhham" class="external-link"><span class="citation">@mhham</span></a>: Trained with French Wikipedia</td>
</tr>
<tr class="odd">
<td>‘he-X’</td>
<td>Hebrew</td>
<td>Added by <a href="https://github.com/zalandoresearch/flair/issues/614" class="external-link"><span class="citation">@stefan-it</span></a>: Trained with Wikipedia/OPUS</td>
</tr>
<tr class="even">
<td>‘hi-X’</td>
<td>Hindi</td>
<td>Added by <a href="https://github.com/zalandoresearch/flair/issues/614" class="external-link"><span class="citation">@stefan-it</span></a>: Trained with Wikipedia/OPUS</td>
</tr>
<tr class="odd">
<td>‘hr-X’</td>
<td>Croatian</td>
<td>Added by <a href="https://github.com/zalandoresearch/flair/issues/614" class="external-link"><span class="citation">@stefan-it</span></a>: Trained with Wikipedia/OPUS</td>
</tr>
<tr class="even">
<td>‘id-X’</td>
<td>Indonesian</td>
<td>Added by <a href="https://github.com/zalandoresearch/flair/issues/614" class="external-link"><span class="citation">@stefan-it</span></a>: Trained with Wikipedia/OPUS</td>
</tr>
<tr class="odd">
<td>‘it-X’</td>
<td>Italian</td>
<td>Added by <a href="https://github.com/zalandoresearch/flair/issues/614" class="external-link"><span class="citation">@stefan-it</span></a>: Trained with Wikipedia/OPUS</td>
</tr>
<tr class="even">
<td>‘ja-X’</td>
<td>Japanese</td>
<td>Added by <a href="https://github.com/zalandoresearch/flair/issues/527" class="external-link"><span class="citation">@frtacoa</span></a>: Trained with 439M words of
Japanese Web crawls (2048 hidden states, 2 layers)</td>
</tr>
<tr class="odd">
<td>‘nl-X’</td>
<td>Dutch</td>
<td>Added by <a href="https://github.com/zalandoresearch/flair/issues/614" class="external-link"><span class="citation">@stefan-it</span></a>: Trained with Wikipedia/OPUS</td>
</tr>
<tr class="even">
<td>‘nl-v0-X’</td>
<td>Dutch</td>
<td>Added by <a href="https://github.com/stefan-it/flair-lms" class="external-link"><span class="citation">@stefan-it</span></a>: LM embeddings (earlier
version)</td>
</tr>
<tr class="odd">
<td>‘no-X’</td>
<td>Norwegian</td>
<td>Added by <a href="https://github.com/zalandoresearch/flair/issues/614" class="external-link"><span class="citation">@stefan-it</span></a>: Trained with Wikipedia/OPUS</td>
</tr>
<tr class="even">
<td>‘pl-X’</td>
<td>Polish</td>
<td>Added by <a href="https://github.com/applicaai/poleval-2018" class="external-link"><span class="citation">@borchmann</span></a>: Trained with web crawls (Polish
part of CommonCrawl)</td>
</tr>
<tr class="odd">
<td>‘pl-opus-X’</td>
<td>Polish</td>
<td>Added by <a href="https://github.com/zalandoresearch/flair/issues/614" class="external-link"><span class="citation">@stefan-it</span></a>: Trained with Wikipedia/OPUS</td>
</tr>
<tr class="even">
<td>‘pt-X’</td>
<td>Portuguese</td>
<td>Added by <a href="https://github.com/ericlief/language_models" class="external-link"><span class="citation">@ericlief</span></a>: LM embeddings</td>
</tr>
<tr class="odd">
<td>‘sl-X’</td>
<td>Slovenian</td>
<td>Added by <a href="https://github.com/zalandoresearch/flair/issues/614" class="external-link"><span class="citation">@stefan-it</span></a>: Trained with Wikipedia/OPUS</td>
</tr>
<tr class="even">
<td>‘sl-v0-X’</td>
<td>Slovenian</td>
<td>Added by <a href="https://github.com/stefan-it/flair-lms" class="external-link"><span class="citation">@stefan-it</span></a>: Trained with various sources
(Europarl, Wikipedia and OpenSubtitles2018)</td>
</tr>
<tr class="odd">
<td>‘sv-X’</td>
<td>Swedish</td>
<td>Added by <a href="https://github.com/zalandoresearch/flair/issues/614" class="external-link"><span class="citation">@stefan-it</span></a>: Trained with Wikipedia/OPUS</td>
</tr>
<tr class="even">
<td>‘sv-v0-X’</td>
<td>Swedish</td>
<td>Added by <a href="https://github.com/stefan-it/flair-lms" class="external-link"><span class="citation">@stefan-it</span></a>: Trained with various sources
(Europarl, Wikipedia or OpenSubtitles2018)</td>
</tr>
<tr class="odd">
<td>‘ta-X’</td>
<td>Tamil</td>
<td>Added by <a href="https://github.com/stefan-it/plur" class="external-link"><span class="citation">@stefan-it</span></a>
</td>
</tr>
<tr class="even">
<td>‘pubmed-X’</td>
<td>English</td>
<td>Added by <a href="https://github.com/zalandoresearch/flair/pull/519" class="external-link"><span class="citation">@jessepeng</span></a>: Trained with 5% of PubMed
abstracts until 2015 (1150 hidden states, 3 layers)</td>
</tr>
<tr class="odd">
<td>‘de-impresso-hipe-v1-X’</td>
<td>German (historical)</td>
<td>In-domain data (Swiss and Luxembourgish newspapers) for <a href="https://impresso.github.io/CLEF-HIPE-2020" class="external-link">CLEF HIPE Shared
task</a>. More information on the shared task can be found in <a href="https://zenodo.org/record/3752679#.XqgzxXUzZzU" class="external-link">this
paper</a>
</td>
</tr>
<tr class="even">
<td>‘en-impresso-hipe-v1-X’</td>
<td>English (historical)</td>
<td>In-domain data (Chronicling America material) for <a href="https://impresso.github.io/CLEF-HIPE-2020" class="external-link">CLEF HIPE Shared
task</a>. More information on the shared task can be found in <a href="https://zenodo.org/record/3752679#.XqgzxXUzZzU" class="external-link">this
paper</a>
</td>
</tr>
<tr class="odd">
<td>‘fr-impresso-hipe-v1-X’</td>
<td>French (historical)</td>
<td>In-domain data (Swiss and Luxembourgish newspapers) for <a href="https://impresso.github.io/CLEF-HIPE-2020" class="external-link">CLEF HIPE Shared
task</a>. More information on the shared task can be found in <a href="https://zenodo.org/record/3752679#.XqgzxXUzZzU" class="external-link">this
paper</a>
</td>
</tr>
<tr class="even">
<td>‘am-X’</td>
<td>Amharic</td>
<td>Based on 6.5m Amharic text corpus crawled from different sources.
See <a href="https://www.mdpi.com/1999-5903/13/11/275" class="external-link">this paper</a>
and the official <a href="https://github.com/uhh-lt/amharicmodels" class="external-link">GitHub Repository</a> for
more information.</td>
</tr>
<tr class="odd">
<td>‘uk-X’</td>
<td>Ukrainian</td>
<td>Added by <a href="https://github.com/dchaplinsky" class="external-link"><span class="citation">@dchaplinsky</span></a>: Trained with <a href="https://lang.org.ua/en/corpora/" class="external-link">UberText</a> corpus.</td>
</tr>
</tbody>
</table>
<p><strong>Source</strong>: <a href="https://github.com/flairNLP/flair/blob/master/resources/docs/embeddings/FLAIR_EMBEDDINGS.md#flair-embeddings" class="external-link">https://github.com/flairNLP/flair/blob/master/resources/docs/embeddings/FLAIR_EMBEDDINGS.md#flair-embeddings</a></p>
</div>
<div style="text-align: justify">
<p>So, if you want to load embeddings from the German forward LM model,
instantiate the method as follows:</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">flair_de_forward</span> <span class="op">&lt;-</span> <span class="fu">FlairEmbeddings</span><span class="op">(</span><span class="st">'de-forward'</span><span class="op">)</span></span></code></pre></div>
<p>And if you want to load embeddings from the Bulgarian backward LM
model, instantiate the method as follows:</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">flair_bg_backward</span> <span class="op">&lt;-</span> <span class="fu">FlairEmbeddings</span><span class="op">(</span><span class="st">'bg-backward'</span><span class="op">)</span></span></code></pre></div>
</div>
<p> </p>
<hr>
</div>
<div class="section level2">
<h2 id="recommended-flair-usage-in-flair-in-r">Recommended Flair Usage in {flaiR} in R<a class="anchor" aria-label="anchor" href="#recommended-flair-usage-in-flair-in-r"></a>
</h2>
<div style="text-align: justify">
<p>We recommend combining both forward and backward Flair embeddings.
Depending on the task, we also recommend adding standard word embeddings
into the mix. So, our recommended <code>StackedEmbedding</code> for most
English tasks is:</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">FlairEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">FlairEmbeddings</span></span>
<span><span class="va">WordEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">WordEmbeddings</span></span>
<span><span class="va">StackedEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">StackedEmbeddings</span></span>
<span></span>
<span><span class="co"># create a StackedEmbedding object that combines glove and forward/backward flair embeddings</span></span>
<span><span class="va">stacked_embeddings</span> <span class="op">&lt;-</span> <span class="fu">StackedEmbeddings</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="fu">WordEmbeddings</span><span class="op">(</span><span class="st">"glove"</span><span class="op">)</span>,</span>
<span>                                             <span class="fu">FlairEmbeddings</span><span class="op">(</span><span class="st">"news-forward"</span><span class="op">)</span>,</span>
<span>                                             <span class="fu">FlairEmbeddings</span><span class="op">(</span><span class="st">"news-backward"</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>That’s it! Now just use this embedding like all the other embeddings,
i.e. call the <code><a href="https://rdrr.io/r/stats/embed.html" class="external-link">embed()</a></code> method over your sentences.</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># create a sentence</span></span>
<span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span>
<span><span class="va">sentence</span> <span class="op">=</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">'The grass is green .'</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># just embed a sentence using the StackedEmbedding as you would with any single embedding.</span></span>
<span><span class="va">stacked_embeddings</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># now check out the embedded tokens.</span></span>
<span><span class="co"># Note that Python is indexing from 0. In an R for loop, using seq_along(sentence) - 1 achieves the same effect.</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span>  <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq_along</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">[</span><span class="va">i</span><span class="op">]</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">[</span><span class="va">i</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<pre><code><span><span class="co">## Token[0]: "The"</span></span>
<span><span class="co">## tensor([-0.0382, -0.2449,  0.7281,  ..., -0.0065, -0.0053,  0.0090])</span></span>
<span><span class="co">## Token[1]: "grass"</span></span>
<span><span class="co">## tensor([-0.8135,  0.9404, -0.2405,  ...,  0.0354, -0.0255, -0.0143])</span></span>
<span><span class="co">## Token[2]: "is"</span></span>
<span><span class="co">## tensor([-5.4264e-01,  4.1476e-01,  1.0322e+00,  ..., -5.3691e-04,</span></span>
<span><span class="co">##         -9.6750e-03, -2.7541e-02])</span></span>
<span><span class="co">## Token[3]: "green"</span></span>
<span><span class="co">## tensor([-0.6791,  0.3491, -0.2398,  ..., -0.0007, -0.1333,  0.0161])</span></span>
<span><span class="co">## Token[4]: "."</span></span>
<span><span class="co">## tensor([-0.3398,  0.2094,  0.4635,  ...,  0.0005, -0.0177,  0.0032])</span></span></code></pre>
<p>Words are now embedded using a concatenation of three different
embeddings. This combination often gives state-of-the-art accuracy.</p>
</div>
<p> </p>
<hr>
</div>
<div class="section level2">
<h2 id="pooled-flair-embeddings">Pooled Flair Embeddings<a class="anchor" aria-label="anchor" href="#pooled-flair-embeddings"></a>
</h2>
<div style="text-align: justify">
<p>We also developed a pooled variant of the
<code>FlairEmbeddings</code>. These embeddings differ in that they
<em>constantly evolve over time</em>, even at prediction time
(i.e. after training is complete). This means that the same words in the
same sentence at two different points in time may have different
embeddings.</p>
<p><code>PooledFlairEmbeddings</code> manage a ‘global’ representation
of each distinct word by using a pooling operation of all past
occurences. More details on how this works may be found in <a href="https://www.aclweb.org/anthology/N19-1078/" class="external-link">Akbik et
al. (2019)</a>.</p>
<p>You can instantiate and use <code>PooledFlairEmbeddings</code> like
any other embedding:</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># initiate embedding from Flair NLP</span></span>
<span><span class="va">PooledFlairEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">PooledFlairEmbeddings</span></span>
<span><span class="va">flair_embedding_forward</span> <span class="op">&lt;-</span> <span class="fu">PooledFlairEmbeddings</span><span class="op">(</span><span class="st">'news-forward'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># create a sentence object</span></span>
<span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">'The grass is green .'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># embed words in sentence</span></span>
<span><span class="va">flair_embedding_forward</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [[1]]</span></span>
<span><span class="co">## Sentence[5]: "The grass is green ."</span></span></code></pre>
<p>Note that while we get some of our best results with
<code>PooledFlairEmbeddings</code> they are very ineffective memory-wise
since they keep past embeddings of all words in memory. In many cases,
regular <code>FlairEmbeddings</code> will be nearly as good but with
much lower memory requirements.</p>
</div>
<p> </p>
<hr>
</div>
<div class="section level2">
<h2 id="transformer-embeddings">Transformer Embeddings<a class="anchor" aria-label="anchor" href="#transformer-embeddings"></a>
</h2>
<div style="text-align: justify">
<p>Please note that content and examples in this section have been
extensively revised from the <a href="https://github.com/flairNLP/flair/tree/master/resources/docs/embeddings/TRANSFORMER_EMBEDDINGS.md" class="external-link"><code>TransformerWordEmbeddings</code></a>
official documentation. Flair supports various Transformer-based
architectures like BERT or XLNet from <a href="https://github.com/huggingface" class="external-link">HuggingFace</a>, with two classes
<a href="#flair.embeddings.token.TransformerWordEmbeddings"><code>TransformerWordEmbeddings</code></a>
(to embed words or tokens) and <a href="#flair.embeddings.document.TransformerDocumentEmbeddings"><code>TransformerDocumentEmbeddings</code></a>
(to embed documents).</p>
</div>
<p> </p>
<div class="section level3">
<h3 id="embeddings-words-with-transformers">Embeddings Words with Transformers<a class="anchor" aria-label="anchor" href="#embeddings-words-with-transformers"></a>
</h3>
<div style="text-align: justify">
<p>For instance, to load a standard BERT transformer model, do:</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va">flaiR</span><span class="op">)</span></span>
<span><span class="co"># initiate embedding and load BERT model from HugginFaces</span></span>
<span><span class="va">TransformerWordEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">TransformerWordEmbeddings</span></span>
<span><span class="va">embedding</span> <span class="op">&lt;-</span> <span class="fu">TransformerWordEmbeddings</span><span class="op">(</span><span class="st">'bert-base-uncased'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># create a sentence</span></span>
<span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span>
<span><span class="va">sentence</span> <span class="op">=</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">'The grass is green .'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># embed words in sentence</span></span>
<span><span class="va">embedding</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [[1]]</span></span>
<span><span class="co">## Sentence[5]: "The grass is green ."</span></span></code></pre>
<p>If instead you want to use RoBERTa, do:</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">TransformerWordEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">TransformerWordEmbeddings</span></span>
<span><span class="va">embedding</span> <span class="op">&lt;-</span> <span class="fu">TransformerWordEmbeddings</span><span class="op">(</span><span class="st">'roberta-base'</span><span class="op">)</span></span>
<span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">'The grass is green .'</span><span class="op">)</span></span>
<span><span class="va">embedding</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [[1]]</span></span>
<span><span class="co">## Sentence[5]: "The grass is green ."</span></span></code></pre>
<p>{<code>flaiR</code>} interacts with Flair NLP (<a href="https://github.com/zalandoresearch/" class="external-link">Zalando Research</a>),
allowing you to use pre-trained models from <a href="https://https://huggingface.co/models" class="external-link">HuggingFace</a> , where you
can search for models to use.</p>
<div class="section level3">
<h3 id="embedding-documents-with-transformers">Embedding Documents with Transformers<a class="anchor" aria-label="anchor" href="#embedding-documents-with-transformers"></a>
</h3>
<p>To embed a whole sentence as one (instead of each word in the
sentence), simply use the <a href="#flair.embeddings.document.TransformerDocumentEmbeddings"><code>TransformerDocumentEmbeddings</code></a>
instead:</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">TransformerDocumentEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">TransformerDocumentEmbeddings</span></span>
<span><span class="va">embedding</span> <span class="op">&lt;-</span> <span class="fu">TransformerDocumentEmbeddings</span><span class="op">(</span><span class="st">'roberta-base'</span><span class="op">)</span></span>
<span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">'The grass is green .'</span><span class="op">)</span></span>
<span><span class="va">embedding</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [[1]]</span></span>
<span><span class="co">## Sentence[5]: "The grass is green ."</span></span></code></pre>
</div>
</div>
</div>
<div class="section level3">
<h3 id="arguments">Arguments<a class="anchor" aria-label="anchor" href="#arguments"></a>
</h3>
<div style="text-align: justify">
<p>There are several options that you can set when you init the <a href="#flair.embeddings.token.TransformerWordEmbeddings"><code>TransformerWordEmbeddings</code></a>
and <a href="#flair.embeddings.document.TransformerDocumentEmbeddings"><code>TransformerDocumentEmbeddings</code></a>
classes:</p>
<table class="table">
<colgroup>
<col width="19%">
<col width="17%">
<col width="62%">
</colgroup>
<thead><tr class="header">
<th>Argument</th>
<th>Default</th>
<th>Description</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><code>model</code></td>
<td><code>bert-base-uncased</code></td>
<td>The string identifier of the transformer model you want to use (see
above)</td>
</tr>
<tr class="even">
<td><code>layers</code></td>
<td><code>all</code></td>
<td>Defines the layers of the Transformer-based model that produce the
embedding</td>
</tr>
<tr class="odd">
<td><code>subtoken_pooling</code></td>
<td><code>first</code></td>
<td>See <a href="#Pooling-operation">Pooling operation section</a>.</td>
</tr>
<tr class="even">
<td><code>layer_mean</code></td>
<td><code>True</code></td>
<td>See <a href="#Layer-mean">Layer mean section</a>.</td>
</tr>
<tr class="odd">
<td><code>fine_tune</code></td>
<td><code>False</code></td>
<td>Whether or not embeddings are fine-tuneable.</td>
</tr>
<tr class="even">
<td><code>allow_long_sentences</code></td>
<td><code>True</code></td>
<td>Whether or not texts longer than maximal sequence length are
supported.</td>
</tr>
<tr class="odd">
<td><code>use_context</code></td>
<td><code>False</code></td>
<td>Set to True to include context outside of sentences. This can
greatly increase accuracy on some tasks, but slows down embedding
generation.</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section level3">
<h3 id="layers">Layers<a class="anchor" aria-label="anchor" href="#layers"></a>
</h3>
<div style="text-align: justify">
<p>The <code>layers</code> argument controls which transformer layers
are used for the embedding. If you set this value to ‘-1,-2,-3,-4’, the
top 4 layers are used to make an embedding. If you set it to ‘-1’, only
the last layer is used. If you set it to “all”, then all layers are
used. This affects the length of an embedding, since layers are just
concatenated.</p>
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span>
<span><span class="va">TransformerWordEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">TransformerWordEmbeddings</span></span>
<span><span class="va">sentence</span> <span class="op">=</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">'The grass is green.'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># use only last layers</span></span>
<span><span class="va">embeddings</span> <span class="op">&lt;-</span> <span class="fu">TransformerWordEmbeddings</span><span class="op">(</span><span class="st">'bert-base-uncased'</span>, layers<span class="op">=</span><span class="st">'-1'</span>, layer_mean <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="va">embeddings</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [[1]]</span></span>
<span><span class="co">## Sentence[5]: "The grass is green."</span></span></code></pre>
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">size</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## torch.Size([768])</span></span></code></pre>
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">sentence</span><span class="op">$</span><span class="fu">clear_embeddings</span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">'The grass is green.'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># use only last layers</span></span>
<span><span class="va">embeddings</span> <span class="op">&lt;-</span> <span class="fu">TransformerWordEmbeddings</span><span class="op">(</span><span class="st">'bert-base-uncased'</span>, layers <span class="op">=</span> <span class="st">"-1"</span>, layer_mean <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="va">embeddings</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [[1]]</span></span>
<span><span class="co">## Sentence[5]: "The grass is green."</span></span></code></pre>
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">size</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## torch.Size([768])</span></span></code></pre>
<div class="sourceCode" id="cb32"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">sentence</span><span class="op">$</span><span class="fu">clear_embeddings</span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb33"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># use last two layers</span></span>
<span><span class="va">embeddings</span> <span class="op">&lt;-</span> <span class="fu">TransformerWordEmbeddings</span><span class="op">(</span><span class="st">'bert-base-uncased'</span>, layers<span class="op">=</span><span class="st">'-1,-2'</span>, layer_mean <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="va">embeddings</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [[1]]</span></span>
<span><span class="co">## Sentence[5]: "The grass is green."</span></span></code></pre>
<div class="sourceCode" id="cb35"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">size</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## torch.Size([1536])</span></span></code></pre>
<div class="sourceCode" id="cb37"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">sentence</span><span class="op">$</span><span class="fu">clear_embeddings</span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb38"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># use ALL layers</span></span>
<span><span class="va">embeddings</span> <span class="op">=</span> <span class="fu">TransformerWordEmbeddings</span><span class="op">(</span><span class="st">'bert-base-uncased'</span>, layers<span class="op">=</span><span class="st">'all'</span>, layer_mean<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="va">embeddings</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [[1]]</span></span>
<span><span class="co">## Sentence[5]: "The grass is green."</span></span></code></pre>
<div class="sourceCode" id="cb40"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">size</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## torch.Size([9984])</span></span></code></pre>
<p>Here’s an example of how it might be done:</p>
<p>You can directly import torch from reticulate since it has already
been installed through the flair dependency when you installed flair in
Python.</p>
<div class="sourceCode" id="cb42"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># You can directly import torch from reticulate since it has already been installed through the flair dependency when you installed flair in Python.</span></span>
<span><span class="va">torch</span> <span class="op">&lt;-</span> <span class="fu">reticulate</span><span class="fu">::</span><span class="fu"><a href="https://rstudio.github.io/reticulate/reference/import.html" class="external-link">import</a></span><span class="op">(</span><span class="st">'torch'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Attempting to create a tensor with integer dimensions</span></span>
<span><span class="va">torch</span><span class="op">$</span><span class="fu">Size</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="fl">768L</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## torch.Size([768])</span></span></code></pre>
<div class="sourceCode" id="cb44"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">torch</span><span class="op">$</span><span class="fu">Size</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="fl">1536L</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## torch.Size([1536])</span></span></code></pre>
<div class="sourceCode" id="cb46"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">torch</span><span class="op">$</span><span class="fu">Size</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="fl">9984L</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## torch.Size([9984])</span></span></code></pre>
<p>Notice the L after the numbers in the list? This ensures that R
treats the numbers as integers. If you’re generating these numbers
dynamically (e.g., through computation), you might want to ensure they
are integers before attempting to create the tensor. I.e. the size of
the embedding increases the mode layers we use (but ONLY if layer_mean
is set to False, otherwise the length is always the same).</p>
</div>
</div>
<div class="section level3">
<h3 id="pooling-operation">Pooling Operation<a class="anchor" aria-label="anchor" href="#pooling-operation"></a>
</h3>
<div style="text-align: justify">
<p>Most of the Transformer-based models use subword tokenization. E.g.
the following token <code>puppeteer</code> could be tokenized into the
subwords: <code>pupp</code>, <code>##ete</code> and
<code>##er</code>.</p>
<p>We implement different pooling operations for these subwords to
generate the final token representation:</p>
<ul>
<li>
<code>first</code>: only the embedding of the first subword is
used</li>
<li>
<code>last</code>: only the embedding of the last subword is
used</li>
<li>
<code>first_last</code>: embeddings of the first and last subwords
are concatenated and used</li>
<li>
<code>mean</code>: a <code>torch.mean</code> over all subword
embeddings is calculated and used</li>
</ul>
<p>You can choose which one to use by passing this in the
constructor:</p>
<div class="sourceCode" id="cb48"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># use first and last subtoken for each word</span></span>
<span><span class="va">embeddings</span> <span class="op">=</span> <span class="fu">TransformerWordEmbeddings</span><span class="op">(</span><span class="st">'bert-base-uncased'</span>, subtoken_pooling<span class="op">=</span><span class="st">'first_last'</span><span class="op">)</span></span>
<span><span class="va">embeddings</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [[1]]</span></span>
<span><span class="co">## Sentence[5]: "The grass is green."</span></span></code></pre>
<div class="sourceCode" id="cb50"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">size</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## torch.Size([9984])</span></span></code></pre>
</div>
</div>
<div class="section level3">
<h3 id="layer-mean">Layer Mean<a class="anchor" aria-label="anchor" href="#layer-mean"></a>
</h3>
<div style="text-align: justify">
<p>The Transformer-based models have a certain number of layers. By
default, all layers you select are concatenated as explained above.
Alternatively, you can set <code>layer_mean=True</code> to do a mean
over all selected layers. The resulting vector will then always have the
same dimensionality as a single layer:</p>
<div class="sourceCode" id="cb52"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># initiate embedding from transformer. This model will be downloaded from Flair NLP huggingface.</span></span>
<span><span class="va">embeddings</span> <span class="op">&lt;-</span> <span class="fu">TransformerWordEmbeddings</span><span class="op">(</span><span class="st">'bert-base-uncased'</span>, layers<span class="op">=</span><span class="st">"all"</span>, layer_mean<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># create a sentence object</span></span>
<span><span class="va">sentence</span> <span class="op">=</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">"The Oktoberfest is the world's largest Volksfest ."</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># embed words in sentence</span></span>
<span><span class="va">embedding</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [[1]]</span></span>
<span><span class="co">## Sentence[9]: "The Oktoberfest is the world's largest Volksfest ."</span></span></code></pre>
</div>
</div>
<div class="section level3">
<h3 id="fine-tuneable-or-not">Fine-tuneable or Not<a class="anchor" aria-label="anchor" href="#fine-tuneable-or-not"></a>
</h3>
<div style="text-align: justify">
<p>Here’s an example of how it might be done: In some setups, you may
wish to fine-tune the transformer embeddings. In this case, set
<code>fine_tune=True</code> in the init method. When fine-tuning, you
should also only use the topmost layer, so best set
<code>layers='-1'</code>.</p>
<div class="sourceCode" id="cb54"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># use first and last subtoken for each word</span></span>
<span><span class="va">TransformerWordEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">TransformerWordEmbeddings</span></span>
<span><span class="va">embeddings</span> <span class="op">&lt;-</span> <span class="fu">TransformerWordEmbeddings</span><span class="op">(</span><span class="st">'bert-base-uncased'</span>, fine_tune<span class="op">=</span><span class="cn">TRUE</span>, layers<span class="op">=</span><span class="st">'-1'</span><span class="op">)</span></span>
<span><span class="va">embeddings</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [[1]]</span></span>
<span><span class="co">## Sentence[9]: "The Oktoberfest is the world's largest Volksfest ."</span></span></code></pre>
<p>This will print a tensor that now has a gradient function and can be
fine-tuned if you use it in a training routine.</p>
<div class="sourceCode" id="cb56"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## tensor([-6.5871e-01,  1.0410e-01,  3.4632e-01, -3.3775e-01, -2.1013e-01,</span></span>
<span><span class="co">##         -1.3036e-02,  5.1998e-01,  1.6574e+00, -5.2519e-02, -4.8634e-02,</span></span>
<span><span class="co">##         -7.8968e-01, -9.5546e-01, -1.9723e-01,  9.4999e-01, -1.0336e+00,</span></span>
<span><span class="co">##          8.6670e-02,  9.8104e-02,  5.6512e-02,  3.1074e-02,  2.4157e-01,</span></span>
<span><span class="co">##         -1.1427e-01, -2.3692e-01, -2.0700e-01,  7.7984e-01,  2.5460e-01,</span></span>
<span><span class="co">##         -5.0826e-03, -2.4110e-01,  2.2436e-01, -7.3250e-02, -8.1094e-01,</span></span>
<span><span class="co">##         -1.8778e-01,  2.1219e-01, -5.9514e-01,  6.3128e-02, -4.8880e-01,</span></span>
<span><span class="co">##         -3.2300e-02, -1.9124e-02, -1.0991e-01, -1.5604e-02,  4.3068e-01,</span></span>
<span><span class="co">##         -1.7968e-01, -5.4499e-01,  7.0608e-01, -4.0512e-01,  1.7761e-01,</span></span>
<span><span class="co">##         -8.5820e-01,  2.3438e-02, -1.4981e-01, -9.0368e-01, -2.1097e-01,</span></span>
<span><span class="co">##         -3.3535e-01,  1.4919e-01, -7.4523e-03,  1.0239e+00, -6.1777e-02,</span></span>
<span><span class="co">##          3.3913e-01,  8.5811e-02,  6.9401e-01, -7.7482e-02,  3.1483e-01,</span></span>
<span><span class="co">##         -4.3921e-01,  1.2933e+00,  5.8008e-03, -7.0992e-01,  2.7525e-01,</span></span>
<span><span class="co">##          8.8792e-01,  2.6305e-03,  1.3640e+00,  5.6886e-01, -2.4904e-01,</span></span>
<span><span class="co">##         -4.5158e-02, -1.7575e-01, -3.4730e-01,  5.8363e-02, -2.0346e-01,</span></span>
<span><span class="co">##         -1.2505e+00, -3.0592e-01, -3.6104e-02, -2.4066e-01, -5.1250e-01,</span></span>
<span><span class="co">##          2.6930e-01,  1.4068e-01,  3.4056e-01,  7.3297e-01,  2.6848e-01,</span></span>
<span><span class="co">##          2.4303e-01, -9.4885e-01, -9.0367e-01, -1.3184e-01,  6.7348e-01,</span></span>
<span><span class="co">##         -3.2995e-02,  4.7660e-01, -7.1619e-03, -3.4141e-01,  6.8473e-01,</span></span>
<span><span class="co">##         -4.4869e-01, -4.9831e-01, -8.0143e-01,  1.4073e+00,  5.3251e-01,</span></span>
<span><span class="co">##          2.4643e-01, -4.2528e-01,  9.1615e-02,  6.4495e-01,  1.7931e-01,</span></span>
<span><span class="co">##         -2.1473e-01,  1.5447e-01, -3.2978e-01,  1.0799e-01, -1.9402e+00,</span></span>
<span><span class="co">##         -5.0380e-01, -2.7636e-01, -1.1228e-01,  1.1576e-01,  2.5885e-01,</span></span>
<span><span class="co">##         -1.7916e-01,  6.6166e-01, -9.6098e-01, -5.1242e-01, -3.5424e-01,</span></span>
<span><span class="co">##          2.1383e-01,  6.6456e-01,  2.5498e-01,  3.7250e-01, -1.1821e+00,</span></span>
<span><span class="co">##         -4.9551e-01, -2.0858e-01,  1.1511e+00, -1.0365e-02, -1.0682e+00,</span></span>
<span><span class="co">##          3.7277e-01,  6.4048e-01,  2.3308e-01, -9.3824e-01,  9.5013e-02,</span></span>
<span><span class="co">##          5.7904e-01,  6.3969e-01,  8.2360e-02, -1.4075e-01,  3.0107e-01,</span></span>
<span><span class="co">##          3.5823e-03, -4.4684e-01, -2.6913e+00, -3.3933e-01,  2.8729e-03,</span></span>
<span><span class="co">##         -1.3639e-01, -7.1054e-01, -1.1048e+00,  2.2374e-01,  1.1830e-01,</span></span>
<span><span class="co">##          4.8416e-01, -2.9110e-01, -6.7650e-01,  2.3202e-01, -1.0123e-01,</span></span>
<span><span class="co">##         -1.9174e-01,  4.9959e-02,  5.2067e-01,  1.3272e+00,  6.8250e-01,</span></span>
<span><span class="co">##          5.5332e-01, -1.0886e+00,  4.5160e-01, -1.5010e-01, -9.8074e-01,</span></span>
<span><span class="co">##          8.5111e-02,  1.6498e-01,  6.6032e-01,  1.0815e-02,  1.8952e-01,</span></span>
<span><span class="co">##         -5.6608e-01, -1.3743e-02,  9.1170e-01,  2.7812e-01,  2.9551e-01,</span></span>
<span><span class="co">##         -3.5637e-01,  3.2030e-01,  5.6738e-01, -1.5707e-01,  3.5326e-01,</span></span>
<span><span class="co">##         -4.7747e-01,  7.8646e-01,  1.3765e-01,  2.2440e-01,  4.2422e-01,</span></span>
<span><span class="co">##         -2.6504e-01,  2.2016e-02, -6.7154e-01, -8.7999e-02,  1.4284e-01,</span></span>
<span><span class="co">##          4.0983e-01,  1.0931e-02, -1.0704e+00, -1.9350e-01,  6.0051e-01,</span></span>
<span><span class="co">##          5.0544e-02,  1.1433e-02, -8.0243e-01, -6.6871e-01,  5.3953e-01,</span></span>
<span><span class="co">##         -5.9856e-01, -1.6915e-01, -3.5307e-01,  4.4568e-01, -7.2761e-01,</span></span>
<span><span class="co">##          1.1629e+00, -3.1553e-01, -7.9747e-01, -2.0582e-01,  3.7320e-01,</span></span>
<span><span class="co">##          5.9379e-01, -3.1898e-01, -1.6932e-01, -6.2492e-01,  5.7047e-01,</span></span>
<span><span class="co">##         -2.9779e-01, -5.9106e-01,  8.5436e-02, -2.1839e-01, -2.2214e-01,</span></span>
<span><span class="co">##          7.9233e-01,  8.0536e-01, -5.9784e-01,  4.0474e-01,  3.9265e-01,</span></span>
<span><span class="co">##          5.8169e-01, -5.2506e-01,  6.9786e-01,  1.1163e-01,  8.7434e-02,</span></span>
<span><span class="co">##          1.7549e-01,  9.1439e-02,  5.8816e-01,  6.4338e-01, -2.7138e-01,</span></span>
<span><span class="co">##         -5.3449e-01, -1.0168e+00, -5.1335e-02,  3.0099e-01, -7.6696e-02,</span></span>
<span><span class="co">##         -2.1126e-01,  5.8143e-01,  1.3599e-01,  6.2759e-01, -6.2810e-01,</span></span>
<span><span class="co">##          5.9966e-01,  3.5836e-01, -3.0706e-02,  1.5563e-01, -1.4016e-01,</span></span>
<span><span class="co">##         -2.0155e-01, -1.3755e+00, -9.1877e-02, -6.9892e-01,  7.9439e-02,</span></span>
<span><span class="co">##         -4.2926e-01,  3.7988e-01,  7.6741e-01,  5.3094e-01,  8.5981e-01,</span></span>
<span><span class="co">##          4.4184e-02, -6.3507e-01,  3.9587e-01, -3.6635e-01, -7.0770e-01,</span></span>
<span><span class="co">##          8.3651e-04, -3.0055e-01,  2.1360e-01, -4.1649e-01,  6.9457e-01,</span></span>
<span><span class="co">##         -6.2715e-01, -5.1101e-01,  3.0331e-01, -2.3804e+00, -1.0567e-02,</span></span>
<span><span class="co">##         -9.4488e-01,  4.3318e-02,  2.4188e-01,  1.9204e-02,  1.5705e-03,</span></span>
<span><span class="co">##         -3.0374e-01,  3.1933e-01, -7.4432e-01,  1.4599e-01, -5.2101e-01,</span></span>
<span><span class="co">##         -5.2269e-01,  1.3274e-01, -2.8936e-01,  4.1706e-02,  2.6143e-01,</span></span>
<span><span class="co">##         -4.4796e-01,  7.3136e-01,  6.3894e-02,  4.7398e-01, -5.1062e-01,</span></span>
<span><span class="co">##         -1.3705e-01,  2.0763e-01, -3.9115e-01,  2.8822e-01, -3.5283e-01,</span></span>
<span><span class="co">##          3.4881e-02, -3.3602e-01,  1.7210e-01,  1.3537e-02, -5.3036e-01,</span></span>
<span><span class="co">##          1.2847e-01, -4.5576e-01, -3.7251e-01, -3.2254e+00, -3.1650e-01,</span></span>
<span><span class="co">##         -2.6144e-01, -9.4983e-02,  2.7650e-02, -2.3750e-01,  3.1001e-01,</span></span>
<span><span class="co">##          1.1428e-01, -1.2870e-01, -4.7496e-01,  4.4594e-01, -3.6137e-01,</span></span>
<span><span class="co">##         -3.1009e-01, -9.9612e-02,  5.3967e-01,  1.2840e-02,  1.4507e-01,</span></span>
<span><span class="co">##         -2.5181e-01,  1.9310e-01,  4.1073e-01,  5.9776e-01, -2.5585e-01,</span></span>
<span><span class="co">##          5.7184e-02, -5.1505e-01, -6.8709e-02,  4.7767e-01, -1.2078e-01,</span></span>
<span><span class="co">##         -5.0894e-01, -9.2884e-01,  7.8471e-01,  2.0216e-01,  4.3242e-01,</span></span>
<span><span class="co">##          3.2803e-01, -1.0122e-01,  3.3529e-01, -1.2183e-01, -5.5060e-01,</span></span>
<span><span class="co">##          3.5427e-01,  7.4559e-02, -3.1411e-01, -1.7512e-01,  2.2485e-01,</span></span>
<span><span class="co">##          4.2295e-01,  7.7110e-02,  1.8063e+00,  7.6644e-03, -1.1082e-02,</span></span>
<span><span class="co">##         -2.8605e-02,  7.7144e-02,  8.2345e-02,  8.0270e-02, -1.1858e+00,</span></span>
<span><span class="co">##          2.0523e-01,  3.4053e-01,  2.0424e-01, -2.0574e-02,  3.0466e-01,</span></span>
<span><span class="co">##         -2.1858e-01,  6.3737e-01, -5.6264e-01,  1.4153e-01,  2.4319e-01,</span></span>
<span><span class="co">##         -5.6688e-01,  7.2375e-02, -2.9329e-01,  4.6562e-02,  1.8977e-01,</span></span>
<span><span class="co">##          2.4977e-01,  9.1892e-01,  1.1346e-01,  3.8588e-01, -3.5543e-01,</span></span>
<span><span class="co">##         -1.3380e+00, -8.5645e-01, -5.5443e-01, -7.2317e-01, -2.9225e-01,</span></span>
<span><span class="co">##         -1.4389e-01,  6.9714e-01, -5.9852e-01, -6.8932e-01, -6.0952e-01,</span></span>
<span><span class="co">##          1.8234e-01, -7.5840e-02,  3.6445e-01, -3.8286e-01,  2.6545e-01,</span></span>
<span><span class="co">##         -2.6569e-01, -4.9999e-01, -3.8354e-01, -2.2809e-01,  8.8314e-01,</span></span>
<span><span class="co">##          2.9041e-01,  5.4803e-01, -1.0668e+00,  4.7405e-01,  7.8804e-02,</span></span>
<span><span class="co">##         -1.1559e+00, -3.0649e-01,  6.0480e-02, -7.1279e-01, -4.3335e-01,</span></span>
<span><span class="co">##         -8.2446e-04, -1.0236e-01,  3.5497e-01,  1.8665e-01,  1.2045e-01,</span></span>
<span><span class="co">##          1.2071e-01,  6.2911e-01,  3.1421e-01, -2.1635e-01, -8.9416e-01,</span></span>
<span><span class="co">##          6.6360e-01, -9.2980e-01,  6.9193e-01, -2.5403e-01, -2.5836e-02,</span></span>
<span><span class="co">##          1.2342e+00, -6.5908e-01,  7.5741e-01,  2.9014e-01,  3.0760e-01,</span></span>
<span><span class="co">##         -1.0249e+00, -2.7089e-01,  4.6132e-01,  6.1510e-02,  2.5385e-01,</span></span>
<span><span class="co">##         -5.2075e-01, -3.5107e-01,  3.3694e-01, -2.5047e-01, -2.7855e-01,</span></span>
<span><span class="co">##          2.0280e-01, -1.5703e-01,  4.1618e-02,  1.4451e-01, -1.6666e-01,</span></span>
<span><span class="co">##         -3.0519e-01, -9.4271e-02, -1.7083e-01,  5.2454e-01,  2.4524e-01,</span></span>
<span><span class="co">##          2.0731e-01,  3.7948e-01,  9.7359e-02, -3.2451e-02,  5.5792e-01,</span></span>
<span><span class="co">##         -2.4703e-01,  5.2864e-01,  5.6343e-01, -1.9198e-01, -8.3369e-02,</span></span>
<span><span class="co">##         -6.5377e-01, -5.4104e-01,  1.8289e-01, -4.9146e-01,  6.6423e-01,</span></span>
<span><span class="co">##         -5.2809e-01, -1.4797e-01, -4.5526e-02, -3.9593e-01,  1.2841e-01,</span></span>
<span><span class="co">##         -7.8591e-01, -3.7564e-02,  6.1912e-01,  3.2458e-01,  3.7858e-01,</span></span>
<span><span class="co">##          1.8744e-01, -5.0738e-01,  8.0222e-02, -3.1468e-02, -1.5145e-01,</span></span>
<span><span class="co">##          1.6657e-01, -5.2251e-01, -2.5940e-01, -3.8505e-01, -7.4942e-02,</span></span>
<span><span class="co">##          3.9530e-01, -2.1742e-01, -1.7113e-01, -5.2492e-01, -7.7781e-02,</span></span>
<span><span class="co">##         -6.9759e-01,  2.2570e-01, -1.2935e-01,  3.0750e-01, -1.3554e-01,</span></span>
<span><span class="co">##          6.0182e-02, -1.1479e-01,  4.7263e-01,  3.7957e-01,  8.9523e-01,</span></span>
<span><span class="co">##         -3.6411e-01, -6.6355e-01, -7.6647e-01, -1.4479e+00, -5.2238e-01,</span></span>
<span><span class="co">##          2.3336e-02, -4.5736e-01,  5.9981e-01,  6.8700e-01,  4.2190e-02,</span></span>
<span><span class="co">##          1.5894e-01,  2.0743e-02,  9.2333e-02, -7.2747e-01,  1.2388e-01,</span></span>
<span><span class="co">##         -4.7257e-01, -2.9889e-01,  4.8955e-01, -9.1618e-01, -1.9497e-01,</span></span>
<span><span class="co">##         -1.4157e-01, -1.7472e-01,  4.9250e-02, -2.2264e-01,  6.1700e-01,</span></span>
<span><span class="co">##         -2.4691e-01,  6.0937e-01,  3.6134e-01,  4.3398e-01, -2.7615e-01,</span></span>
<span><span class="co">##         -2.6582e-01, -1.3132e-01, -4.4155e-02,  5.3686e-01,  1.2956e-01,</span></span>
<span><span class="co">##         -6.4218e-01, -1.5820e-01, -1.0249e+00, -9.3587e-03, -3.5060e-01,</span></span>
<span><span class="co">##          3.6650e-01,  4.9503e-01,  7.4325e-01,  9.6525e-02,  4.3141e-01,</span></span>
<span><span class="co">##          3.9512e-02, -7.0727e-02,  6.2696e-01,  1.3066e-01,  1.0243e-01,</span></span>
<span><span class="co">##          3.3839e-01,  1.9224e-01,  4.8800e-01, -2.1052e-01,  3.9523e-02,</span></span>
<span><span class="co">##          7.7567e-01, -1.2005e-01, -1.1262e-01,  8.7001e-02,  2.7273e-01,</span></span>
<span><span class="co">##         -4.6830e-02, -2.4966e-01, -3.2083e-01, -2.6389e-01,  1.6225e-01,</span></span>
<span><span class="co">##          2.8800e-01, -1.0799e-01, -1.0841e-01,  6.6873e-01,  3.4369e-01,</span></span>
<span><span class="co">##          5.8675e-01,  9.2084e-01, -1.8131e-01,  5.6373e-02, -5.7125e-01,</span></span>
<span><span class="co">##          3.1048e-01,  3.1629e-02,  1.2097e+00,  4.4492e-01, -2.3792e-01,</span></span>
<span><span class="co">##         -9.9342e-02, -5.0657e-01, -3.1333e-02,  1.5045e-01,  3.1493e-01,</span></span>
<span><span class="co">##         -4.1287e-01, -1.8618e-01, -4.2638e-02,  1.8266e+00,  4.8565e-01,</span></span>
<span><span class="co">##          6.3892e-01, -2.9107e-01, -3.2557e-01,  1.1089e-01, -1.3212e+00,</span></span>
<span><span class="co">##          7.1113e-01,  2.3618e-01,  2.1473e-01,  1.6360e-01, -5.2535e-01,</span></span>
<span><span class="co">##          3.4322e-01,  9.0777e-01,  1.8697e-01, -3.0532e-01,  2.7574e-01,</span></span>
<span><span class="co">##          5.1451e-01, -2.6733e-01,  2.4208e-01, -3.3234e-01,  6.3520e-01,</span></span>
<span><span class="co">##          2.5884e-01, -5.7923e-01,  3.0204e-01,  4.1746e-02,  4.7539e-02,</span></span>
<span><span class="co">##         -6.7038e-01,  4.6699e-01, -1.6951e-01, -1.5161e-01, -1.2805e-01,</span></span>
<span><span class="co">##         -4.3990e-01,  1.0177e+00, -3.8138e-01,  4.3114e-01, -7.5435e-03,</span></span>
<span><span class="co">##          2.7385e-01,  4.6314e-01, -8.6565e-02, -7.9458e-01,  1.4370e-02,</span></span>
<span><span class="co">##          2.6016e-01,  9.2556e-03,  9.3968e-01,  7.9679e-01,  3.3140e-03,</span></span>
<span><span class="co">##         -5.6733e-01,  2.9052e-01, -9.5894e-02,  1.8630e-01,  1.4475e-01,</span></span>
<span><span class="co">##          1.8935e-01,  5.1735e-01, -1.2187e+00, -1.3298e-01, -4.3538e-01,</span></span>
<span><span class="co">##         -6.5398e-01, -2.9286e-01,  1.3199e-01,  3.9075e-01,  9.0172e-01,</span></span>
<span><span class="co">##          9.9439e-01,  6.2783e-01, -1.6103e-01,  1.4158e-03, -9.1476e-01,</span></span>
<span><span class="co">##          7.7760e-01,  1.2264e+00,  8.1482e-02,  6.6732e-01, -7.4576e-01,</span></span>
<span><span class="co">##         -1.0470e-01, -6.7781e-01,  8.0405e-01,  3.6676e-02,  3.6362e-01,</span></span>
<span><span class="co">##          4.4962e-01,  8.9600e-01, -1.8276e+00,  6.7828e-01, -9.4125e-03,</span></span>
<span><span class="co">##          3.8665e-01, -2.2149e-02,  7.4756e-02,  3.7438e-01, -1.2696e-01,</span></span>
<span><span class="co">##         -5.3396e-01, -3.5782e-01,  3.0400e-01,  7.7663e-01, -1.9122e-01,</span></span>
<span><span class="co">##         -1.3041e-01, -2.1522e-01,  1.1086e+00,  1.0237e+00, -4.7552e-02,</span></span>
<span><span class="co">##         -3.9538e-01,  1.1568e+00, -4.2549e-01, -2.5640e-02,  2.1993e-01,</span></span>
<span><span class="co">##         -4.7488e-01, -7.7624e-02, -5.5211e-01, -5.3169e-01, -5.3790e-02,</span></span>
<span><span class="co">##         -6.0535e-01,  4.2789e-01, -3.8606e-01,  9.8630e-01,  4.3330e-01,</span></span>
<span><span class="co">##          4.8414e-01, -1.3518e-01, -6.5505e-01, -2.2913e-01, -3.1254e-01,</span></span>
<span><span class="co">##          1.2920e-01, -7.7763e-02, -3.1123e-01,  8.2576e-01,  8.6486e-01,</span></span>
<span><span class="co">##         -3.4766e-01, -3.8491e-01,  3.5732e-02,  3.7518e-01, -3.7511e-01,</span></span>
<span><span class="co">##          5.2371e-01, -7.9721e-01,  3.3401e-01,  8.3976e-01, -3.2525e-01,</span></span>
<span><span class="co">##         -3.0268e-01, -1.3558e-01,  2.2812e-01,  1.5632e-01,  3.1584e-01,</span></span>
<span><span class="co">##          9.3902e-02, -3.8647e-01, -1.0177e-01, -2.8833e-01,  3.6028e-01,</span></span>
<span><span class="co">##          2.2565e-01, -1.5595e-01, -4.4974e-01, -5.0904e-01,  4.5058e-01,</span></span>
<span><span class="co">##          7.9030e-01,  2.7041e-01, -3.6712e-01, -3.9090e-01,  2.3358e-01,</span></span>
<span><span class="co">##          1.2162e+00, -1.1371e+00, -8.2702e-01, -9.2747e-02,  5.8958e-01,</span></span>
<span><span class="co">##          4.4429e-02, -2.3344e-01, -5.6492e-01,  4.9406e-01, -4.0302e-01,</span></span>
<span><span class="co">##          5.0951e-01, -1.6740e-01, -4.0176e+00, -8.2092e-01, -3.9132e-01,</span></span>
<span><span class="co">##         -2.9754e-01, -2.6797e-01, -2.5174e-01,  6.6282e-01, -5.7532e-02,</span></span>
<span><span class="co">##          7.7360e-01,  2.5238e-01,  2.5732e-02,  1.7694e-01,  9.4647e-02,</span></span>
<span><span class="co">##          2.6885e-01,  9.3711e-01, -8.3929e-02])</span></span></code></pre>
<!-- ```python -->
<!-- tensor([-0.0323, -0.3904, -1.1946,  ...,  0.1305, -0.1365, -0.4323], -->
<!--        device='cuda:0', grad_fn=<CatBackward>) -->
<!-- ``` -->
</div>
</div>
<div class="section level3">
<h3 id="models">Models<a class="anchor" aria-label="anchor" href="#models"></a>
</h3>
<div style="text-align: justify">
<p>Please have a look at the awesome <a href="https://huggingface.co/models" class="external-link">HuggingFace</a> for all supported
pre-trained models!</p>
</div>
<p> </p>
<hr>
</div>
</div>
<div class="section level2">
<h2 id="classic-word-embeddings">Classic Word Embeddings<a class="anchor" aria-label="anchor" href="#classic-word-embeddings"></a>
</h2>
<div style="text-align: justify">
<p>Classic word embeddings are static and word-level, meaning that each
distinct word gets exactly one pre-computed embedding. Most embeddings
fall under this class, including the popular GloVe or Komninos
embeddings.</p>
<p>Simply instantiate the <code>WordEmbeddings</code> class and pass a
string identifier of the embedding you wish to load. So, if you want to
use GloVe embeddings, pass the string ‘glove’ to the constructor:</p>
<div class="sourceCode" id="cb58"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va">flaiR</span><span class="op">)</span></span>
<span><span class="co"># initiate embedding with glove</span></span>
<span><span class="va">WordEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">WordEmbeddings</span></span>
<span><span class="va">glove_embedding</span> <span class="op">&lt;-</span>  <span class="fu">WordEmbeddings</span><span class="op">(</span><span class="st">'glove'</span><span class="op">)</span></span></code></pre></div>
<p>Now, create an example sentence and call the embedding’s
<code><a href="https://rdrr.io/r/stats/embed.html" class="external-link">embed()</a></code> method. You can also pass a list of sentences to
this method since some embedding types make use of batching to increase
speed.</p>
<div class="sourceCode" id="cb59"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va">flaiR</span><span class="op">)</span></span>
<span><span class="co"># initiate a sentence object</span></span>
<span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span>
<span></span>
<span><span class="co"># create sentence object.</span></span>
<span><span class="va">sentence</span> <span class="op">=</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">'The grass is green .'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># embed a sentence using glove.</span></span>
<span><span class="va">glove_embedding</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [[1]]</span></span>
<span><span class="co">## Sentence[5]: "The grass is green ."</span></span></code></pre>
<p>This prints out the tokens and their embeddings. GloVe embeddings are
Pytorch vectors of dimensionality 100.</p>
<div class="sourceCode" id="cb61"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># view embedded tokens.</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">token</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq_along</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">[</span><span class="va">token</span><span class="op">]</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">[</span><span class="va">token</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">numpy</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<pre><code><span><span class="co">## Token[0]: "The"</span></span>
<span><span class="co">##   [1] -0.038194 -0.244870  0.728120 -0.399610  0.083172  0.043953</span></span>
<span><span class="co">##   [7] -0.391410  0.334400 -0.575450  0.087459  0.287870 -0.067310</span></span>
<span><span class="co">##  [13]  0.309060 -0.263840 -0.132310 -0.207570  0.333950 -0.338480</span></span>
<span><span class="co">##  [19] -0.317430 -0.483360  0.146400 -0.373040  0.345770  0.052041</span></span>
<span><span class="co">##  [25]  0.449460 -0.469710  0.026280 -0.541550 -0.155180 -0.141070</span></span>
<span><span class="co">##  [31] -0.039722  0.282770  0.143930  0.234640 -0.310210  0.086173</span></span>
<span><span class="co">##  [37]  0.203970  0.526240  0.171640 -0.082378 -0.717870 -0.415310</span></span>
<span><span class="co">##  [43]  0.203350 -0.127630  0.413670  0.551870  0.579080 -0.334770</span></span>
<span><span class="co">##  [49] -0.365590 -0.548570 -0.062892  0.265840  0.302050  0.997750</span></span>
<span><span class="co">##  [55] -0.804810 -3.024300  0.012540 -0.369420  2.216700  0.722010</span></span>
<span><span class="co">##  [61] -0.249780  0.921360  0.034514  0.467450  1.107900 -0.193580</span></span>
<span><span class="co">##  [67] -0.074575  0.233530 -0.052062 -0.220440  0.057162 -0.158060</span></span>
<span><span class="co">##  [73] -0.307980 -0.416250  0.379720  0.150060 -0.532120 -0.205500</span></span>
<span><span class="co">##  [79] -1.252600  0.071624  0.705650  0.497440 -0.420630  0.261480</span></span>
<span><span class="co">##  [85] -1.538000 -0.302230 -0.073438 -0.283120  0.371040 -0.252170</span></span>
<span><span class="co">##  [91]  0.016215 -0.017099 -0.389840  0.874240 -0.725690 -0.510580</span></span>
<span><span class="co">##  [97] -0.520280 -0.145900  0.827800  0.270620</span></span>
<span><span class="co">## Token[1]: "grass"</span></span>
<span><span class="co">##   [1] -0.8135300  0.9404200 -0.2404800 -0.1350100  0.0556780  0.3362500</span></span>
<span><span class="co">##   [7]  0.0802090 -0.1014800 -0.5477600 -0.3536500  0.0733820  0.2586800</span></span>
<span><span class="co">##  [13]  0.1986600 -0.1432800  0.2507000  0.4281400  0.1949800  0.5345600</span></span>
<span><span class="co">##  [19]  0.7424100  0.0578160 -0.3178100  0.9435900  0.8145000 -0.0823750</span></span>
<span><span class="co">##  [25]  0.6165800  0.7284400 -0.3262300 -1.3641000  0.1232000  0.5372800</span></span>
<span><span class="co">##  [31] -0.5122800  0.0245900  1.0822001 -0.2295900  0.6038500  0.5541500</span></span>
<span><span class="co">##  [37] -0.9609900  0.4803300  0.0022260  0.5591300 -0.1636500 -0.8468100</span></span>
<span><span class="co">##  [43]  0.0740790 -0.6215700  0.0259670 -0.5162100 -0.0524620 -0.1417700</span></span>
<span><span class="co">##  [49] -0.0161230 -0.4971900 -0.5534500 -0.4037100  0.5095600  1.0276000</span></span>
<span><span class="co">##  [55] -0.0840000 -1.1179000  0.3225700  0.4928100  0.9487600  0.2040300</span></span>
<span><span class="co">##  [61]  0.5388300  0.8397200 -0.0688830  0.3136100  1.0450000 -0.2266900</span></span>
<span><span class="co">##  [67] -0.0896010 -0.6427100  0.6442900 -1.1001000 -0.0095814  0.2668200</span></span>
<span><span class="co">##  [73] -0.3230200 -0.6065200  0.0479150 -0.1663700  0.8571200  0.2335500</span></span>
<span><span class="co">##  [79]  0.2539500  1.2546000  0.5471600 -0.1979600 -0.7186300  0.2076000</span></span>
<span><span class="co">##  [85] -0.2587500 -0.3649900  0.0834360  0.6931700  0.1573700  1.0931000</span></span>
<span><span class="co">##  [91]  0.0912950 -1.3773000 -0.2717000  0.7070800  0.1872000 -0.3307200</span></span>
<span><span class="co">##  [97] -0.2835900  0.1029600  1.2228000  0.8374100</span></span>
<span><span class="co">## Token[2]: "is"</span></span>
<span><span class="co">##   [1] -0.5426400  0.4147600  1.0322000 -0.4024400  0.4669100  0.2181600</span></span>
<span><span class="co">##   [7] -0.0748640  0.4733200  0.0809960 -0.2207900 -0.1280800 -0.1144000</span></span>
<span><span class="co">##  [13]  0.5089100  0.1156800  0.0282110 -0.3628000  0.4382300  0.0475110</span></span>
<span><span class="co">##  [19]  0.2028200  0.4985700 -0.1006800  0.1326900  0.1697200  0.1165300</span></span>
<span><span class="co">##  [25]  0.3135500  0.2571300  0.0927830 -0.5682600 -0.5297500 -0.0514560</span></span>
<span><span class="co">##  [31] -0.6732600  0.9253300  0.2693000  0.2273400  0.6636500  0.2622100</span></span>
<span><span class="co">##  [37]  0.1971900  0.2609000  0.1877400 -0.3454000 -0.4263500  0.1397500</span></span>
<span><span class="co">##  [43]  0.5633800 -0.5690700  0.1239800 -0.1289400  0.7248400 -0.2610500</span></span>
<span><span class="co">##  [49] -0.2631400 -0.4360500  0.0789080 -0.8414600  0.5159500  1.3997000</span></span>
<span><span class="co">##  [55] -0.7646000 -3.1452999 -0.2920200 -0.3124700  1.5129000  0.5243500</span></span>
<span><span class="co">##  [61]  0.2145600  0.4245200 -0.0884110 -0.1780500  1.1876000  0.1057900</span></span>
<span><span class="co">##  [67]  0.7657100  0.2191400  0.3582400 -0.1163600  0.0932610 -0.6248300</span></span>
<span><span class="co">##  [73] -0.2189800  0.2179600  0.7405600 -0.4373500  0.1434300  0.1471900</span></span>
<span><span class="co">##  [79] -1.1605000 -0.0505080  0.1267700 -0.0143950 -0.9867600 -0.0912970</span></span>
<span><span class="co">##  [85] -1.2054000 -0.1197400  0.0478470 -0.5400100  0.5245700 -0.7096300</span></span>
<span><span class="co">##  [91] -0.3252800 -0.1346000 -0.4131400  0.3343500 -0.0072412  0.3225300</span></span>
<span><span class="co">##  [97] -0.0442190 -1.2969000  0.7621700  0.4634900</span></span>
<span><span class="co">## Token[3]: "green"</span></span>
<span><span class="co">##   [1] -0.67907000  0.34908000 -0.23984000 -0.99651998  0.73782003</span></span>
<span><span class="co">##   [6] -0.00065911  0.28009999  0.01728700 -0.36063001  0.03695500</span></span>
<span><span class="co">##  [11] -0.40395001  0.02409200  0.28957999  0.40496999  0.69992000</span></span>
<span><span class="co">##  [16]  0.25268999  0.80350000  0.04937000  0.15561999 -0.00632860</span></span>
<span><span class="co">##  [21] -0.29414001  0.14727999  0.18977000 -0.51791000  0.36985999</span></span>
<span><span class="co">##  [26]  0.74581999  0.08268900 -0.72601002 -0.40939000 -0.09782200</span></span>
<span><span class="co">##  [31] -0.14095999  0.71121001  0.61932999 -0.25014001  0.42250001</span></span>
<span><span class="co">##  [36]  0.48458001 -0.51915002  0.77125001  0.36684999  0.49652001</span></span>
<span><span class="co">##  [41] -0.04129800 -1.46829998  0.20038000  0.18591000  0.04986000</span></span>
<span><span class="co">##  [46] -0.17523000 -0.35528001  0.94152999 -0.11898000 -0.51902997</span></span>
<span><span class="co">##  [51] -0.01188700 -0.39186001 -0.17478999  0.93450999 -0.58930999</span></span>
<span><span class="co">##  [56] -2.77010012  0.34522000  0.86532998  1.08080006 -0.10291000</span></span>
<span><span class="co">##  [61] -0.09122000  0.55092001 -0.39473000  0.53675997  1.03830004</span></span>
<span><span class="co">##  [66] -0.40658000  0.24590001 -0.26797000 -0.26036000 -0.14150999</span></span>
<span><span class="co">##  [71] -0.12022000  0.16234000 -0.74320000 -0.64727998  0.04713300</span></span>
<span><span class="co">##  [76]  0.51642001  0.19898000  0.23919000  0.12549999  0.22471000</span></span>
<span><span class="co">##  [81]  0.82612997  0.07832800 -0.57020003  0.02393400 -0.15410000</span></span>
<span><span class="co">##  [86] -0.25738999  0.41262001 -0.46967000  0.87914002  0.72628999</span></span>
<span><span class="co">##  [91]  0.05386200 -1.15750003 -0.47835001  0.20139000 -1.00510001</span></span>
<span><span class="co">##  [96]  0.11515000 -0.96609002  0.12960000  0.18388000 -0.03038300</span></span>
<span><span class="co">## Token[4]: "."</span></span>
<span><span class="co">##   [1] -0.3397900  0.2094100  0.4634800 -0.6479200 -0.3837700  0.0380340</span></span>
<span><span class="co">##   [7]  0.1712700  0.1597800  0.4661900 -0.0191690  0.4147900 -0.3434900</span></span>
<span><span class="co">##  [13]  0.2687200  0.0446400  0.4213100 -0.4103200  0.1545900  0.0222390</span></span>
<span><span class="co">##  [19] -0.6465300  0.2525600  0.0431360 -0.1944500  0.4651600  0.4565100</span></span>
<span><span class="co">##  [25]  0.6858800  0.0912950  0.2187500 -0.7035100  0.1678500 -0.3507900</span></span>
<span><span class="co">##  [31] -0.1263400  0.6638400 -0.2582000  0.0365420 -0.1360500  0.4025300</span></span>
<span><span class="co">##  [37]  0.1428900  0.3813200 -0.1228300 -0.4588600 -0.2528200 -0.3043200</span></span>
<span><span class="co">##  [43] -0.1121500 -0.2618200 -0.2248200 -0.4455400  0.2991000 -0.8561200</span></span>
<span><span class="co">##  [49] -0.1450300 -0.4908600  0.0082973 -0.1749100  0.2752400  1.4401000</span></span>
<span><span class="co">##  [55] -0.2123900 -2.8434999 -0.2795800 -0.4572200  1.6386000  0.7880800</span></span>
<span><span class="co">##  [61] -0.5526200  0.6500000  0.0864260  0.3901200  1.0632000 -0.3537900</span></span>
<span><span class="co">##  [67]  0.4832800  0.3460000  0.8417400  0.0987070 -0.2421300 -0.2705300</span></span>
<span><span class="co">##  [73]  0.0452870 -0.4014700  0.1139500  0.0062226  0.0366730  0.0185180</span></span>
<span><span class="co">##  [79] -1.0213000 -0.2080600  0.6407200 -0.0687630 -0.5863500  0.3347600</span></span>
<span><span class="co">##  [85] -1.1432000 -0.1148000 -0.2509100 -0.4590700 -0.0968190 -0.1794600</span></span>
<span><span class="co">##  [91] -0.0633510 -0.6741200 -0.0688950  0.5360400 -0.8777300  0.3180200</span></span>
<span><span class="co">##  [97] -0.3924200 -0.2339400  0.4729800 -0.0288030</span></span></code></pre>
<p>You choose which pre-trained embeddings you load by passing the
appropriate id string to the constructor of the
<code>WordEmbeddings</code> class. Typically, you use the
<strong>two-letter language code</strong> to init an embedding, so ‘en’
for English and ‘de’ for German and so on. By default, this will
initialize FastText embeddings trained over Wikipedia. You can also
always use <em>FastText</em> embeddings over Web crawls, by
instantiating with ‘-crawl’. So ‘de-crawl’ to use embeddings trained
over German web crawls.</p>
<p>For English, we provide a few more options, so here you can choose
between instantiating ‘<code>en-glove</code>’, ‘<code>en-extvec</code>’
and so on.</p>
<div class="section level4">
<h4 id="suppored-models">Suppored Models:<a class="anchor" aria-label="anchor" href="#suppored-models"></a>
</h4>
<p>The following embeddings are currently supported:</p>
<table class="table">
<colgroup>
<col width="33%">
<col width="33%">
<col width="33%">
</colgroup>
<thead><tr class="header">
<th>ID</th>
<th>Language</th>
<th>Embedding</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>‘en-glove’ (or ‘glove’)</td>
<td>English</td>
<td>GloVe embeddings</td>
</tr>
<tr class="even">
<td>‘en-extvec’ (or ‘extvec’)</td>
<td>English</td>
<td>Komninos embeddings</td>
</tr>
<tr class="odd">
<td>‘en-crawl’ (or ‘crawl’)</td>
<td>English</td>
<td>FastText embeddings over Web crawls</td>
</tr>
<tr class="even">
<td>‘en-twitter’ (or ‘twitter’)</td>
<td>English</td>
<td>Twitter embeddings</td>
</tr>
<tr class="odd">
<td>‘en-turian’ (or ‘turian’)</td>
<td>English</td>
<td>Turian embeddings (small)</td>
</tr>
<tr class="even">
<td>‘en’ (or ‘en-news’ or ‘news’)</td>
<td>English</td>
<td>FastText embeddings over news and wikipedia data</td>
</tr>
<tr class="odd">
<td>‘de’</td>
<td>German</td>
<td>German FastText embeddings</td>
</tr>
<tr class="even">
<td>‘nl’</td>
<td>Dutch</td>
<td>Dutch FastText embeddings</td>
</tr>
<tr class="odd">
<td>‘fr’</td>
<td>French</td>
<td>French FastText embeddings</td>
</tr>
<tr class="even">
<td>‘it’</td>
<td>Italian</td>
<td>Italian FastText embeddings</td>
</tr>
<tr class="odd">
<td>‘es’</td>
<td>Spanish</td>
<td>Spanish FastText embeddings</td>
</tr>
<tr class="even">
<td>‘pt’</td>
<td>Portuguese</td>
<td>Portuguese FastText embeddings</td>
</tr>
<tr class="odd">
<td>‘ro’</td>
<td>Romanian</td>
<td>Romanian FastText embeddings</td>
</tr>
<tr class="even">
<td>‘ca’</td>
<td>Catalan</td>
<td>Catalan FastText embeddings</td>
</tr>
<tr class="odd">
<td>‘sv’</td>
<td>Swedish</td>
<td>Swedish FastText embeddings</td>
</tr>
<tr class="even">
<td>‘da’</td>
<td>Danish</td>
<td>Danish FastText embeddings</td>
</tr>
<tr class="odd">
<td>‘no’</td>
<td>Norwegian</td>
<td>Norwegian FastText embeddings</td>
</tr>
<tr class="even">
<td>‘fi’</td>
<td>Finnish</td>
<td>Finnish FastText embeddings</td>
</tr>
<tr class="odd">
<td>‘pl’</td>
<td>Polish</td>
<td>Polish FastText embeddings</td>
</tr>
<tr class="even">
<td>‘cz’</td>
<td>Czech</td>
<td>Czech FastText embeddings</td>
</tr>
<tr class="odd">
<td>‘sk’</td>
<td>Slovak</td>
<td>Slovak FastText embeddings</td>
</tr>
<tr class="even">
<td>‘sl’</td>
<td>Slovenian</td>
<td>Slovenian FastText embeddings</td>
</tr>
<tr class="odd">
<td>‘sr’</td>
<td>Serbian</td>
<td>Serbian FastText embeddings</td>
</tr>
<tr class="even">
<td>‘hr’</td>
<td>Croatian</td>
<td>Croatian FastText embeddings</td>
</tr>
<tr class="odd">
<td>‘bg’</td>
<td>Bulgarian</td>
<td>Bulgarian FastText embeddings</td>
</tr>
<tr class="even">
<td>‘ru’</td>
<td>Russian</td>
<td>Russian FastText embeddings</td>
</tr>
<tr class="odd">
<td>‘ar’</td>
<td>Arabic</td>
<td>Arabic FastText embeddings</td>
</tr>
<tr class="even">
<td>‘he’</td>
<td>Hebrew</td>
<td>Hebrew FastText embeddings</td>
</tr>
<tr class="odd">
<td>‘tr’</td>
<td>Turkish</td>
<td>Turkish FastText embeddings</td>
</tr>
<tr class="even">
<td>‘fa’</td>
<td>Persian</td>
<td>Persian FastText embeddings</td>
</tr>
<tr class="odd">
<td>‘ja’</td>
<td>Japanese</td>
<td>Japanese FastText embeddings</td>
</tr>
<tr class="even">
<td>‘ko’</td>
<td>Korean</td>
<td>Korean FastText embeddings</td>
</tr>
<tr class="odd">
<td>‘zh’</td>
<td>Chinese</td>
<td>Chinese FastText embeddings</td>
</tr>
<tr class="even">
<td>‘hi’</td>
<td>Hindi</td>
<td>Hindi FastText embeddings</td>
</tr>
<tr class="odd">
<td>‘id’</td>
<td>Indonesian</td>
<td>Indonesian FastText embeddings</td>
</tr>
<tr class="even">
<td>‘eu’</td>
<td>Basque</td>
<td>Basque FastText embeddings</td>
</tr>
</tbody>
</table>
<p>So, if you want to load German FastText embeddings, instantiate as
follows:</p>
<p>Alternatively, if you want to load German FastText embeddings trained
over crawls, instantiate as follows:</p>
</div>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



   </div>
  <footer><div class="container">
  <div class="pkgdown-footer-left">
  <p>Developed by <a href="https://davidycliao.github.io" class="external-link">Yen-Chieh Liao</a>, <a href="https://muellerstefan.net" class="external-link">Stefan Müller</a>, Akbik Alan, Blythe Duncan, Vollgraf Roland.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

  </div></footer>
</body>
</html>
