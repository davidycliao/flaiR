---
title: "Flair Embeddings"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Flair Embeddings}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r, include = FALSE}
library(reticulate)
library(flaiR)
reticulate::py_install("flair")
system(paste(reticulate::py_config()$python, "-m pip install flair"))
```


## Classic Wordembeddings

<div style="text-align: justify">

Flair is a very popular natural language processing library, providing a variety of embedding methods for text representation through Flair. In Flair, the simplest form of embedding that still contains semantic information about the word is called classic word embeddings. These embeddings are pre-trained and non-contextual. Let's retrieve a few word embeddings. Then, we can utilize FastText embeddings with the following code. To use them, we simply instantiate a WordEmbeddings class by passing in the ID of the embedding of our choice. Then, we simply wrap our text into a Sentence object, and call the embed(sentence) method on our WordEmbeddings class.


```{r}
embedding = flair_embeddings.WordEmbeddings('crawl') 
sentence = flair_data.sentence("one two three one") 
embedding$embed(sentence) 

for (i in seq_along(sentence$tokens)) {
  print(head(sentence$tokens[[i]]$embedding), n =5)
}

```

Flair supports a range of classic word embeddings, each offering unique features and application scopes. Below is an overview, detailing the ID required to load each embedding and its corresponding language.


| Embedding Type                 | ID     | Language   |
|--------------------------------|--------|------------|
| GloVe                          | glove  | English    |
| Komninos                       | extvec | English    |
| Twitter                        | twitter| English    |
| Turian (small)                 | turian | English    |
| FastText (crawl)               | crawl  | English    |
| FastText (news & Wikipedia)    | ar     | Arabic     |
| FastText (news & Wikipedia)    | bg     | Bulgarian  |
| FastText (news & Wikipedia)    | ca     | Catalan    |
| FastText (news & Wikipedia)    | cz     | Czech      |
| FastText (news & Wikipedia)    | da     | Danish     |
| FastText (news & Wikipedia)    | de     | German     |
| FastText (news & Wikipedia)    | es     | Spanish    |
| FastText (news & Wikipedia)    | en     | English    |
| FastText (news & Wikipedia)    | eu     | Basque     |
| FastText (news & Wikipedia)    | fa     | Persian    |
| FastText (news & Wikipedia)    | fi     | Finnish    |
| FastText (news & Wikipedia)    | fr     | French     |
| FastText (news & Wikipedia)    | he     | Hebrew     |
| FastText (news & Wikipedia)    | hi     | Hindi      |
| FastText (news & Wikipedia)    | hr     | Croatian   |
| FastText (news & Wikipedia)    | id     | Indonesian |
| FastText (news & Wikipedia)    | it     | Italian    |
| FastText (news & Wikipedia)    | ja     | Japanese   |
| FastText (news & Wikipedia)    | ko     | Korean     |
| FastText (news & Wikipedia)    | nl     | Dutch      |
| FastText (news & Wikipedia)    | no     | Norwegian  |
| FastText (news & Wikipedia)    | pl     | Polish     |
| FastText (news & Wikipedia)    | pt     | Portuguese |
| FastText (news & Wikipedia)    | ro     | Romanian   |
| FastText (news & Wikipedia)    | ru     | Russian    |
| FastText (news & Wikipedia)    | si     | Slovenian  |
| FastText (news & Wikipedia)    | sk     | Slovak     |
| FastText (news & Wikipedia)    | sr     | Serbian    |
| FastText (news & Wikipedia)    | sv     | Swedish    |
| FastText (news & Wikipedia)    | tr     | Turkish    |
| FastText (news & Wikipedia)    | zh     | Chinese    |

</div>

&nbsp;

--- 

## Contexual Embeddings

<div style="text-align: justify">

Understanding the contextuality of Flair embeddings The idea behind contextual string embeddings is that each word embedding should be defined by not only its syntactic-semantic meaning but also the context it appears in. What this means is that each word will have a different embedding for every context it appears in. Each pre-trained Flair model offers a **forward** version and a **backward** version. Let's assume you are processing a language that, just like this book, uses the left-to-right script. The forward version takes into account the context that happens before the word – on the left-hand side. The backward version works in the opposite direction. It takes into account the context after the word – on the right-hand side of the word. If this is true, then two same words that appear at the beginning of two different sentences should have identical forward embeddings, because their context is null. Let's test this out:


Because we are using a forward model, it only takes into account the context that occurs before a word. Additionally, since our word has no context on the left-hand side of its position in the sentence, the two embeddings are identical, and the code assumes they are identical, indeed output is __True__.


```{r}
embedding <- flair_embeddings.FlairEmbeddings('news-forward')
s1 <- flair_data.sentence("nice shirt") 
s2 <- flair_data.sentence("nice pants") 

embedding$embed(s1) 
embedding$embed(s2) 
cat(" s1 sentence:", paste(s1[0], sep = ""), "\n", "s2 sentence:", paste(s2[0], sep = ""))
```


We test whether the sum of the two 2048 embeddings of 'nice' is equal to 2048. If it is true, it indicates that the embedding results are consistent, which should theoretically be the case.

```{r}
length(s1[0]$embedding$numpy()) == sum(s1[0]$embedding$numpy() ==  s2[0]$embedding$numpy())
```


Now we separately add a few more words, `very` and `pretty`, into two sentence objects.

```{r}
embedding <- flair_embeddings.FlairEmbeddings('news-forward')
s1 <- flair_data.sentence("nice shirt") 
s2 <- flair_data.sentence("nice pants") 
```

```{r}
embedding <- flair_embeddings.FlairEmbeddings('news-forward')
s1 <- flair_data.sentence("very nice shirt") 
s2 <- flair_data.sentence("pretty nice pants") 

embedding$embed(s1) 
embedding$embed(s2) 
```

The two sets of embeddings are not identical because the words are different, so it returns __False__.

```{r}
length(s1[0]$embedding$numpy()) == sum(s1[0]$embedding$numpy() ==  s2[0]$embedding$numpy())
```

The measure of similarity between two vectors in an inner product space is known as cosine similarity. The formula for calculating cosine similarity between two vectors, such as vectors A and B, is as follows:

$Cosine Similarity = \frac{\sum_{i} (A_i \cdot B_i)}{\sqrt{\sum_{i} (A_i^2)} \cdot \sqrt{\sum_{i} (B_i^2)}}$


```{r}
library(lsa)
vector1 <- as.numeric(s1[0]$embedding$numpy())
vector2 <- as.numeric(s2[0]$embedding$numpy())
```


We can observe that the similarity between the two words is 0.55.
```{r}
cosine_similarity <- cosine(vector1, vector2)
print(cosine_similarity)
```


</div>

&nbsp;

-----

## Extracting Embeddings from BERT

<div style="text-align: justify">

First, we utilize the flair.embeddings.TransformerWordEmbeddings function to download BERT, and more transformer models can also be found on [Flair NLP's Hugging Face](https://huggingface.co/flair).

```{r}
TransformerWordEmbeddings <- flair_embeddings.TransformerWordEmbeddings("bert-base-uncased")
```


```{r}
embedding <- TransformerWordEmbeddings$embed(sentence)
```

Traverse each token in the sentence and print them. To view each token, it's necessary to use` reticulate::py_str(token)` since the sentence is a Python object.

```{r}
# Iterate through each token in the sentence, printing them. 
# Utilize reticulate::py_str(token) to view each token, given that the sentence is a Python object.
for (i in seq_along(sentence$tokens)) {
  cat("Token: ", reticulate::py_str(sentence$tokens[[i]]), "\n")
  # Access the embedding of the token, converting it to an R object, 
  # and print the first 10 elements of the vector.
  token_embedding <- sentence$tokens[[i]]$embedding
  print(head(token_embedding, 10))
}
```


</div>
