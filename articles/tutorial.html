<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Tutorial • flaiR</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/Source_Sans_Pro-0.4.9/font.css" rel="stylesheet">
<link href="../deps/Source_Code_Pro-0.4.9/font.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Tutorial">
<!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=G-3ZG40PPG98"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-3ZG40PPG98');
</script><script defer data-domain="{YOUR DOMAIN},all.tidyverse.org" src="https://plausible.io/js/plausible.js"></script>
</head>
<body>
    <a href="#container" class="visually-hidden-focusable">Skip to content</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-none" data-bs-theme="inverse" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">flaiR</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Released version">0.0.6</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../index.html"><span class="fa fa-home"></span></a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-quick-start" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true"><span class="fa fa-rocket"></span> Quick Start</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-quick-start">
<li><a class="dropdown-item" href="../articles/quickstart.html#nlp-tasks">NLP Tasks</a></li>
    <li><a class="dropdown-item" href="../articles/quickstart.html#class-and-ojbect">Class and Ojbect</a></li>
    <li><a class="dropdown-item" href="../articles/quickstart.html#more-details-about-installation">More Details about Installation</a></li>
  </ul>
</li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-tutorial" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true"><span class="fa fa-project-diagram"></span> Tutorial</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-tutorial">
<li><a class="dropdown-item" href="../articles/tutorial.html#introduction">Introduction</a></li>
    <li><a class="dropdown-item" href="../articles/tutorial.html#sentence-and-token">Sentence and Token</a></li>
    <li><a class="dropdown-item" href="../articles/tutorial.html#sequence-taggings">Sequence Taggings</a></li>
    <li><a class="dropdown-item" href="../articles/tutorial.html#performing-ner-tasks">Performing NER Tasks</a></li>
    <li><a class="dropdown-item" href="../articles/tutorial.html#embedding">Embeddings</a></li>
    <li><a class="dropdown-item" href="../articles/tutorial.html#training-a-binary-classifier">Training a Binary Classifier</a></li>
    <li><a class="dropdown-item" href="../articles/tutorial.html#training-rnns">Training RNNs</a></li>
    <li><a class="dropdown-item" href="../articles/tutorial.html#finetune-transformers">Finetune Transformers </a></li>
  </ul>
</li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-expanded-feats" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true"><span class="fa fa-newspaper-o"></span> Expanded Feats</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-expanded-feats">
<li><a class="dropdown-item" href="../articles/get_pos.html">Part-of-speech Tagging</a></li>
    <li><a class="dropdown-item" href="../articles/get_entities.html">Named Entity Recognition</a></li>
    <li><a class="dropdown-item" href="../articles/get_sentiments.html">Tagging Sentiment</a></li>
    <li><a class="dropdown-item" href="../articles/highlight_text.html">The Coloring Entities</a></li>
  </ul>
</li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-reference" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true"><span class="fa fa-file-code-o"></span> Reference</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-reference">
<li><a class="dropdown-item" href="../reference/index.html">All Function Reference</a></li>
  </ul>
</li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-news" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true"><span class="fa fa-newspaper-o"></span> News</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-news">
<li><a class="dropdown-item" href="../news/index.html#flair-006-2023-10-29">0.0.6</a></li>
    <li><a class="dropdown-item" href="../news/index.html#flair-005-2023-10-01">0.0.5</a></li>
    <li><a class="dropdown-item" href="../news/index.html#flair-003-2023-09-10">0.0.3</a></li>
    <li><a class="dropdown-item" href="../news/index.html#flair-001-development-version">0.0.1</a></li>
  </ul>
</li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/davidycliao/flaiR"><span class="fa fa-github fa-lg"></span> GitHub</a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article" id="container">



<link href="tutorial_files/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet">
<script src="tutorial_files/htmlwidgets-1.6.4/htmlwidgets.js"></script><script src="tutorial_files/plotly-binding-4.10.4/plotly.js"></script><script src="tutorial_files/typedarray-0.1/typedarray.min.js"></script><link href="tutorial_files/crosstalk-1.2.1/css/crosstalk.min.css" rel="stylesheet">
<script src="tutorial_files/crosstalk-1.2.1/js/crosstalk.min.js"></script><link href="tutorial_files/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet">
<script src="tutorial_files/plotly-main-2.11.1/plotly-latest.min.js"></script><div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>Tutorial</h1>
                        <h4 data-toc-skip class="author">Yen-Chieh
(David) Liao | Sohini Timbadia | Stefan Müller</h4>
            <address class="author_afil">
      University of Birmingham &amp; University College
Dublin<br><small class="dont-index">Source: <a href="https://github.com/davidycliao/flaiR/blob/HEAD/vignettes/tutorial.Rmd" class="external-link"><code>vignettes/tutorial.Rmd</code></a></small>
      <div class="d-none name"><code>tutorial.Rmd</code></div>
    </address>
</div>

    
    
<div class="section level2">
<h2 id="the-overview">The Overview<a class="anchor" aria-label="anchor" href="#the-overview"></a>
</h2>
<div style="text-align: justify;">
<p><strong>Flair NLP</strong> is an open-source library for Natural
Language Processing (NLP) developed by <a href="https://github.com/zalandoresearch/" class="external-link">Zalando Research</a>. Known
for its state-of-the-art solutions, such as contextual string embeddings
for NLP tasks like Named Entity Recognition (NER), Part-of-Speech
tagging (POS), and more, it has garnered the attention of the NLP
community for its ease of use and powerful functionalities.</p>
<p>In addition, Flair NLP offers pre-trained models for various
languages and tasks, and is compatible with fine-tuned transformers
hosted on Hugging Face.</p>
<ul>
<li><p><a href="#tutorial.html#sentence-and-token"><strong>Sentence and
Token Object</strong></a></p></li>
<li><p><a href="#tutorial.html#sequence-taggings"><strong>Sequence
Taggings</strong></a></p></li>
<li><p><a href="#tutorial.html#flair-embedding"><strong>Embedding in
flaiR</strong></a></p></li>
<li><p><a href="#tutorial.html#performing-ner-tasks"><strong>Performing
NER Tasks</strong></a></p></li>
<li><p><a href="#tutorial.html#training-a-binary-classifier-in-flair"><strong>Training
a Binary Classifier in flaiR</strong></a></p></li>
<li><p><a href="#tutorial.html#training-rnns"><strong>Training
RNNa</strong></a></p></li>
<li><p><a href="#tutorial.html#finetune-transformers"><strong>Finetune
BERT</strong></a></p></li>
</ul>
</div>
<hr>
</div>
<div class="section level2">
<h2 id="sentence-and-token">Sentence and Token<a class="anchor" aria-label="anchor" href="#sentence-and-token"></a>
</h2>
<p>Sentence and Token are fundamental classes.</p>
<div class="section level3">
<h3 id="sentence">
<strong>Sentence</strong><a class="anchor" aria-label="anchor" href="#sentence"></a>
</h3>
<div style="text-align: justify;">
<p>A Sentence in Flair is an object that contains a sequence of Token
objects, and it can be annotated with labels, such as named entities,
part-of-speech tags, and more. It also can store embeddings for the
sentence as a whole and different kinds of linguistic annotations.</p>
<p>Here’s a simple example of how you create a Sentence:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Creating a Sentence object</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://davidycliao.github.io/flaiR">flaiR</a></span><span class="op">)</span></span>
<span><span class="va">string</span> <span class="op">&lt;-</span> <span class="st">"What I see in UCD today, what I have seen of UCD in its impact on my own life and the life of Ireland."</span></span>
<span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span>
<span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="va">string</span><span class="op">)</span></span></code></pre></div>
<p><code>Sentence[26]</code> means that there are a total of 26 tokens
in the sentence.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="co">#&gt; Sentence[26]: "What I see in UCD today, what I have seen of UCD in its impact on my own life and the life of Ireland."</span></span></code></pre></div>
</div>
</div>
<div class="section level3">
<h3 id="token">
<strong>Token</strong><a class="anchor" aria-label="anchor" href="#token"></a>
</h3>
<div style="text-align: justify;">
<p>When you use Flair to handle text data,<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content="&lt;p&gt;Flair is built on PyTorch, which is a library in
Python.&lt;/p&gt;"><sup>1</sup></a> <code>Sentence</code>
and <code>Token</code> objects often play central roles in many use
cases. When you create a Sentence object, it automatically tokenizes the
text, removing the need to create the Token object manually.</p>
<p>Unlike R, which indexes from 1, Python indexes from 0. Therefore,
when using a for loop, I use <code>seq_along(sentence) - 1</code>. The
output should be something like:</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># The Sentence object has automatically created and contains multiple Token objects</span></span>
<span><span class="co"># We can iterate through the Sentence object to view each Token</span></span>
<span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq_along</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="co">#&gt; Token[0]: "What"</span></span>
<span><span class="co">#&gt; Token[1]: "I"</span></span>
<span><span class="co">#&gt; Token[2]: "see"</span></span>
<span><span class="co">#&gt; Token[3]: "in"</span></span>
<span><span class="co">#&gt; Token[4]: "UCD"</span></span>
<span><span class="co">#&gt; Token[5]: "today"</span></span>
<span><span class="co">#&gt; Token[6]: ","</span></span>
<span><span class="co">#&gt; Token[7]: "what"</span></span>
<span><span class="co">#&gt; Token[8]: "I"</span></span>
<span><span class="co">#&gt; Token[9]: "have"</span></span>
<span><span class="co">#&gt; Token[10]: "seen"</span></span>
<span><span class="co">#&gt; Token[11]: "of"</span></span>
<span><span class="co">#&gt; Token[12]: "UCD"</span></span>
<span><span class="co">#&gt; Token[13]: "in"</span></span>
<span><span class="co">#&gt; Token[14]: "its"</span></span>
<span><span class="co">#&gt; Token[15]: "impact"</span></span>
<span><span class="co">#&gt; Token[16]: "on"</span></span>
<span><span class="co">#&gt; Token[17]: "my"</span></span>
<span><span class="co">#&gt; Token[18]: "own"</span></span>
<span><span class="co">#&gt; Token[19]: "life"</span></span>
<span><span class="co">#&gt; Token[20]: "and"</span></span>
<span><span class="co">#&gt; Token[21]: "the"</span></span>
<span><span class="co">#&gt; Token[22]: "life"</span></span>
<span><span class="co">#&gt; Token[23]: "of"</span></span>
<span><span class="co">#&gt; Token[24]: "Ireland"</span></span>
<span><span class="co">#&gt; Token[25]: "."</span></span></code></pre></div>
<p>Or you can directly use <code>$tokens</code> method to print all
tokens.</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="va">tokens</span><span class="op">)</span></span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Token[0]: "What"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[2]]</span></span>
<span><span class="co">#&gt; Token[1]: "I"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[3]]</span></span>
<span><span class="co">#&gt; Token[2]: "see"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[4]]</span></span>
<span><span class="co">#&gt; Token[3]: "in"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[5]]</span></span>
<span><span class="co">#&gt; Token[4]: "UCD"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[6]]</span></span>
<span><span class="co">#&gt; Token[5]: "today"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[7]]</span></span>
<span><span class="co">#&gt; Token[6]: ","</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[8]]</span></span>
<span><span class="co">#&gt; Token[7]: "what"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[9]]</span></span>
<span><span class="co">#&gt; Token[8]: "I"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[10]]</span></span>
<span><span class="co">#&gt; Token[9]: "have"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[11]]</span></span>
<span><span class="co">#&gt; Token[10]: "seen"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[12]]</span></span>
<span><span class="co">#&gt; Token[11]: "of"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[13]]</span></span>
<span><span class="co">#&gt; Token[12]: "UCD"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[14]]</span></span>
<span><span class="co">#&gt; Token[13]: "in"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[15]]</span></span>
<span><span class="co">#&gt; Token[14]: "its"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[16]]</span></span>
<span><span class="co">#&gt; Token[15]: "impact"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[17]]</span></span>
<span><span class="co">#&gt; Token[16]: "on"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[18]]</span></span>
<span><span class="co">#&gt; Token[17]: "my"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[19]]</span></span>
<span><span class="co">#&gt; Token[18]: "own"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[20]]</span></span>
<span><span class="co">#&gt; Token[19]: "life"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[21]]</span></span>
<span><span class="co">#&gt; Token[20]: "and"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[22]]</span></span>
<span><span class="co">#&gt; Token[21]: "the"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[23]]</span></span>
<span><span class="co">#&gt; Token[22]: "life"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[24]]</span></span>
<span><span class="co">#&gt; Token[23]: "of"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[25]]</span></span>
<span><span class="co">#&gt; Token[24]: "Ireland"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[26]]</span></span>
<span><span class="co">#&gt; Token[25]: "."</span></span></code></pre></div>
<p><strong>Retrieve the Token</strong></p>
<p>To comprehend the string representation format of the Sentence
object, tagging at least one token is adequate. Python’s
<code>get_token(n)</code> method allows us to retrieve the Token object
for a particular token. Additionally, we can use
<strong><code>[]</code></strong> to index a specific token.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># method in Python</span></span>
<span><span class="va">sentence</span><span class="op">$</span><span class="fu">get_token</span><span class="op">(</span><span class="fl">5</span><span class="op">)</span></span>
<span><span class="co">#&gt; Token[4]: "UCD"</span></span></code></pre></div>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># indexing in R </span></span>
<span><span class="va">sentence</span><span class="op">[</span><span class="fl">6</span><span class="op">]</span></span>
<span><span class="co">#&gt; Token[6]: ","</span></span></code></pre></div>
<p>Each word (and punctuation) in the text is treated as an individual
Token object. These Token objects store text information and other
possible linguistic information (such as part-of-speech tags or named
entity tags) and embeddings (if you used a model to generate them).</p>
<p>While you do not need to create Token objects manually, understanding
how to manage them is useful in situations where you might want to
fine-tune the tokenization process. For example, you can control the
exactness of tokenization by manually creating Token objects from a
Sentence object.</p>
<p>This makes Flair very flexible when handling text data since the
automatic tokenization feature can be used for rapid development, while
also allowing users to fine-tune their tokenization.</p>
<p><strong>Annotate POS tag and NER tag</strong></p>
<p>The <code>add_label(label_type, value)</code> method can be employed
to assign a label to the token. In Universal POS tags, if
<code>sentence[10]</code> is ‘see’, ‘seen’ might be tagged as
<code>VERB</code>, indicating it is a past participle form of a
verb.</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">sentence</span><span class="op">[</span><span class="fl">10</span><span class="op">]</span><span class="op">$</span><span class="fu">add_label</span><span class="op">(</span><span class="st">'manual-pos'</span>, <span class="st">'VERB'</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">[</span><span class="fl">10</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="co">#&gt; Token[10]: "seen" → VERB (1.0000)</span></span></code></pre></div>
<p>We can also add a NER (Named Entity Recognition) tag to
<code>sentence[4]</code>, “UCD”, identifying it as a university in
Dublin.</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">sentence</span><span class="op">[</span><span class="fl">4</span><span class="op">]</span><span class="op">$</span><span class="fu">add_label</span><span class="op">(</span><span class="st">'ner'</span>, <span class="st">'ORG'</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">[</span><span class="fl">4</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="co">#&gt; Token[4]: "UCD" → ORG (1.0000)</span></span></code></pre></div>
<p>If we print the sentence object, <code>Sentence[50]</code> provides
information for 50 tokens → [‘in’/ORG, ‘seen’/VERB], thus displaying two
tagging pieces of information.</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="co">#&gt; Sentence[26]: "What I see in UCD today, what I have seen of UCD in its impact on my own life and the life of Ireland." → ["UCD"/ORG, "seen"/VERB]</span></span></code></pre></div>
</div>
</div>
<div class="section level3">
<h3 id="corpus">
<strong>Corpus</strong><a class="anchor" aria-label="anchor" href="#corpus"></a>
</h3>
<div style="text-align: justify;">
<p>The Corpus object in Flair is a fundamental data structure that
represents a dataset containing text samples, usually comprising of a
training set, a development set (or validation set), and a test set.
It’s designed to work smoothly with Flair’s models for tasks like named
entity recognition, text classification, and more.</p>
<p><strong>Attributes:</strong></p>
<ul>
<li>
<code>train</code>: A list of sentences (List[Sentence]) that form
the training dataset.</li>
<li>
<code>dev</code> (or development): A list of sentences
(List[Sentence]) that form the development (or validation) dataset.</li>
<li>
<code>test</code>: A list of sentences (List[Sentence]) that form
the test dataset.</li>
</ul>
<p><strong>Important Methods:</strong></p>
<ul>
<li>
<code>downsample</code>: This method allows you to downsample
(reduce) the number of sentences in the train, dev, and test
splits.</li>
<li>
<code>obtain_statistics</code>: This method gives a quick overview
of the statistics of the corpus, including the number of sentences and
the distribution of labels.</li>
<li>
<code>make_vocab_dictionary</code>: Used to create a vocabulary
dictionary from the corpus.</li>
</ul>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://davidycliao.github.io/flaiR">flaiR</a></span><span class="op">)</span></span>
<span><span class="va">Corpus</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Corpus</span></span>
<span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span></code></pre></div>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Create some example sentences</span></span>
<span><span class="va">train</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="fu">Sentence</span><span class="op">(</span><span class="st">'This is a training example.'</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">dev</span> <span class="op">&lt;-</span>  <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="fu">Sentence</span><span class="op">(</span><span class="st">'This is a validation example.'</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">test</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="fu">Sentence</span><span class="op">(</span><span class="st">'This is a test example.'</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Create a corpus using the custom data splits</span></span>
<span><span class="va">corp</span> <span class="op">&lt;-</span>  <span class="fu">Corpus</span><span class="op">(</span>train <span class="op">=</span> <span class="va">train</span>, dev <span class="op">=</span> <span class="va">dev</span>, test <span class="op">=</span> <span class="va">test</span><span class="op">)</span></span></code></pre></div>
<p><code>$obtain_statistics()</code> method of the Corpus object in the
Flair library provides an overview of the dataset statistics. The method
returns a <a href="https://www.w3schools.com/python/python_dictionaries.asp" class="external-link">Python
dictionary</a> with details about the training, validation
(development), and test datasets that make up the corpus. In R, you can
use the jsonlite package to format JSON.</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://jeroen.r-universe.dev/jsonlite" class="external-link">jsonlite</a></span><span class="op">)</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/jsonlite/man/fromJSON.html" class="external-link">fromJSON</a></span><span class="op">(</span><span class="va">corp</span><span class="op">$</span><span class="fu">obtain_statistics</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">formatted_str</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/jsonlite/man/fromJSON.html" class="external-link">toJSON</a></span><span class="op">(</span><span class="va">data</span>, pretty<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">formatted_str</span><span class="op">)</span></span>
<span><span class="co">#&gt; {</span></span>
<span><span class="co">#&gt;   "TRAIN": {</span></span>
<span><span class="co">#&gt;     "dataset": ["TRAIN"],</span></span>
<span><span class="co">#&gt;     "total_number_of_documents": [1],</span></span>
<span><span class="co">#&gt;     "number_of_documents_per_class": {},</span></span>
<span><span class="co">#&gt;     "number_of_tokens_per_tag": {},</span></span>
<span><span class="co">#&gt;     "number_of_tokens": {</span></span>
<span><span class="co">#&gt;       "total": [6],</span></span>
<span><span class="co">#&gt;       "min": [6],</span></span>
<span><span class="co">#&gt;       "max": [6],</span></span>
<span><span class="co">#&gt;       "avg": [6]</span></span>
<span><span class="co">#&gt;     }</span></span>
<span><span class="co">#&gt;   },</span></span>
<span><span class="co">#&gt;   "TEST": {</span></span>
<span><span class="co">#&gt;     "dataset": ["TEST"],</span></span>
<span><span class="co">#&gt;     "total_number_of_documents": [1],</span></span>
<span><span class="co">#&gt;     "number_of_documents_per_class": {},</span></span>
<span><span class="co">#&gt;     "number_of_tokens_per_tag": {},</span></span>
<span><span class="co">#&gt;     "number_of_tokens": {</span></span>
<span><span class="co">#&gt;       "total": [6],</span></span>
<span><span class="co">#&gt;       "min": [6],</span></span>
<span><span class="co">#&gt;       "max": [6],</span></span>
<span><span class="co">#&gt;       "avg": [6]</span></span>
<span><span class="co">#&gt;     }</span></span>
<span><span class="co">#&gt;   },</span></span>
<span><span class="co">#&gt;   "DEV": {</span></span>
<span><span class="co">#&gt;     "dataset": ["DEV"],</span></span>
<span><span class="co">#&gt;     "total_number_of_documents": [1],</span></span>
<span><span class="co">#&gt;     "number_of_documents_per_class": {},</span></span>
<span><span class="co">#&gt;     "number_of_tokens_per_tag": {},</span></span>
<span><span class="co">#&gt;     "number_of_tokens": {</span></span>
<span><span class="co">#&gt;       "total": [6],</span></span>
<span><span class="co">#&gt;       "min": [6],</span></span>
<span><span class="co">#&gt;       "max": [6],</span></span>
<span><span class="co">#&gt;       "avg": [6]</span></span>
<span><span class="co">#&gt;     }</span></span>
<span><span class="co">#&gt;   }</span></span>
<span><span class="co">#&gt; }</span></span></code></pre></div>
<p><strong>In R</strong></p>
<p>Below, we use data from the article <a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;ved=2ahUKEwjz_bS3p5KCAxWEWEEAHcuVAi4QFnoECA8QAQ&amp;url=https%3A%2F%2Fwww.journals.uchicago.edu%2Fdoi%2Ffull%2F10.1086%2F715165&amp;usg=AOvVaw3f_J3sXTrym2ZR64pF3ZtN&amp;opi=89978449" class="external-link"><em>The
Temporal Focus of Campaign Communication</em></a> by <a href="https://muellerstefan.net" class="external-link">Stefan Muller</a>, published in the
<em>Journal of Politics</em> in 2020, as an example.</p>
<p>First, we vectorize the <code>cc_muller$text</code> using the
Sentence function to transform it into a list object. Then, we reformat
<code>cc_muller$class_pro_retro</code> as a factor. It’s essential to
note that R handles numerical values differently than Python. In R,
numerical values are represented with a floating point, so it’s
advisable to convert them into factors or strings. Lastly, we employ the
map function from the purrr package to assign labels to each sentence
corpus using the <code>$add_label</code> method.</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://purrr.tidyverse.org/" class="external-link">purrr</a></span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Attaching package: 'purrr'</span></span>
<span><span class="co">#&gt; The following object is masked from 'package:jsonlite':</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     flatten</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="va">cc_muller</span><span class="op">)</span></span>
<span><span class="co"># The `Sentence` object tokenizes text </span></span>
<span><span class="va">text</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html" class="external-link">lapply</a></span><span class="op">(</span> <span class="va">cc_muller</span><span class="op">$</span><span class="va">text</span>, <span class="va">Sentence</span><span class="op">)</span></span>
<span><span class="co"># split sentence object to train and test. </span></span>
<span><span class="va">labels</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html" class="external-link">as.factor</a></span><span class="op">(</span><span class="va">cc_muller</span><span class="op">$</span><span class="va">class_pro_retro</span><span class="op">)</span></span>
<span><span class="co"># `$add_label` method assigns the corresponding coded type to each Sentence corpus.</span></span>
<span><span class="va">text</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://purrr.tidyverse.org/reference/map2.html" class="external-link">map2</a></span><span class="op">(</span><span class="va">text</span>, <span class="va">labels</span>, <span class="op">~</span> <span class="va">.x</span><span class="op">$</span><span class="fu">add_label</span><span class="op">(</span><span class="st">"classification"</span>, <span class="va">.y</span><span class="op">)</span>, .progress <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<p>To perform a train-test split using base R, we can follow these
steps:</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">2046</span><span class="op">)</span></span>
<span><span class="va">sample</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html" class="external-link">sample</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="cn">TRUE</span>, <span class="cn">FALSE</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">text</span><span class="op">)</span>, replace<span class="op">=</span><span class="cn">TRUE</span>, prob<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0.8</span>, <span class="fl">0.2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">train</span>  <span class="op">&lt;-</span> <span class="va">text</span><span class="op">[</span><span class="va">sample</span><span class="op">]</span></span>
<span><span class="va">test</span>   <span class="op">&lt;-</span> <span class="va">text</span><span class="op">[</span><span class="op">!</span><span class="va">sample</span><span class="op">]</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html" class="external-link">sprintf</a></span><span class="op">(</span><span class="st">"Corpus object sizes - Train: %d |  Test: %d"</span>, <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">train</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">test</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "Corpus object sizes - Train: 4710 |  Test: 1148"</span></span></code></pre></div>
<p>If you don’t provide a dev set, flaiR will not force you to carve out
a portion of your test set to serve as a dev set. However, in some cases
when only the train and test sets are provided without a dev set, flaiR
might automatically take a fraction of the train set (e.g., 10%) to use
as a dev set (<a href="https://github.com/flairNLP/flair/issues/2259#issuecomment-830040253" class="external-link">#2259</a>).
This is to offer a mechanism for model selection and to prevent the
model from overfitting on the train set.</p>
<p>In the “Corpus” function, there is a random selection of the “dev”
dataset. To ensure reproducibility, we need to set the seed in the flaiR
framework. We can accomplish this by calling the top-level module
“flair” from {flaiR} and using <code>$set_seed(1964L)</code> to set the
seed.</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">flair</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/import_flair.html">import_flair</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">flair</span><span class="op">$</span><span class="fu">set_seed</span><span class="op">(</span><span class="fl">1964L</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">corp</span> <span class="op">&lt;-</span> <span class="fu">Corpus</span><span class="op">(</span>train<span class="op">=</span><span class="va">train</span>, </span>
<span>                 <span class="co"># dev=test,</span></span>
<span>                 test<span class="op">=</span><span class="va">test</span><span class="op">)</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:19,587 No dev split found. Using 10% (i.e. 471 samples) of the train split as dev data</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html" class="external-link">sprintf</a></span><span class="op">(</span><span class="st">"Corpus object sizes - Train: %d | Test: %d | Dev: %d"</span>, </span>
<span>        <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">corp</span><span class="op">$</span><span class="va">train</span><span class="op">)</span>, </span>
<span>        <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">corp</span><span class="op">$</span><span class="va">test</span><span class="op">)</span>,</span>
<span>        <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">corp</span><span class="op">$</span><span class="va">dev</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "Corpus object sizes - Train: 4239 | Test: 1148 | Dev: 471"</span></span></code></pre></div>
<p>In the later sections, there will be more similar processing using
the <code>Corpus</code>. Following that, we will focus on advanced NLP
applications.</p>
</div>
<hr>
</div>
</div>
<div class="section level2">
<h2 id="sequence-taggings">Sequence Taggings<a class="anchor" aria-label="anchor" href="#sequence-taggings"></a>
</h2>
<div class="section level3">
<h3 id="tag-entities-in-text">
<strong>Tag Entities in Text</strong><a class="anchor" aria-label="anchor" href="#tag-entities-in-text"></a>
</h3>
<div style="text-align: justify;">
<p>Let’s run named entity recognition over the following example
sentence: “I love Berlin and New York”. To do this, all you need to do
is make a Sentence object for this text, load a pre-trained model and
use it to predict tags for the object.</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># attach flaiR in R</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://davidycliao.github.io/flaiR">flaiR</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># make a sentence</span></span>
<span></span>
<span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span>
<span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">'I love Berlin and New York.'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># load the NER tagger</span></span>
<span><span class="va">Classifier</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_nn.html">flair_nn</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Classifier</span></span>
<span><span class="va">tagger</span> <span class="op">&lt;-</span> <span class="va">Classifier</span><span class="op">$</span><span class="fu">load</span><span class="op">(</span><span class="st">'ner'</span><span class="op">)</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:20,817 SequenceTagger predicts: Dictionary with 20 tags: &lt;unk&gt;, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, &lt;START&gt;, &lt;STOP&gt;</span></span>
<span></span>
<span><span class="co"># run NER over sentence</span></span>
<span><span class="va">tagger</span><span class="op">$</span><span class="fu">predict</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span></code></pre></div>
<p>To print all annotations:</p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># print the sentence with all annotations</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="co">#&gt; Sentence[7]: "I love Berlin and New York." → ["Berlin"/LOC, "New York"/LOC]</span></span></code></pre></div>
<p>Use a for loop to print out each POS tag. It’s important to note that
Python is indexed from 0. Therefore, in an R environment, we must use
<code>seq_along(sentence$get_labels()) - 1</code>.</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq_along</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="fu">get_labels</span><span class="op">(</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="fu">get_labels</span><span class="op">(</span><span class="op">)</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span><span class="co">#&gt; 'Span[2:3]: "Berlin"'/'LOC' (0.9812)</span></span>
<span><span class="co">#&gt; 'Span[4:6]: "New York"'/'LOC' (0.9957)</span></span></code></pre></div>
</div>
</div>
<div class="section level3">
<h3 id="tag-part-of-speech-in-text">
<strong>Tag Part-of-Speech in Text</strong><a class="anchor" aria-label="anchor" href="#tag-part-of-speech-in-text"></a>
</h3>
<div style="text-align: justify;">
<p>We use flaiR/POS-english for POS tagging in the standard models on
Hugging Face.</p>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># attach flaiR in R</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://davidycliao.github.io/flaiR">flaiR</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># make a sentence</span></span>
<span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span>
<span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">'I love Berlin and New York.'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># load the NER tagger</span></span>
<span><span class="va">Classifier</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_nn.html">flair_nn</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Classifier</span></span>
<span><span class="va">tagger</span> <span class="op">&lt;-</span> <span class="va">Classifier</span><span class="op">$</span><span class="fu">load</span><span class="op">(</span><span class="st">'pos'</span><span class="op">)</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:21,800 SequenceTagger predicts: Dictionary with 53 tags: &lt;unk&gt;, O, UH, ,, VBD, PRP, VB, PRP$, NN, RB, ., DT, JJ, VBP, VBG, IN, CD, NNS, NNP, WRB, VBZ, WDT, CC, TO, MD, VBN, WP, :, RP, EX, JJR, FW, XX, HYPH, POS, RBR, JJS, PDT, NNPS, RBS, AFX, WP$, -LRB-, -RRB-, ``, '', LS, $, SYM, ADD</span></span>
<span></span>
<span><span class="co"># run NER over sentence</span></span>
<span><span class="va">tagger</span><span class="op">$</span><span class="fu">predict</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span></code></pre></div>
<p>To print all annotations:</p>
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># print the sentence with all annotations</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="co">#&gt; Sentence[7]: "I love Berlin and New York." → ["I"/PRP, "love"/VBP, "Berlin"/NNP, "and"/CC, "New"/NNP, "York"/NNP, "."/.]</span></span></code></pre></div>
<p>Use a for loop to print out each POS tag.</p>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq_along</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="fu">get_labels</span><span class="op">(</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="fu">get_labels</span><span class="op">(</span><span class="op">)</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span><span class="co">#&gt; 'Token[0]: "I"'/'PRP' (1.0)</span></span>
<span><span class="co">#&gt; 'Token[1]: "love"'/'VBP' (1.0)</span></span>
<span><span class="co">#&gt; 'Token[2]: "Berlin"'/'NNP' (0.9999)</span></span>
<span><span class="co">#&gt; 'Token[3]: "and"'/'CC' (1.0)</span></span>
<span><span class="co">#&gt; 'Token[4]: "New"'/'NNP' (1.0)</span></span>
<span><span class="co">#&gt; 'Token[5]: "York"'/'NNP' (1.0)</span></span>
<span><span class="co">#&gt; 'Token[6]: "."'/'.' (1.0)</span></span></code></pre></div>
</div>
</div>
<div class="section level3">
<h3 id="detect-sentiment">
<strong>Detect Sentiment</strong><a class="anchor" aria-label="anchor" href="#detect-sentiment"></a>
</h3>
<div style="text-align: justify;">
<p>Let’s run sentiment analysis over the same sentence to determine
whether it is POSITIVE or NEGATIVE.</p>
<p>You can do this with essentially the same code as above. Instead of
loading the ‘ner’ model, you now load the ‘sentiment’ model:</p>
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># attach flaiR in R</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://davidycliao.github.io/flaiR">flaiR</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># make a sentence</span></span>
<span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span>
<span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">'I love Berlin and New York.'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># load the Classifier tagger from flair.nn module</span></span>
<span><span class="va">Classifier</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_nn.html">flair_nn</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Classifier</span></span>
<span><span class="va">tagger</span> <span class="op">&lt;-</span> <span class="va">Classifier</span><span class="op">$</span><span class="fu">load</span><span class="op">(</span><span class="st">'sentiment'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># run sentiment analysis over sentence</span></span>
<span><span class="va">tagger</span><span class="op">$</span><span class="fu">predict</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># print the sentence with all annotations</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="co">#&gt; Sentence[7]: "I love Berlin and New York." → POSITIVE (0.9982)</span></span></code></pre></div>
</div>
<hr>
<div class="section level4">
<h4 id="tagging-parts-of-speech-with-flair-models">
<strong>Tagging Parts-of-Speech with Flair Models</strong><a class="anchor" aria-label="anchor" href="#tagging-parts-of-speech-with-flair-models"></a>
</h4>
<div style="text-align: justify;">
<p>You can load the pre-trained model <code>"pos-fast"</code>. For more
pre-trained models, see <a href="https://flairnlp.github.io/docs/tutorial-basics/part-of-speech-tagging#-in-english" class="external-link uri">https://flairnlp.github.io/docs/tutorial-basics/part-of-speech-tagging#-in-english</a>.</p>
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">texts</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"UCD is one of the best universities in Ireland."</span>,</span>
<span>           <span class="st">"UCD has a good campus but is very far from my apartment in Dublin."</span>,</span>
<span>           <span class="st">"Essex is famous for social science research."</span>,</span>
<span>           <span class="st">"Essex is not in the Russell Group, but it is famous for political science research and in 1994 Group."</span>,</span>
<span>           <span class="st">"TCD is the oldest university in Ireland."</span>,</span>
<span>           <span class="st">"TCD is similar to Oxford."</span><span class="op">)</span></span>
<span></span>
<span><span class="va">doc_ids</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"doc1"</span>, <span class="st">"doc2"</span>, <span class="st">"doc3"</span>, <span class="st">"doc4"</span>, <span class="st">"doc5"</span>, <span class="st">"doc6"</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://davidycliao.github.io/flaiR">flaiR</a></span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb29"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">tagger_pos</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/load_tagger_pos.html">load_tagger_pos</a></span><span class="op">(</span><span class="st">"pos-fast"</span><span class="op">)</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:25,236 SequenceTagger predicts: Dictionary with 53 tags: &lt;unk&gt;, O, UH, ,, VBD, PRP, VB, PRP$, NN, RB, ., DT, JJ, VBP, VBG, IN, CD, NNS, NNP, WRB, VBZ, WDT, CC, TO, MD, VBN, WP, :, RP, EX, JJR, FW, XX, HYPH, POS, RBR, JJS, PDT, NNPS, RBS, AFX, WP$, -LRB-, -RRB-, ``, '', LS, $, SYM, ADD</span></span></code></pre></div>
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">results</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/get_pos.html">get_pos</a></span><span class="op">(</span><span class="va">texts</span>, <span class="va">doc_ids</span>, <span class="va">tagger_pos</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">results</span>, n <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span>
<span><span class="co">#&gt;     doc_id token_id text_id        token    tag precision</span></span>
<span><span class="co">#&gt;     &lt;char&gt;    &lt;num&gt;  &lt;lgcl&gt;       &lt;char&gt; &lt;char&gt;     &lt;num&gt;</span></span>
<span><span class="co">#&gt;  1:   doc1        0      NA          UCD    NNP    0.9967</span></span>
<span><span class="co">#&gt;  2:   doc1        1      NA           is    VBZ    1.0000</span></span>
<span><span class="co">#&gt;  3:   doc1        2      NA          one     CD    0.9993</span></span>
<span><span class="co">#&gt;  4:   doc1        3      NA           of     IN    1.0000</span></span>
<span><span class="co">#&gt;  5:   doc1        4      NA          the     DT    1.0000</span></span>
<span><span class="co">#&gt;  6:   doc1        5      NA         best    JJS    0.9988</span></span>
<span><span class="co">#&gt;  7:   doc1        6      NA universities    NNS    0.9997</span></span>
<span><span class="co">#&gt;  8:   doc1        7      NA           in     IN    1.0000</span></span>
<span><span class="co">#&gt;  9:   doc1        8      NA      Ireland    NNP    1.0000</span></span>
<span><span class="co">#&gt; 10:   doc1        9      NA            .      .    0.9998</span></span></code></pre></div>
</div>
</div>
<div class="section level4">
<h4 id="tagging-entities-with-flair-models">
<strong>Tagging Entities with Flair Models</strong><a class="anchor" aria-label="anchor" href="#tagging-entities-with-flair-models"></a>
</h4>
<div style="text-align: justify;">
<p>Load the pre-trained model <code>ner</code>. For more pre-trained
models, see <a href="https://flairnlp.github.io/docs/tutorial-basics/tagging-entities" class="external-link uri">https://flairnlp.github.io/docs/tutorial-basics/tagging-entities</a>.</p>
<div class="sourceCode" id="cb31"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://davidycliao.github.io/flaiR">flaiR</a></span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb32"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">tagger_ner</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/load_tagger_ner.html">load_tagger_ner</a></span><span class="op">(</span><span class="st">"ner"</span><span class="op">)</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:26,155 SequenceTagger predicts: Dictionary with 20 tags: &lt;unk&gt;, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, &lt;START&gt;, &lt;STOP&gt;</span></span></code></pre></div>
<div class="sourceCode" id="cb33"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">results</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/get_entities.html">get_entities</a></span><span class="op">(</span><span class="va">texts</span>, <span class="va">doc_ids</span>, <span class="va">tagger_ner</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">results</span>, n <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span>
<span><span class="co">#&gt;     doc_id        entity    tag</span></span>
<span><span class="co">#&gt;     &lt;char&gt;        &lt;char&gt; &lt;char&gt;</span></span>
<span><span class="co">#&gt;  1:   doc1           UCD    ORG</span></span>
<span><span class="co">#&gt;  2:   doc1       Ireland    LOC</span></span>
<span><span class="co">#&gt;  3:   doc2           UCD    ORG</span></span>
<span><span class="co">#&gt;  4:   doc2        Dublin    LOC</span></span>
<span><span class="co">#&gt;  5:   doc3         Essex    ORG</span></span>
<span><span class="co">#&gt;  6:   doc4         Essex    ORG</span></span>
<span><span class="co">#&gt;  7:   doc4 Russell Group    ORG</span></span>
<span><span class="co">#&gt;  8:   doc5           TCD    ORG</span></span>
<span><span class="co">#&gt;  9:   doc5       Ireland    LOC</span></span>
<span><span class="co">#&gt; 10:   doc6           TCD    ORG</span></span></code></pre></div>
</div>
</div>
<div class="section level4">
<h4 id="tagging-sentiment">
<strong>Tagging Sentiment</strong><a class="anchor" aria-label="anchor" href="#tagging-sentiment"></a>
</h4>
<div style="text-align: justify;">
<p>Load the pre-trained model “<code>sentiment</code>”. The pre-trained
models of “<code>sentiment</code>”, “<code>sentiment-fast</code>”, and
“<code>de-offensive-language</code>” are currently available. For more
pre-trained models, see <a href="https://flairnlp.github.io/docs/tutorial-basics/tagging-sentiment" class="external-link uri">https://flairnlp.github.io/docs/tutorial-basics/tagging-sentiment</a>.</p>
<div class="sourceCode" id="cb34"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://davidycliao.github.io/flaiR">flaiR</a></span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb35"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">tagger_sent</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/load_tagger_sentiments.html">load_tagger_sentiments</a></span><span class="op">(</span><span class="st">"sentiment"</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb36"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">results</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/get_sentiments.html">get_sentiments</a></span><span class="op">(</span><span class="va">texts</span>, <span class="va">doc_ids</span>, <span class="va">tagger_sent</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">results</span>, n <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span>
<span><span class="co">#&gt;    doc_id sentiment     score</span></span>
<span><span class="co">#&gt;    &lt;char&gt;    &lt;char&gt;     &lt;num&gt;</span></span>
<span><span class="co">#&gt; 1:   doc1  POSITIVE 0.9970598</span></span>
<span><span class="co">#&gt; 2:   doc2  NEGATIVE 0.8472342</span></span>
<span><span class="co">#&gt; 3:   doc3  POSITIVE 0.9928006</span></span>
<span><span class="co">#&gt; 4:   doc4  POSITIVE 0.9901404</span></span>
<span><span class="co">#&gt; 5:   doc5  POSITIVE 0.9952670</span></span>
<span><span class="co">#&gt; 6:   doc6  POSITIVE 0.9291791</span></span></code></pre></div>
</div>
<!-- # Performing NER Tasks -->
<!-- <div style="text-align: justify"> -->
<!-- Flair NLP also provides a set of functions to perform NLP tasks, such as named entity recognition, sentiment analysis, and part-of-speech tagging.  -->
<!-- First, we load the data and the model to perform NER task on the text below. -->
<!-- > _Yesterday, Dr. Jane Smith spoke at the United Nations in New York. She discussed climate change and its impact on global economies. The event was attended by representatives from various countries including France and Japan. Dr. Smith mentioned that by 2050, the world could see a rise in sea level by approximately 2 feet. The World Health Organization (WHO) has pledged $50 million to combat the health effects of global warming. In an interview with The New York Times, Dr. Smith emphasized the urgent need for action. Later that day, she flew back to London, arriving at 10:00 PM GMT._ -->
<!-- ```{r} -->
<!-- Classifier <- flair_nn()$Classifier -->
<!-- Sentence <- flair_data()$Sentence -->
<!-- # load the model flair NLP already trained for us -->
<!-- tagger <- Classifier$load('ner') -->
<!-- # make a sentence object -->
<!-- text <- "Yesterday, Dr. Jane Smith spoke at the United Nations in New York. She discussed climate change and its impact on global economies. The event was attended by representatives from various countries including France and Japan. Dr. Smith mentioned that by 2050, the world could see a rise in sea level by approximately 2 feet. The World Health Organization (WHO) has pledged $50 million to combat the health effects of global warming. In an interview with The New York Times, Dr. Smith emphasized the urgent need for action. Later that day, she flew back to London, arriving at 10:00 PM GMT." -->
<!-- sentence <- Sentence(text) -->
<!-- # predict NER tags -->
<!-- tagger$predict(sentence) -->
<!-- # print sentence with predicted tags -->
<!-- print(sentence) -->
<!-- ``` -->
<!-- Alternatively, to facilitate more efficient use for social science research, {`flairR`} expands {`flairNLP/flair`}'s core functionality for working with three major functions to extract features in a tidy and fast format-- [data.table](https://cran.r-project.org/web/packages/data.table/index.html) in R.  -->
<!-- The expanded features in `flaiR` can be used to perform and extract features from the sentence object in a tidy format.  -->
<!-- - [**named entity recognition**](https://davidycliao.github.io/flaiR/articles/get_entities.html) -->
<!-- - [**transformer-based sentiment analysis**](https://davidycliao.github.io/flaiR/articles/get_sentiments.html) -->
<!-- - [**part-of-speech tagging**](https://davidycliao.github.io/flaiR/articles/get_pos.html) -->
<!-- For example, we can use the `get_entities` function and `load_tagger_ner("ner") `in flaiR to extract the named entities from the sentence object in a tidy format.  -->
<!-- ```{r} -->
<!-- tagger_ner <- load_tagger_ner("ner") -->
<!-- results <- get_entities(text = text,  -->
<!--                         doc_ids = "example text", -->
<!--                         tagger_ner) -->
<!-- print(results) -->
<!-- ``` -->
<!-- In most cases, we need to extract the named entities from a large corpus. For example, we can use Stefan's data from ___The Temporal Focus of Campaign Communication___ (JOP 2022) as an example. -->
<!-- ```{r} -->
<!-- library(flaiR) -->
<!-- data(cc_muller) -->
<!-- examples <- head(cc_muller, 10) -->
<!-- examples[c("text", "countryname")] -->
<!-- ``` -->
<!-- ```{r} -->
<!-- tagger_ner <- load_tagger_ner("ner") -->
<!-- results <- get_entities(text = examples$text,  -->
<!--                         doc_ids = examples$countryname, -->
<!--                         tagger_ner) -->
<!-- print(results) -->
<!-- ``` -->
<!-- In addition, to handle the load on RAM when dealing with larger corpus, {`flairR`}  supports batch processing to handle texts in batches, which is especially useful when dealing with large datasets, to optimize memory usage and performance. The implementation of batch processing can also utilize GPU acceleration for faster computations. -->
<hr>
</div>
</div>
</div>
<div class="section level2">
<h2 id="embedding">Embedding<a class="anchor" aria-label="anchor" href="#embedding"></a>
</h2>
<div style="text-align: justify;">
<p>Flair is a very popular natural language processing library,
providing a variety of embedding methods for text representation. Flair
Embeddings is a word embedding framework developed by <a href="https://engineering.zalando.com/posts/2018/11/zalando-research-releases-flair.html" class="external-link">Zalando</a>.
It focuses on word-level representation and can capture contextual
information of words, allowing the same word to have different
embeddings in different contexts. Unlike traditional word embeddings
(such as Word2Vec or GloVe), Flair can dynamically generate word
embeddings based on context and has achieved excellent results in
various NLP tasks. Below are some key points about Flair Embeddings:</p>
<p><strong>Context-Aware</strong></p>
<p>Flair is a dynamic word embedding technique that can understand the
meaning of words based on context. In contrast, static word embeddings,
such as Word2Vec or GloVe, provide a fixed embedding for each word
without considering its context in a sentence.</p>
<p>Therefore, context-sensitive embedding techniques, such as Flair, can
capture the meaning of words in specific sentences more accurately, thus
enhancing the performance of language models in various tasks.</p>
<p>Example:</p>
<p>Consider the following two English sentences:</p>
<ul>
<li>“I am interested in the bank of the river.”</li>
<li>“I need to go to the bank to withdraw money.”</li>
</ul>
<p>Here, the word “bank” has two different meanings. In the first
sentence, it refers to the edge or shore of a river. In the second
sentence, it refers to a financial institution.</p>
<p>For static embeddings, the word “bank” might have an embedding that
lies somewhere between these two meanings because it doesn’t consider
context. But for dynamic embeddings like Flair, “bank” in the first
sentence will have an embedding related to rivers, and in the second
sentence, it will have an embedding related to finance.</p>
<div class="sourceCode" id="cb37"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">FlairEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">FlairEmbeddings</span></span>
<span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span>
<span></span>
<span></span>
<span><span class="co"># Initialize Flair embeddings</span></span>
<span><span class="va">flair_embedding_forward</span> <span class="op">&lt;-</span> <span class="fu">FlairEmbeddings</span><span class="op">(</span><span class="st">'news-forward'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Define the two sentences</span></span>
<span><span class="va">sentence1</span> <span class="op">&lt;-</span>  <span class="fu">Sentence</span><span class="op">(</span><span class="st">"I am interested in the bank of the river."</span><span class="op">)</span></span>
<span><span class="va">sentence2</span> <span class="op">&lt;-</span>  <span class="fu">Sentence</span><span class="op">(</span><span class="st">"I need to go to the bank to withdraw money."</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Get the embeddings</span></span>
<span></span>
<span><span class="va">flair_embedding_forward</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence1</span><span class="op">)</span></span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Sentence[10]: "I am interested in the bank of the river."</span></span>
<span><span class="va">flair_embedding_forward</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence2</span><span class="op">)</span></span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Sentence[11]: "I need to go to the bank to withdraw money."</span></span>
<span></span>
<span><span class="co"># Extract the embedding for "bank" from the sentences</span></span>
<span><span class="va">bank_embedding_sentence1</span> <span class="op">=</span> <span class="va">sentence1</span><span class="op">[</span><span class="fl">5</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span>  <span class="co"># "bank" is the seventh word</span></span>
<span><span class="va">bank_embedding_sentence2</span> <span class="op">=</span> <span class="va">sentence2</span><span class="op">[</span><span class="fl">6</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span>  <span class="co"># "bank" is the sixth word</span></span></code></pre></div>
<p>Same word, similar vector representation, but essentially different.
In this way, you can see how the dynamic embeddings for “bank” in the
two sentences differ based on context. Although we printed the
embeddings here, in reality, they would be high-dimensional vectors, so
you might see a lot of numbers. If you want a more intuitive view of the
differences, you could compute the cosine similarity or other metrics
between the two embeddings.</p>
<p>This is just a simple demonstration. In practice, you can also
combine multiple embedding techniques, such as
<code>WordEmbeddings</code> and <code>FlairEmbeddings</code>, to get
richer word vectors.</p>
<div class="sourceCode" id="cb38"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va">lsa</span><span class="op">)</span></span>
<span><span class="co">#&gt; Loading required package: SnowballC</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/lsa/man/cosine.html" class="external-link">cosine</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/numeric.html" class="external-link">as.numeric</a></span><span class="op">(</span> <span class="va">bank_embedding_sentence1</span><span class="op">$</span><span class="fu">numpy</span><span class="op">(</span><span class="op">)</span><span class="op">)</span>, </span>
<span>       <span class="fu"><a href="https://rdrr.io/r/base/numeric.html" class="external-link">as.numeric</a></span><span class="op">(</span> <span class="va">bank_embedding_sentence2</span><span class="op">$</span><span class="fu">numpy</span><span class="op">(</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt;           [,1]</span></span>
<span><span class="co">#&gt; [1,] 0.7329551</span></span></code></pre></div>
<p><strong>Character-Based</strong></p>
<p>Flair uses a character-level language model, meaning it can generate
embeddings for rare words or even misspelled words. This is an important
feature because it allows the model to understand and process words that
have never appeared in the training data. Flair uses a bidirectional
LSTM (Long Short-Term Memory) network that operates at a character
level. This allows it to feed individual characters into the LSTM
instead of words.</p>
<p><strong>Multilingual Support</strong></p>
<p>Flair provides various pre-trained character-level language models,
supporting contextual word embeddings for multiple languages. It allows
you to easily combine different word embeddings (e.g., Flair Embeddings,
Word2Vec, GloVe, etc.) to create powerful stacked embeddings.</p>
</div>
<div class="section level3">
<h3 id="classic-wordembeddings">Classic Wordembeddings<a class="anchor" aria-label="anchor" href="#classic-wordembeddings"></a>
</h3>
<div style="text-align: justify;">
<p>In Flair, the simplest form of embeddings that still contains
semantic information about the word are called classic word embeddings.
These embeddings are pre-trained and non-contextual.</p>
<p>Let’s retrieve a few word embeddings and use FastText embeddings with
the following code. To do so, we simply instantiate a WordEmbeddings
class by passing in the ID of the embedding of our choice. Then, we
simply wrap our text into a Sentence object, and call the
<code>embed(sentence)</code> method on our WordEmbeddings class.</p>
<div class="sourceCode" id="cb39"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">WordEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">WordEmbeddings</span></span>
<span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span>
<span><span class="va">embedding</span> <span class="op">&lt;-</span> <span class="fu">WordEmbeddings</span><span class="op">(</span><span class="st">'crawl'</span><span class="op">)</span></span>
<span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">"one two three one"</span><span class="op">)</span> </span>
<span><span class="va">embedding</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span> </span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Sentence[4]: "one two three one"</span></span>
<span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq_along</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="va">tokens</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="va">tokens</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">)</span>, n <span class="op">=</span><span class="fl">5</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="co">#&gt; tensor([-0.0535, -0.0368, -0.2851, -0.0381, -0.0486,  0.2383])</span></span>
<span><span class="co">#&gt; tensor([ 0.0282, -0.0786, -0.1236,  0.1756, -0.1199,  0.0964])</span></span>
<span><span class="co">#&gt; tensor([-0.0920, -0.0690, -0.1475,  0.2313, -0.0872,  0.0799])</span></span>
<span><span class="co">#&gt; tensor([-0.0535, -0.0368, -0.2851, -0.0381, -0.0486,  0.2383])</span></span></code></pre></div>
<p>Flair supports a range of classic word embeddings, each offering
unique features and application scopes. Below is an overview, detailing
the ID required to load each embedding and its corresponding
language.</p>
<table class="table">
<thead><tr class="header">
<th>Embedding Type</th>
<th>ID</th>
<th>Language</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>GloVe</td>
<td>glove</td>
<td>English</td>
</tr>
<tr class="even">
<td>Komninos</td>
<td>extvec</td>
<td>English</td>
</tr>
<tr class="odd">
<td>Twitter</td>
<td>twitter</td>
<td>English</td>
</tr>
<tr class="even">
<td>Turian (small)</td>
<td>turian</td>
<td>English</td>
</tr>
<tr class="odd">
<td>FastText (crawl)</td>
<td>crawl</td>
<td>English</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>ar</td>
<td>Arabic</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>bg</td>
<td>Bulgarian</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>ca</td>
<td>Catalan</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>cz</td>
<td>Czech</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>da</td>
<td>Danish</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>de</td>
<td>German</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>es</td>
<td>Spanish</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>en</td>
<td>English</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>eu</td>
<td>Basque</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>fa</td>
<td>Persian</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>fi</td>
<td>Finnish</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>fr</td>
<td>French</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>he</td>
<td>Hebrew</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>hi</td>
<td>Hindi</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>hr</td>
<td>Croatian</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>id</td>
<td>Indonesian</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>it</td>
<td>Italian</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>ja</td>
<td>Japanese</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>ko</td>
<td>Korean</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>nl</td>
<td>Dutch</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>no</td>
<td>Norwegian</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>pl</td>
<td>Polish</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>pt</td>
<td>Portuguese</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>ro</td>
<td>Romanian</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>ru</td>
<td>Russian</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>si</td>
<td>Slovenian</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>sk</td>
<td>Slovak</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>sr</td>
<td>Serbian</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>sv</td>
<td>Swedish</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>tr</td>
<td>Turkish</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>zh</td>
<td>Chinese</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section level3">
<h3 id="contexual-embeddings">Contexual Embeddings<a class="anchor" aria-label="anchor" href="#contexual-embeddings"></a>
</h3>
<div style="text-align: justify;">
<p>The idea behind contextual string embeddings is that each word
embedding should be defined by not only its syntactic-semantic meaning
but also the context it appears in. What this means is that each word
will have a different embedding for every context it appears in. Each
pre-trained Flair model offers a <strong>forward</strong> version and a
<strong>backward</strong> version. Let’s assume you are processing a
language that, just like this text, uses the left-to-right script. The
forward version takes into account the context that happens before the
word – on the left-hand side. The backward version works in the opposite
direction. It takes into account the context after the word – on the
right-hand side of the word. If this is true, then two same words that
appear at the beginning of two different sentences should have identical
forward embeddings, because their context is null. Let’s test this
out:</p>
<p>Because we are using a forward model, it only takes into account the
context that occurs before a word. Additionally, since our word has no
context on the left-hand side of its position in the sentence, the two
embeddings are identical, and the code assumes they are identical,
indeed output is <strong>True</strong>.</p>
<div class="sourceCode" id="cb40"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">FlairEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">FlairEmbeddings</span></span>
<span><span class="va">embedding</span> <span class="op">&lt;-</span> <span class="fu">FlairEmbeddings</span><span class="op">(</span><span class="st">'news-forward'</span><span class="op">)</span></span>
<span><span class="va">s1</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">"nice shirt"</span><span class="op">)</span> </span>
<span><span class="va">s2</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">"nice pants"</span><span class="op">)</span> </span>
<span></span>
<span><span class="va">embedding</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">s1</span><span class="op">)</span> </span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Sentence[2]: "nice shirt"</span></span>
<span><span class="va">embedding</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">s2</span><span class="op">)</span> </span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Sentence[2]: "nice pants"</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">" s1 sentence:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste</a></span><span class="op">(</span><span class="va">s1</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span>, sep <span class="op">=</span> <span class="st">""</span><span class="op">)</span>, <span class="st">"\n"</span>, <span class="st">"s2 sentence:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste</a></span><span class="op">(</span><span class="va">s2</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span>, sep <span class="op">=</span> <span class="st">""</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt;  s1 sentence: Token[0]: "nice" </span></span>
<span><span class="co">#&gt;  s2 sentence: Token[0]: "nice"</span></span></code></pre></div>
<p>We test whether the sum of the two 2048 embeddings of
<code>nice</code> is equal to 2048. If it is true, it indicates that the
embedding results are consistent, which should theoretically be the
case.</p>
<div class="sourceCode" id="cb41"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">s1</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">numpy</span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">==</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">s1</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">numpy</span><span class="op">(</span><span class="op">)</span> <span class="op">==</span>  <span class="va">s2</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">numpy</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] TRUE</span></span></code></pre></div>
<p>Now we separately add a few more words, <code>very</code> and
<code>pretty</code>, into two sentence objects.</p>
<div class="sourceCode" id="cb42"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">s1</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">"very nice shirt"</span><span class="op">)</span> </span>
<span><span class="va">s2</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">"pretty nice pants"</span><span class="op">)</span> </span>
<span></span>
<span><span class="va">embedding</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">s1</span><span class="op">)</span> </span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Sentence[3]: "very nice shirt"</span></span>
<span><span class="va">embedding</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">s2</span><span class="op">)</span> </span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Sentence[3]: "pretty nice pants"</span></span></code></pre></div>
<p>The two sets of embeddings are not identical because the words are
different, so it returns <strong>FALSE</strong>.</p>
<div class="sourceCode" id="cb43"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">s1</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">numpy</span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">==</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">s1</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">numpy</span><span class="op">(</span><span class="op">)</span> <span class="op">==</span>  <span class="va">s2</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">numpy</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] FALSE</span></span></code></pre></div>
<p>The measure of similarity between two vectors in an inner product
space is known as cosine similarity. The formula for calculating cosine
similarity between two vectors, such as vectors A and B, is as
follows:</p>
<p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>n</mi><mi>e</mi><mi>S</mi><mi>i</mi><mi>m</mi><mi>i</mi><mi>l</mi><mi>a</mi><mi>r</mi><mi>i</mi><mi>t</mi><mi>y</mi><mo>=</mo><mfrac><mrow><msub><mo>∑</mo><mi>i</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>A</mi><mi>i</mi></msub><mo>⋅</mo><msub><mi>B</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><msqrt><mrow><msub><mo>∑</mo><mi>i</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mi>A</mi><mi>i</mi><mn>2</mn></msubsup><mo stretchy="true" form="postfix">)</mo></mrow></mrow></msqrt><mo>⋅</mo><msqrt><mrow><msub><mo>∑</mo><mi>i</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mi>B</mi><mi>i</mi><mn>2</mn></msubsup><mo stretchy="true" form="postfix">)</mo></mrow></mrow></msqrt></mrow></mfrac></mrow><annotation encoding="application/x-tex">Cosine Similarity = \frac{\sum_{i} (A_i \cdot B_i)}{\sqrt{\sum_{i} (A_i^2)} \cdot \sqrt{\sum_{i} (B_i^2)}}</annotation></semantics></math></p>
<div class="sourceCode" id="cb44"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va">lsa</span><span class="op">)</span></span>
<span><span class="va">vector1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html" class="external-link">as.numeric</a></span><span class="op">(</span><span class="va">s1</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">numpy</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">vector2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html" class="external-link">as.numeric</a></span><span class="op">(</span><span class="va">s2</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">numpy</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>We can observe that the similarity between the two words is 0.55.</p>
<div class="sourceCode" id="cb45"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cosine_similarity</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/lsa/man/cosine.html" class="external-link">cosine</a></span><span class="op">(</span><span class="va">vector1</span>, <span class="va">vector2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">cosine_similarity</span><span class="op">)</span></span>
<span><span class="co">#&gt;           [,1]</span></span>
<span><span class="co">#&gt; [1,] 0.5571664</span></span></code></pre></div>
</div>
<hr>
</div>
<div class="section level3">
<h3 id="extracting-embeddings-from-bert">Extracting Embeddings from BERT<a class="anchor" aria-label="anchor" href="#extracting-embeddings-from-bert"></a>
</h3>
<div style="text-align: justify;">
<p>First, we utilize the
<code>flair.embeddings.TransformerWordEmbeddings</code> function to
download BERT, and more transformer models can also be found on <a href="https://huggingface.co/flair" class="external-link">Flair NLP’s Hugging Face</a>.</p>
<div class="sourceCode" id="cb46"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://davidycliao.github.io/flaiR">flaiR</a></span><span class="op">)</span></span>
<span><span class="va">TransformerWordEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="fu">TransformerWordEmbeddings</span><span class="op">(</span><span class="st">"bert-base-uncased"</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb47"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">embedding</span> <span class="op">&lt;-</span> <span class="va">TransformerWordEmbeddings</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span></code></pre></div>
<p>Next, we traverse each token in the sentence and print them.</p>
<div class="sourceCode" id="cb48"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Iterate through each token in the sentence, printing them. </span></span>
<span><span class="co"># Utilize reticulate::py_str(token) to view each token, given that the sentence is a Python object.</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq_along</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="va">tokens</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"Token: "</span>, <span class="fu">reticulate</span><span class="fu">::</span><span class="fu"><a href="https://rstudio.github.io/reticulate/reference/py_str.html" class="external-link">py_str</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="va">tokens</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span><span class="op">)</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span>  <span class="co"># Access the embedding of the token, converting it to an R object, </span></span>
<span>  <span class="co"># and print the first 10 elements of the vector.</span></span>
<span>  <span class="va">token_embedding</span> <span class="op">&lt;-</span> <span class="va">sentence</span><span class="op">$</span><span class="va">tokens</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">token_embedding</span>, <span class="fl">10</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="co">#&gt; Token:  Token[0]: "one" </span></span>
<span><span class="co">#&gt; tensor([-0.0535, -0.0368, -0.2851, -0.0381, -0.0486,  0.2383, -0.1200,  0.2620,</span></span>
<span><span class="co">#&gt;         -0.0575,  0.0228])</span></span>
<span><span class="co">#&gt; Token:  Token[1]: "two" </span></span>
<span><span class="co">#&gt; tensor([ 0.0282, -0.0786, -0.1236,  0.1756, -0.1199,  0.0964, -0.1327,  0.4449,</span></span>
<span><span class="co">#&gt;         -0.0264, -0.1168])</span></span>
<span><span class="co">#&gt; Token:  Token[2]: "three" </span></span>
<span><span class="co">#&gt; tensor([-0.0920, -0.0690, -0.1475,  0.2313, -0.0872,  0.0799, -0.0901,  0.4403,</span></span>
<span><span class="co">#&gt;         -0.0103, -0.1494])</span></span>
<span><span class="co">#&gt; Token:  Token[3]: "one" </span></span>
<span><span class="co">#&gt; tensor([-0.0535, -0.0368, -0.2851, -0.0381, -0.0486,  0.2383, -0.1200,  0.2620,</span></span>
<span><span class="co">#&gt;         -0.0575,  0.0228])</span></span></code></pre></div>
</div>
<hr>
<hr>
</div>
<div class="section level3">
<h3 id="visialized-embeddings">Visialized Embeddings<a class="anchor" aria-label="anchor" href="#visialized-embeddings"></a>
</h3>
<div class="section level4">
<h4 id="word-embeddings-glove">1. Word Embeddings (GloVe)<a class="anchor" aria-label="anchor" href="#word-embeddings-glove"></a>
</h4>
<ul>
<li><p>GloVe embeddings are Pytorch vectors of dimensionality
100.</p></li>
<li><p>For English, Flair provides a few more options. Here, you can use
‘en-glove’ and ‘en-extvec’ with the WordEmbeddings class.</p></li>
</ul>
<div class="sourceCode" id="cb49"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Tokenize &amp; Embed</span></span>
<span><span class="co"># load the GloVe embeddings from Flair NLP via flaiR</span></span>
<span><span class="va">WordEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">WordEmbeddings</span></span>
<span><span class="va">embedding</span> <span class="op">&lt;-</span> <span class="fu">WordEmbeddings</span><span class="op">(</span><span class="st">"glove"</span><span class="op">)</span> </span>
<span></span>
<span><span class="co"># Tokenize the text</span></span>
<span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">"King Queen man woman Paris London apple orange Taiwan Dublin Bamberg"</span><span class="op">)</span> </span>
<span></span>
<span><span class="co"># Embed the sentence text using the loaded model.</span></span>
<span><span class="va">embedding</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Sentence[11]: "King Queen man woman Paris London apple orange Taiwan Dublin Bamberg"</span></span></code></pre></div>
<ul>
<li>The <code>sentence</code> is being embedded with the corresponding
vector from the model, store to the list.</li>
</ul>
<div class="sourceCode" id="cb50"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">sen_list</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq_along</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="va">tokens</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="co"># store the tensor vectors to numeric vectors</span></span>
<span>  <span class="va">sen_list</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/vector.html" class="external-link">as.vector</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="va">tokens</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">numpy</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<ul>
<li>Extract the name list to R vector</li>
</ul>
<div class="sourceCode" id="cb51"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">token_texts</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html" class="external-link">sapply</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="va">tokens</span>, <span class="kw">function</span><span class="op">(</span><span class="va">token</span><span class="op">)</span> <span class="va">token</span><span class="op">$</span><span class="va">text</span><span class="op">)</span></span></code></pre></div>
<ul>
<li>Form the dataframe.</li>
</ul>
<div class="sourceCode" id="cb52"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">sen_df</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/do.call.html" class="external-link">do.call</a></span><span class="op">(</span><span class="va">rbind</span>, <span class="fu"><a href="https://rdrr.io/r/base/lapply.html" class="external-link">lapply</a></span><span class="op">(</span><span class="va">sen_list</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/t.html" class="external-link">t</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">sen_df</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html" class="external-link">as.data.frame</a></span><span class="op">(</span><span class="va">sen_df</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html" class="external-link">rownames</a></span><span class="op">(</span><span class="va">sen_df</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="va">token_texts</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sen_df</span><span class="op">[</span>,<span class="fl">1</span><span class="op">:</span><span class="fl">20</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="co">#&gt;                 V1        V2        V3        V4         V5       V6</span></span>
<span><span class="co">#&gt; King    -0.3230700 -0.876160  0.219770  0.252680  0.2297600  0.73880</span></span>
<span><span class="co">#&gt; Queen   -0.5004500 -0.708260  0.553880  0.673000  0.2248600  0.60281</span></span>
<span><span class="co">#&gt; man      0.3729300  0.385030  0.710860 -0.659110 -0.0010128  0.92715</span></span>
<span><span class="co">#&gt; woman    0.5936800  0.448250  0.593200  0.074134  0.1114100  1.27930</span></span>
<span><span class="co">#&gt; Paris    0.9260500 -0.228180 -0.255240  0.739970  0.5007200  0.26424</span></span>
<span><span class="co">#&gt; London   0.6055300 -0.050886 -0.154610 -0.123270  0.6627000 -0.28506</span></span>
<span><span class="co">#&gt; apple   -0.5985000 -0.463210  0.130010 -0.019576  0.4603000 -0.30180</span></span>
<span><span class="co">#&gt; orange  -0.1496900  0.164770 -0.355320 -0.719150  0.6213000  0.74140</span></span>
<span><span class="co">#&gt; Taiwan   0.0061832  0.117350  0.535380  0.787290  0.6427700 -0.56057</span></span>
<span><span class="co">#&gt; Dublin  -0.4281400 -0.168970  0.035079  0.133170  0.4115600  1.03810</span></span>
<span><span class="co">#&gt; Bamberg  0.4854000 -0.296800  0.103520 -0.250310  0.4100900  0.45147</span></span>
<span><span class="co">#&gt;               V7        V8       V9        V10       V11       V12</span></span>
<span><span class="co">#&gt; King    -0.37954 -0.353070 -0.84369 -1.1113000 -0.302660  0.331780</span></span>
<span><span class="co">#&gt; Queen   -0.26194  0.738720 -0.65383 -0.2160600 -0.338060  0.244980</span></span>
<span><span class="co">#&gt; man      0.27615 -0.056203 -0.24294  0.2463200 -0.184490  0.313980</span></span>
<span><span class="co">#&gt; woman    0.16656  0.240700  0.39045  0.3276600 -0.750340  0.350070</span></span>
<span><span class="co">#&gt; Paris    0.40056  0.561450  0.17908  0.0504640  0.024095 -0.064805</span></span>
<span><span class="co">#&gt; London  -0.68844  0.491350 -0.68924  0.3892600  0.143590 -0.488020</span></span>
<span><span class="co">#&gt; apple    0.89770 -0.656340  0.66858 -0.4916400  0.037557 -0.050889</span></span>
<span><span class="co">#&gt; orange   0.68959  0.403710 -0.24239  0.1774000 -0.950790 -0.188870</span></span>
<span><span class="co">#&gt; Taiwan  -0.35941 -0.157720  0.97407 -0.1026900 -0.852620 -0.058598</span></span>
<span><span class="co">#&gt; Dublin  -0.32697  0.333970 -0.16726 -0.0034566 -0.361420 -0.067648</span></span>
<span><span class="co">#&gt; Bamberg -0.08002 -0.264430 -0.47231  0.0170920  0.036594 -0.483970</span></span>
<span><span class="co">#&gt;              V13      V14       V15        V16       V17       V18</span></span>
<span><span class="co">#&gt; King    -0.25113  0.30448 -0.077491 -0.8981500  0.092496 -1.140700</span></span>
<span><span class="co">#&gt; Queen   -0.51497  0.85680 -0.371990 -0.5882400  0.306370 -0.306680</span></span>
<span><span class="co">#&gt; man      0.48983  0.09256  0.329580  0.1505600  0.573170 -0.185290</span></span>
<span><span class="co">#&gt; woman    0.76057  0.38067  0.175170  0.0317910  0.468490 -0.216530</span></span>
<span><span class="co">#&gt; Paris   -0.25491  0.29661 -0.476020  0.2424400 -0.067045 -0.460290</span></span>
<span><span class="co">#&gt; London   0.15746  0.83178 -0.279230  0.0094755 -0.112070 -0.520990</span></span>
<span><span class="co">#&gt; apple    0.64510 -0.53882 -0.376500 -0.0431200  0.513840  0.177830</span></span>
<span><span class="co">#&gt; orange  -0.02344  0.49681  0.081903 -0.3694400  1.225700 -0.119000</span></span>
<span><span class="co">#&gt; Taiwan   1.19080  0.19279 -0.266930 -0.7671900  0.681310 -0.240430</span></span>
<span><span class="co">#&gt; Dublin  -0.45075  1.43470 -0.591370 -0.3136400  0.602490  0.145310</span></span>
<span><span class="co">#&gt; Bamberg -0.18393  0.68727  0.249500  0.2045100  0.517300  0.084214</span></span>
<span><span class="co">#&gt;               V19      V20</span></span>
<span><span class="co">#&gt; King    -0.583240  0.66869</span></span>
<span><span class="co">#&gt; Queen   -0.218700  0.78369</span></span>
<span><span class="co">#&gt; man     -0.522770  0.46191</span></span>
<span><span class="co">#&gt; woman   -0.462820  0.39967</span></span>
<span><span class="co">#&gt; Paris   -0.384060 -0.36540</span></span>
<span><span class="co">#&gt; London  -0.371590 -0.37951</span></span>
<span><span class="co">#&gt; apple    0.285960  0.92063</span></span>
<span><span class="co">#&gt; orange   0.955710 -0.19501</span></span>
<span><span class="co">#&gt; Taiwan  -0.086499 -0.18486</span></span>
<span><span class="co">#&gt; Dublin  -0.351880  0.18191</span></span>
<span><span class="co">#&gt; Bamberg -0.115300 -0.53820</span></span></code></pre></div>
<div class="section level5">
<h5 id="dimension-reduction-pca">Dimension Reduction (PCA)<a class="anchor" aria-label="anchor" href="#dimension-reduction-pca"></a>
</h5>
<div class="sourceCode" id="cb53"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Set the seed for reproducibility</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Execute PCA</span></span>
<span><span class="va">pca_result</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/prcomp.html" class="external-link">prcomp</a></span><span class="op">(</span><span class="va">sen_df</span>, center <span class="op">=</span> <span class="cn">TRUE</span>, scale. <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="va">word_embeddings_matrix</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html" class="external-link">as.data.frame</a></span><span class="op">(</span><span class="va">pca_result</span><span class="op">$</span><span class="va">x</span><span class="op">[</span>,<span class="fl">1</span><span class="op">:</span><span class="fl">3</span><span class="op">]</span> <span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html" class="external-link">rownames</a></span><span class="op">(</span><span class="va">word_embeddings_matrix</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="va">token_texts</span></span>
<span><span class="va">word_embeddings_matrix</span></span>
<span><span class="co">#&gt;                PC1       PC2         PC3</span></span>
<span><span class="co">#&gt; King    -2.9120910  1.285200 -1.95053854</span></span>
<span><span class="co">#&gt; Queen   -2.2413804  2.266714 -1.09020972</span></span>
<span><span class="co">#&gt; man     -5.6381902  2.984461  3.55462010</span></span>
<span><span class="co">#&gt; woman   -6.4891003  2.458607  3.56693660</span></span>
<span><span class="co">#&gt; Paris    3.0702212  5.039061 -2.65962020</span></span>
<span><span class="co">#&gt; London   5.3196216  4.368433 -2.60726627</span></span>
<span><span class="co">#&gt; apple    0.3362535 -8.679358 -0.44752722</span></span>
<span><span class="co">#&gt; orange  -0.0485467 -4.404101  0.77151480</span></span>
<span><span class="co">#&gt; Taiwan  -2.7993829 -4.149287 -6.33296039</span></span>
<span><span class="co">#&gt; Dublin   5.8994096  1.063291 -0.09271925</span></span>
<span><span class="co">#&gt; Bamberg  5.5031854 -2.233020  7.28777009</span></span></code></pre></div>
</div>
<div class="section level5">
<h5 id="d-plot">2D Plot<a class="anchor" aria-label="anchor" href="#d-plot"></a>
</h5>
<div class="sourceCode" id="cb54"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org" class="external-link">ggplot2</a></span><span class="op">)</span></span>
<span><span class="va">glove_plot2D</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html" class="external-link">ggplot</a></span><span class="op">(</span><span class="va">word_embeddings_matrix</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html" class="external-link">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">PC1</span>, y <span class="op">=</span> <span class="va">PC2</span>, color <span class="op">=</span> <span class="va">PC3</span>, </span>
<span>                                             label <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/colnames.html" class="external-link">rownames</a></span><span class="op">(</span><span class="va">word_embeddings_matrix</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html" class="external-link">geom_point</a></span><span class="op">(</span>size <span class="op">=</span> <span class="fl">3</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_text.html" class="external-link">geom_text</a></span><span class="op">(</span>vjust <span class="op">=</span> <span class="fl">1.5</span>, hjust <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span> <span class="op">+</span>  </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_gradient.html" class="external-link">scale_color_gradient</a></span><span class="op">(</span>low <span class="op">=</span> <span class="st">"blue"</span>, high <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html" class="external-link">theme_minimal</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span>  </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html" class="external-link">labs</a></span><span class="op">(</span>title <span class="op">=</span> <span class="st">""</span>, x <span class="op">=</span> <span class="st">"PC1"</span>, y <span class="op">=</span> <span class="st">"PC2"</span>, color <span class="op">=</span> <span class="st">"PC3"</span><span class="op">)</span> </span>
<span>  <span class="co"># guides(color = "none")  </span></span>
<span><span class="va">glove_plot2D</span></span></code></pre></div>
<p><img src="tutorial_files/figure-html/unnamed-chunk-56-1.png" width="80%"></p>
</div>
<div class="section level5">
<h5 id="d-plot-1">3D Plot<a class="anchor" aria-label="anchor" href="#d-plot-1"></a>
</h5>
<p><a href="https://plotly.com/r/" class="external-link">plotly</a> in R API: <a href="https://plotly.com/r/" class="external-link uri">https://plotly.com/r/</a></p>
<div class="sourceCode" id="cb55"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://plotly-r.com" class="external-link">plotly</a></span><span class="op">)</span></span>
<span><span class="va">glove_plot3D</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/plotly/man/plot_ly.html" class="external-link">plot_ly</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">word_embeddings_matrix</span>, </span>
<span>                  x <span class="op">=</span> <span class="op">~</span><span class="va">PC1</span>, y <span class="op">=</span> <span class="op">~</span><span class="va">PC2</span>, z <span class="op">=</span> <span class="op">~</span><span class="va">PC3</span>, </span>
<span>                  type <span class="op">=</span> <span class="st">"scatter3d"</span>, mode <span class="op">=</span> <span class="st">"markers"</span>,</span>
<span>                  marker <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>size <span class="op">=</span> <span class="fl">5</span><span class="op">)</span>, </span>
<span>                  text <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/colnames.html" class="external-link">rownames</a></span><span class="op">(</span><span class="va">word_embeddings_matrix</span><span class="op">)</span>, hoverinfo <span class="op">=</span> <span class="st">'text'</span><span class="op">)</span></span>
<span></span>
<span><span class="va">glove_plot3D</span></span></code></pre></div>
<div class="plotly html-widget html-fill-item" id="htmlwidget-ac96cb3ee4656e2e9ec3" style="width:80%;height:432.632880098888px;"></div>
<script type="application/json" data-for="htmlwidget-ac96cb3ee4656e2e9ec3">{"x":{"visdat":{"f9554e1bf605":["function () ","plotlyVisDat"]},"cur_data":"f9554e1bf605","attrs":{"f9554e1bf605":{"x":{},"y":{},"z":{},"mode":"markers","marker":{"size":5},"text":["King","Queen","man","woman","Paris","London","apple","orange","Taiwan","Dublin","Bamberg"],"hoverinfo":"text","alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d"}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"PC1"},"yaxis":{"title":"PC2"},"zaxis":{"title":"PC3"}},"hovermode":"closest","showlegend":false},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"x":[-2.9120909537095603,-2.2413803786431767,-5.6381902126446377,-6.489100263080025,3.0702212168953986,5.3196216302548889,0.33625352733862268,-0.048546700064410864,-2.7993829314866114,5.8994096251956245,5.503185439943886],"y":[1.2851996577231659,2.2667143730326891,2.9844609788519225,2.4586065811283584,5.0390609708283636,4.3684329527277832,-8.6793584684395366,-4.404100922374389,-4.149287004719004,1.0632913485136049,-2.2330204672729552],"z":[-1.9505385365242927,-1.0902097244737821,3.5546200951647635,3.5669365986108126,-2.6596201976138238,-2.6072662735213945,-0.44752721605925688,0.7715148004016178,-6.3329603909196477,-0.09271924783335489,7.2877700927683549],"mode":"markers","marker":{"color":"rgba(31,119,180,1)","size":5,"line":{"color":"rgba(31,119,180,1)"}},"text":["King","Queen","man","woman","Paris","London","apple","orange","Taiwan","Dublin","Bamberg"],"hoverinfo":["text","text","text","text","text","text","text","text","text","text","text"],"type":"scatter3d","error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.20000000000000001,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
</div>
</div>
<div class="section level4">
<h4 id="stack-embeddings-method-glove-backforwad-flairembeddings-or-more">2. Stack Embeddings Method (GloVe + Back/forwad FlairEmbeddings or
More)<a class="anchor" aria-label="anchor" href="#stack-embeddings-method-glove-backforwad-flairembeddings-or-more"></a>
</h4>
<div class="sourceCode" id="cb56"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Tokenize &amp; Embed</span></span>
<span><span class="co"># load WordEmbeddings and FlairEmbeddings</span></span>
<span><span class="va">WordEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">WordEmbeddings</span></span>
<span><span class="va">FlairEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">FlairEmbeddings</span></span>
<span><span class="va">StackedEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">StackedEmbeddings</span></span>
<span></span>
<span><span class="co"># init standard GloVe embedding</span></span>
<span><span class="va">glove_embedding</span> <span class="op">=</span> <span class="fu">WordEmbeddings</span><span class="op">(</span><span class="st">'glove'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># init Flair forward and backwards embeddings</span></span>
<span><span class="va">flair_embedding_forward</span> <span class="op">&lt;-</span> <span class="fu">FlairEmbeddings</span><span class="op">(</span><span class="st">'news-forward'</span><span class="op">)</span></span>
<span><span class="va">flair_embedding_backward</span> <span class="op">&lt;-</span> <span class="fu">FlairEmbeddings</span><span class="op">(</span><span class="st">'news-backward'</span><span class="op">)</span></span>
<span></span>
<span><span class="va">embedding</span> <span class="op">&lt;-</span> <span class="fu">WordEmbeddings</span><span class="op">(</span><span class="st">"glove"</span><span class="op">)</span> </span>
<span></span>
<span></span>
<span><span class="co"># create a StackedEmbedding object that combines glove and forward/backward flair embeddings</span></span>
<span><span class="va">stacked_embeddings</span> <span class="op">&lt;-</span>  <span class="fu">StackedEmbeddings</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="va">glove_embedding</span>,</span>
<span>                                           <span class="va">flair_embedding_forward</span>,</span>
<span>                                           <span class="va">flair_embedding_backward</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span>
<span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">"King Queen man woman Paris London apple orange Taiwan Dublin Bamberg"</span><span class="op">)</span> </span>
<span></span>
<span><span class="co"># just embed a sentence using the StackedEmbedding as you would with any single embedding.</span></span>
<span><span class="va">stacked_embeddings</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># The `sentence` is being embedded with the corresponding vector from the model, store to the list.</span></span>
<span><span class="va">sen_list</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq_along</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="va">tokens</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="co"># store the tensor vectors to numeric vectors</span></span>
<span>  <span class="va">sen_list</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/vector.html" class="external-link">as.vector</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="va">tokens</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">numpy</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Extract the name list to R vector</span></span>
<span><span class="va">token_texts</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html" class="external-link">sapply</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="va">tokens</span>, <span class="kw">function</span><span class="op">(</span><span class="va">token</span><span class="op">)</span> <span class="va">token</span><span class="op">$</span><span class="va">text</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Form the dataframe. </span></span>
<span><span class="va">sen_df</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/do.call.html" class="external-link">do.call</a></span><span class="op">(</span><span class="va">rbind</span>, <span class="fu"><a href="https://rdrr.io/r/base/lapply.html" class="external-link">lapply</a></span><span class="op">(</span><span class="va">sen_list</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/t.html" class="external-link">t</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">sen_df</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html" class="external-link">as.data.frame</a></span><span class="op">(</span><span class="va">sen_df</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html" class="external-link">rownames</a></span><span class="op">(</span><span class="va">sen_df</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="va">token_texts</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sen_df</span><span class="op">[</span>,<span class="fl">1</span><span class="op">:</span><span class="fl">20</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="co">#&gt;                 V1        V2        V3        V4         V5       V6</span></span>
<span><span class="co">#&gt; King    -0.3230700 -0.876160  0.219770  0.252680  0.2297600  0.73880</span></span>
<span><span class="co">#&gt; Queen   -0.5004500 -0.708260  0.553880  0.673000  0.2248600  0.60281</span></span>
<span><span class="co">#&gt; man      0.3729300  0.385030  0.710860 -0.659110 -0.0010128  0.92715</span></span>
<span><span class="co">#&gt; woman    0.5936800  0.448250  0.593200  0.074134  0.1114100  1.27930</span></span>
<span><span class="co">#&gt; Paris    0.9260500 -0.228180 -0.255240  0.739970  0.5007200  0.26424</span></span>
<span><span class="co">#&gt; London   0.6055300 -0.050886 -0.154610 -0.123270  0.6627000 -0.28506</span></span>
<span><span class="co">#&gt; apple   -0.5985000 -0.463210  0.130010 -0.019576  0.4603000 -0.30180</span></span>
<span><span class="co">#&gt; orange  -0.1496900  0.164770 -0.355320 -0.719150  0.6213000  0.74140</span></span>
<span><span class="co">#&gt; Taiwan   0.0061832  0.117350  0.535380  0.787290  0.6427700 -0.56057</span></span>
<span><span class="co">#&gt; Dublin  -0.4281400 -0.168970  0.035079  0.133170  0.4115600  1.03810</span></span>
<span><span class="co">#&gt; Bamberg  0.4854000 -0.296800  0.103520 -0.250310  0.4100900  0.45147</span></span>
<span><span class="co">#&gt;               V7        V8       V9        V10       V11       V12</span></span>
<span><span class="co">#&gt; King    -0.37954 -0.353070 -0.84369 -1.1113000 -0.302660  0.331780</span></span>
<span><span class="co">#&gt; Queen   -0.26194  0.738720 -0.65383 -0.2160600 -0.338060  0.244980</span></span>
<span><span class="co">#&gt; man      0.27615 -0.056203 -0.24294  0.2463200 -0.184490  0.313980</span></span>
<span><span class="co">#&gt; woman    0.16656  0.240700  0.39045  0.3276600 -0.750340  0.350070</span></span>
<span><span class="co">#&gt; Paris    0.40056  0.561450  0.17908  0.0504640  0.024095 -0.064805</span></span>
<span><span class="co">#&gt; London  -0.68844  0.491350 -0.68924  0.3892600  0.143590 -0.488020</span></span>
<span><span class="co">#&gt; apple    0.89770 -0.656340  0.66858 -0.4916400  0.037557 -0.050889</span></span>
<span><span class="co">#&gt; orange   0.68959  0.403710 -0.24239  0.1774000 -0.950790 -0.188870</span></span>
<span><span class="co">#&gt; Taiwan  -0.35941 -0.157720  0.97407 -0.1026900 -0.852620 -0.058598</span></span>
<span><span class="co">#&gt; Dublin  -0.32697  0.333970 -0.16726 -0.0034566 -0.361420 -0.067648</span></span>
<span><span class="co">#&gt; Bamberg -0.08002 -0.264430 -0.47231  0.0170920  0.036594 -0.483970</span></span>
<span><span class="co">#&gt;              V13      V14       V15        V16       V17       V18</span></span>
<span><span class="co">#&gt; King    -0.25113  0.30448 -0.077491 -0.8981500  0.092496 -1.140700</span></span>
<span><span class="co">#&gt; Queen   -0.51497  0.85680 -0.371990 -0.5882400  0.306370 -0.306680</span></span>
<span><span class="co">#&gt; man      0.48983  0.09256  0.329580  0.1505600  0.573170 -0.185290</span></span>
<span><span class="co">#&gt; woman    0.76057  0.38067  0.175170  0.0317910  0.468490 -0.216530</span></span>
<span><span class="co">#&gt; Paris   -0.25491  0.29661 -0.476020  0.2424400 -0.067045 -0.460290</span></span>
<span><span class="co">#&gt; London   0.15746  0.83178 -0.279230  0.0094755 -0.112070 -0.520990</span></span>
<span><span class="co">#&gt; apple    0.64510 -0.53882 -0.376500 -0.0431200  0.513840  0.177830</span></span>
<span><span class="co">#&gt; orange  -0.02344  0.49681  0.081903 -0.3694400  1.225700 -0.119000</span></span>
<span><span class="co">#&gt; Taiwan   1.19080  0.19279 -0.266930 -0.7671900  0.681310 -0.240430</span></span>
<span><span class="co">#&gt; Dublin  -0.45075  1.43470 -0.591370 -0.3136400  0.602490  0.145310</span></span>
<span><span class="co">#&gt; Bamberg -0.18393  0.68727  0.249500  0.2045100  0.517300  0.084214</span></span>
<span><span class="co">#&gt;               V19      V20</span></span>
<span><span class="co">#&gt; King    -0.583240  0.66869</span></span>
<span><span class="co">#&gt; Queen   -0.218700  0.78369</span></span>
<span><span class="co">#&gt; man     -0.522770  0.46191</span></span>
<span><span class="co">#&gt; woman   -0.462820  0.39967</span></span>
<span><span class="co">#&gt; Paris   -0.384060 -0.36540</span></span>
<span><span class="co">#&gt; London  -0.371590 -0.37951</span></span>
<span><span class="co">#&gt; apple    0.285960  0.92063</span></span>
<span><span class="co">#&gt; orange   0.955710 -0.19501</span></span>
<span><span class="co">#&gt; Taiwan  -0.086499 -0.18486</span></span>
<span><span class="co">#&gt; Dublin  -0.351880  0.18191</span></span>
<span><span class="co">#&gt; Bamberg -0.115300 -0.53820</span></span></code></pre></div>
<div class="sourceCode" id="cb57"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Dimension Reduction </span></span>
<span><span class="co"># Set the seed for reproducibility</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Execute PCA</span></span>
<span><span class="va">pca_result</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/prcomp.html" class="external-link">prcomp</a></span><span class="op">(</span><span class="va">sen_df</span>, center <span class="op">=</span> <span class="cn">TRUE</span>, scale. <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="va">word_embeddings_matrix</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html" class="external-link">as.data.frame</a></span><span class="op">(</span><span class="va">pca_result</span><span class="op">$</span><span class="va">x</span><span class="op">[</span>,<span class="fl">1</span><span class="op">:</span><span class="fl">3</span><span class="op">]</span> <span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html" class="external-link">rownames</a></span><span class="op">(</span><span class="va">word_embeddings_matrix</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="va">token_texts</span></span>
<span><span class="va">word_embeddings_matrix</span></span>
<span><span class="co">#&gt;                PC1         PC2         PC3</span></span>
<span><span class="co">#&gt; King     -8.607474  67.2291112  32.4862807</span></span>
<span><span class="co">#&gt; Queen     1.757707  12.0477210 -26.8302480</span></span>
<span><span class="co">#&gt; man      70.603191  -6.6184707  13.1651688</span></span>
<span><span class="co">#&gt; woman    22.532043  -8.1126267  -0.9073998</span></span>
<span><span class="co">#&gt; Paris   -11.395619  -0.3051693 -17.5197067</span></span>
<span><span class="co">#&gt; London   -8.709174  -2.7450626 -14.1780531</span></span>
<span><span class="co">#&gt; apple    -8.739477 -15.7725211  -6.3796349</span></span>
<span><span class="co">#&gt; orange  -25.178329 -38.8501308  51.4907636</span></span>
<span><span class="co">#&gt; Taiwan   -9.132397  -5.0252091 -11.0918877</span></span>
<span><span class="co">#&gt; Dublin  -10.925014  -3.3407329 -10.1367729</span></span>
<span><span class="co">#&gt; Bamberg -12.205457   1.4930909 -10.0985100</span></span></code></pre></div>
<div class="sourceCode" id="cb58"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># 2D Plot</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org" class="external-link">ggplot2</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">stacked_plot2D</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html" class="external-link">ggplot</a></span><span class="op">(</span><span class="va">word_embeddings_matrix</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html" class="external-link">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">PC1</span>, y <span class="op">=</span> <span class="va">PC2</span>, color <span class="op">=</span> <span class="va">PC3</span>, </span>
<span>                                             label <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/colnames.html" class="external-link">rownames</a></span><span class="op">(</span><span class="va">word_embeddings_matrix</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html" class="external-link">geom_point</a></span><span class="op">(</span>size <span class="op">=</span> <span class="fl">2</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_text.html" class="external-link">geom_text</a></span><span class="op">(</span>vjust <span class="op">=</span> <span class="fl">1.5</span>, hjust <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span> <span class="op">+</span>  </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_gradient.html" class="external-link">scale_color_gradient</a></span><span class="op">(</span>low <span class="op">=</span> <span class="st">"blue"</span>, high <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html" class="external-link">theme_minimal</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span>  </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html" class="external-link">labs</a></span><span class="op">(</span>title <span class="op">=</span> <span class="st">""</span>, x <span class="op">=</span> <span class="st">"PC1"</span>, y <span class="op">=</span> <span class="st">"PC2"</span>, color <span class="op">=</span> <span class="st">"PC3"</span><span class="op">)</span> </span>
<span>  <span class="co"># guides(color = "none") </span></span>
<span></span>
<span><span class="va">stacked_plot2D</span></span></code></pre></div>
<p><img src="tutorial_files/figure-html/unnamed-chunk-60-1.png" width="80%"></p>
</div>
<div class="section level4">
<h4 id="transformer-embeddings-bert-or-more">3. Transformer Embeddings (BERT or More)<a class="anchor" aria-label="anchor" href="#transformer-embeddings-bert-or-more"></a>
</h4>
<div class="sourceCode" id="cb59"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://davidycliao.github.io/flaiR">flaiR</a></span><span class="op">)</span></span>
<span><span class="co"># Load Sentence and BERT model</span></span>
<span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span>
<span><span class="va">TransformerWordEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="fu">TransformerWordEmbeddings</span><span class="op">(</span><span class="st">"bert-base-uncased"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">"King Queen man woman Paris London apple orange Taiwan Dublin Bamberg"</span><span class="op">)</span> </span>
<span><span class="va">TransformerWordEmbeddings</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Sentence[11]: "King Queen man woman Paris London apple orange Taiwan Dublin Bamberg"</span></span></code></pre></div>
<div class="sourceCode" id="cb60"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># The `sentence` is being embedded with the corresponding vector from the model, store to the list.</span></span>
<span><span class="va">sen_list</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq_along</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="va">tokens</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="co"># store the tensor vectors to numeric vectors</span></span>
<span>  <span class="va">sen_list</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/vector.html" class="external-link">as.vector</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="va">tokens</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">numpy</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Extract the name list to R vector</span></span>
<span><span class="va">token_texts</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html" class="external-link">sapply</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="va">tokens</span>, <span class="kw">function</span><span class="op">(</span><span class="va">token</span><span class="op">)</span> <span class="va">token</span><span class="op">$</span><span class="va">text</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Format the dataframe. </span></span>
<span><span class="va">sen_df</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/do.call.html" class="external-link">do.call</a></span><span class="op">(</span><span class="va">rbind</span>, <span class="fu"><a href="https://rdrr.io/r/base/lapply.html" class="external-link">lapply</a></span><span class="op">(</span><span class="va">sen_list</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/t.html" class="external-link">t</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">sen_df</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html" class="external-link">as.data.frame</a></span><span class="op">(</span><span class="va">sen_df</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html" class="external-link">rownames</a></span><span class="op">(</span><span class="va">sen_df</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="va">token_texts</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sen_df</span><span class="op">[</span>,<span class="fl">1</span><span class="op">:</span><span class="fl">20</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="co">#&gt;                  V1           V2         V3          V4         V5</span></span>
<span><span class="co">#&gt; King     0.20243725 -0.302875251 0.88937163 -0.39685369 -0.1486676</span></span>
<span><span class="co">#&gt; Queen    0.38636127 -0.136571497 1.07043457 -0.39234018  0.3492773</span></span>
<span><span class="co">#&gt; man     -0.11609044 -0.278030753 1.11280990 -0.26518247  0.5489236</span></span>
<span><span class="co">#&gt; woman   -0.34034440  0.002125926 0.70898271 -0.01405379  0.1248749</span></span>
<span><span class="co">#&gt; Paris   -0.11196213  0.049277060 0.57255691 -0.17925967 -0.1082409</span></span>
<span><span class="co">#&gt; London  -0.24880084  0.129327983 0.16446528 -0.38713518 -0.3233015</span></span>
<span><span class="co">#&gt; apple    0.01825429  0.289205372 0.08635567 -0.14463882  0.4227849</span></span>
<span><span class="co">#&gt; orange  -0.08206877 -0.346834421 0.14041321 -0.01703753  0.9596846</span></span>
<span><span class="co">#&gt; Taiwan  -0.21529624  0.164859354 0.55788797  0.08166986 -0.3301849</span></span>
<span><span class="co">#&gt; Dublin  -0.26578656  0.304162681 0.24737860 -0.05140827 -0.6028203</span></span>
<span><span class="co">#&gt; Bamberg -0.02511016 -0.568264186 0.43023247 -0.07210951  0.1909387</span></span>
<span><span class="co">#&gt;                   V6          V7         V8           V9         V10</span></span>
<span><span class="co">#&gt; King     0.008254345 -0.03680899 0.06456959 -0.368780434 -0.32244465</span></span>
<span><span class="co">#&gt; Queen    0.004886352  0.44847178 0.65026569  0.058996066 -0.47662011</span></span>
<span><span class="co">#&gt; man      0.342995614  0.20067003 0.30138496  0.188928455 -0.53515178</span></span>
<span><span class="co">#&gt; woman    0.437581152  0.50878483 0.29988331 -0.082988761 -0.02024770</span></span>
<span><span class="co">#&gt; Paris    0.158250496 -0.25374153 0.23834446 -0.014040919  0.05501739</span></span>
<span><span class="co">#&gt; London  -0.247915953 -0.51886940 0.10059015  0.064994000 -0.13463272</span></span>
<span><span class="co">#&gt; apple    0.303519875 -0.24322695 0.55958885  0.132094145 -0.30050987</span></span>
<span><span class="co">#&gt; orange   0.240883887 -0.36664891 0.37507194 -0.170591414 -0.57967031</span></span>
<span><span class="co">#&gt; Taiwan  -0.088986777 -0.17876987 0.29112476  0.422848225  0.22081339</span></span>
<span><span class="co">#&gt; Dublin  -0.238457471  0.05760814 0.11389303  0.008800384  0.24732894</span></span>
<span><span class="co">#&gt; Bamberg -0.657174826  0.12236608 0.04169786 -1.401770711  0.27092379</span></span>
<span><span class="co">#&gt;                 V11          V12         V13         V14         V15</span></span>
<span><span class="co">#&gt; King    -0.21798825  0.094209142  0.28516501  0.15710688 -0.35424700</span></span>
<span><span class="co">#&gt; Queen   -0.38251543 -0.486830354 -0.02046072  0.58473915 -0.58738035</span></span>
<span><span class="co">#&gt; man     -0.33577001 -0.177165955 -0.09695894  0.24606071 -0.45691946</span></span>
<span><span class="co">#&gt; woman   -0.18178533  0.001003223  0.26221192  0.06231856 -0.38738111</span></span>
<span><span class="co">#&gt; Paris   -0.22901714  0.457912028  0.04404202 -0.25944969  0.06393644</span></span>
<span><span class="co">#&gt; London  -0.18075228  0.514925241  0.12762237 -0.24772680 -0.05417314</span></span>
<span><span class="co">#&gt; apple   -0.15327577  0.442788988  0.21187572 -0.26953286  0.29568702</span></span>
<span><span class="co">#&gt; orange  -0.10221374 -0.048551828  0.07183206  0.26971415 -0.12249112</span></span>
<span><span class="co">#&gt; Taiwan  -0.36020637  0.418745756  0.25126299 -0.63927531  0.20376265</span></span>
<span><span class="co">#&gt; Dublin  -0.40517145  0.282728165  0.16409895 -0.12579527  0.14734903</span></span>
<span><span class="co">#&gt; Bamberg -0.01255565 -0.380868316  0.24334061  0.05542289 -1.02990639</span></span>
<span><span class="co">#&gt;                 V16         V17         V18         V19         V20</span></span>
<span><span class="co">#&gt; King    -0.10926335 -0.04176838  0.08259771 -0.03663497 -0.10094819</span></span>
<span><span class="co">#&gt; Queen    0.16076291 -0.14089347 -0.03677157 -0.13579193 -0.19312018</span></span>
<span><span class="co">#&gt; man     -0.39715117 -0.21910432  0.14614260 -0.38266426 -0.35972801</span></span>
<span><span class="co">#&gt; woman   -0.00680415 -0.20422488  0.07532670  0.10188285 -0.04957973</span></span>
<span><span class="co">#&gt; Paris    0.29756585 -0.06224668 -0.12700970  0.46103561  0.43724096</span></span>
<span><span class="co">#&gt; London  -0.02949974  0.09688052 -0.07327853  0.96583605  0.40061006</span></span>
<span><span class="co">#&gt; apple   -0.08441193  0.05031339  0.22658241  0.33377019  0.24120221</span></span>
<span><span class="co">#&gt; orange   0.22620890  0.15824266  0.48752186 -0.78691489 -0.11886403</span></span>
<span><span class="co">#&gt; Taiwan   0.07144606  0.28152949 -0.17945638  0.49592388  0.26298687</span></span>
<span><span class="co">#&gt; Dublin   0.16665313  0.41901654 -0.24815717  0.71733290  0.39634189</span></span>
<span><span class="co">#&gt; Bamberg  0.92171913  0.52353865 -0.23750052 -0.27257800  0.32791424</span></span>
<span></span>
<span><span class="co"># Dimension Reduction using PCA </span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span>  <span class="co"># Set the seed for reproducibility</span></span>
<span><span class="va">pca_result</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/prcomp.html" class="external-link">prcomp</a></span><span class="op">(</span><span class="va">sen_df</span>, center <span class="op">=</span> <span class="cn">TRUE</span>, scale. <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="va">word_embeddings_matrix</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html" class="external-link">as.data.frame</a></span><span class="op">(</span><span class="va">pca_result</span><span class="op">$</span><span class="va">x</span><span class="op">[</span>,<span class="fl">1</span><span class="op">:</span><span class="fl">3</span><span class="op">]</span> <span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html" class="external-link">rownames</a></span><span class="op">(</span><span class="va">word_embeddings_matrix</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="va">token_texts</span></span></code></pre></div>
<div class="sourceCode" id="cb61"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># 2D Plot</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org" class="external-link">ggplot2</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">bert_plot2D</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html" class="external-link">ggplot</a></span><span class="op">(</span><span class="va">word_embeddings_matrix</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html" class="external-link">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">PC1</span>, y <span class="op">=</span> <span class="va">PC2</span>, color <span class="op">=</span> <span class="va">PC3</span>, </span>
<span>                                             label <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/colnames.html" class="external-link">rownames</a></span><span class="op">(</span><span class="va">word_embeddings_matrix</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html" class="external-link">geom_point</a></span><span class="op">(</span>size <span class="op">=</span> <span class="fl">2</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_text.html" class="external-link">geom_text</a></span><span class="op">(</span>vjust <span class="op">=</span> <span class="fl">1.5</span>, hjust <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span> <span class="op">+</span>  </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_gradient.html" class="external-link">scale_color_gradient</a></span><span class="op">(</span>low <span class="op">=</span> <span class="st">"blue"</span>, high <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html" class="external-link">theme_minimal</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span>  </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html" class="external-link">labs</a></span><span class="op">(</span>title <span class="op">=</span> <span class="st">""</span>, x <span class="op">=</span> <span class="st">"PC1"</span>, y <span class="op">=</span> <span class="st">"PC2"</span>, color <span class="op">=</span> <span class="st">"PC3"</span><span class="op">)</span> </span>
<span>  <span class="co"># guides(color = "none") </span></span>
<span></span>
<span><span class="va">stacked_plot2D</span></span></code></pre></div>
<p><img src="tutorial_files/figure-html/unnamed-chunk-63-1.png" width="80%"></p>
</div>
<div class="section level4">
<h4 id="embedding-models-comparison">4. Embedding Models Comparison<a class="anchor" aria-label="anchor" href="#embedding-models-comparison"></a>
</h4>
<div class="sourceCode" id="cb62"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://rpkgs.datanovia.com/ggpubr/" class="external-link">ggpubr</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">figure</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rpkgs.datanovia.com/ggpubr/reference/ggarrange.html" class="external-link">ggarrange</a></span><span class="op">(</span><span class="va">glove_plot2D</span>, <span class="va">stacked_plot2D</span>, <span class="va">bert_plot2D</span>,</span>
<span>                   labels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"Glove"</span>, <span class="st">"Stacked Embedding"</span>, <span class="st">"BERT"</span><span class="op">)</span>,</span>
<span>                   ncol <span class="op">=</span> <span class="fl">3</span>, nrow <span class="op">=</span> <span class="fl">1</span>,</span>
<span>                   common.legend <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                   legend <span class="op">=</span> <span class="st">"bottom"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">figure</span></span></code></pre></div>
<p><img src="tutorial_files/figure-html/unnamed-chunk-64-1.png" width="80%"></p>
<hr>
</div>
</div>
</div>
<div class="section level2">
<h2 id="training-a-binary-classifier">Training a Binary Classifier<a class="anchor" aria-label="anchor" href="#training-a-binary-classifier"></a>
</h2>
<div style="text-align: justify;">
<p>In this section, we’ll train a sentiment analysis model that can
categorize text as either positive or negative. This case study is
adapted from pages 116 to 130 of Tadej Magajna’s book, ‘<a href="https://www.packtpub.com/product/natural-language-processing-with-flair/9781801072311" class="external-link">Natural
Language Processing with Flair</a>’. The process for training text
classifiers in Flair mirrors the process followed for sequence labeling
models. Specifically, the steps to train text classifiers are:</p>
<ul>
<li>Load a tagged corpus and compute the label dictionary map.</li>
<li>Prepare the document embeddings.</li>
<li>Initialize the <code>TextClassifier</code> class.</li>
<li>Train the model.</li>
</ul>
</div>
<div class="section level3">
<h3 id="loading-a-tagged-corpus">Loading a Tagged Corpus<a class="anchor" aria-label="anchor" href="#loading-a-tagged-corpus"></a>
</h3>
<div style="text-align: justify;">
<p>Training text classification models requires a set of text documents
(typically, sentences or paragraphs) where each document is associated
with one or more classification labels. To train our sentiment analysis
text classification model, we will be using the famous Internet Movie
Database (IMDb) dataset, which contains 50,000 movie reviews from IMDB,
where each review is labeled as either positive or negative. References
to this dataset are already baked into Flair, so loading the dataset
couldn’t be easier:</p>
<div class="sourceCode" id="cb63"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://davidycliao.github.io/flaiR">flaiR</a></span><span class="op">)</span></span>
<span><span class="co"># load IMDB from flair_datasets module</span></span>
<span><span class="va">Corpus</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Corpus</span></span>
<span><span class="va">IMDB</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_datasets.html">flair_datasets</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">IMDB</span></span></code></pre></div>
<div class="sourceCode" id="cb64"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># downsize to 0.05</span></span>
<span><span class="va">corpus</span> <span class="op">=</span> <span class="fu">IMDB</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:44,029 Reading data from /Users/yenchiehliao/.flair/datasets/imdb_v4-rebalanced</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:44,029 Train: /Users/yenchiehliao/.flair/datasets/imdb_v4-rebalanced/train.txt</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:44,029 Dev: None</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:44,029 Test: None</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:44,570 No test split found. Using 10% (i.e. 5000 samples) of the train split as test data</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:44,580 No dev split found. Using 10% (i.e. 4500 samples) of the train split as dev data</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:44,581 Initialized corpus /Users/yenchiehliao/.flair/datasets/imdb_v4-rebalanced (label type name is 'sentiment')</span></span>
<span><span class="va">corpus</span><span class="op">$</span><span class="fu">downsample</span><span class="op">(</span><span class="fl">0.05</span><span class="op">)</span></span>
<span><span class="co">#&gt; &lt;flair.datasets.document_classification.IMDB object at 0x391d5b010&gt;</span></span></code></pre></div>
<p>Print the sizes in the corpus object as follows - test: %d | train:
%d | dev: %d”</p>
<div class="sourceCode" id="cb65"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">test_size</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">corpus</span><span class="op">$</span><span class="va">test</span><span class="op">)</span></span>
<span><span class="va">train_size</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">corpus</span><span class="op">$</span><span class="va">train</span><span class="op">)</span></span>
<span><span class="va">dev_size</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">corpus</span><span class="op">$</span><span class="va">dev</span><span class="op">)</span></span>
<span><span class="va">output</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sprintf.html" class="external-link">sprintf</a></span><span class="op">(</span><span class="st">"Corpus object sizes - Test: %d | Train: %d | Dev: %d"</span>, <span class="va">test_size</span>, <span class="va">train_size</span>, <span class="va">dev_size</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">output</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "Corpus object sizes - Test: 250 | Train: 2025 | Dev: 225"</span></span></code></pre></div>
<div class="sourceCode" id="cb66"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">lbl_type</span> <span class="op">=</span> <span class="st">'sentiment'</span></span>
<span><span class="va">label_dict</span> <span class="op">=</span> <span class="va">corpus</span><span class="op">$</span><span class="fu">make_label_dictionary</span><span class="op">(</span>label_type<span class="op">=</span><span class="va">lbl_type</span><span class="op">)</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:44,688 Computing label dictionary. Progress:</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:48,025 Dictionary created for label 'sentiment' with 2 values: POSITIVE (seen 1014 times), NEGATIVE (seen 1011 times)</span></span></code></pre></div>
</div>
</div>
<div class="section level3">
<h3 id="loading-the-embeddings">Loading the Embeddings<a class="anchor" aria-label="anchor" href="#loading-the-embeddings"></a>
</h3>
<div style="text-align: justify;">
<p>flaiR covers all the different types of document embeddings that we
can use. Here, we simply use <code>DocumentPoolEmbeddings</code>. They
require no training prior to training the classification model
itself:</p>
<div class="sourceCode" id="cb67"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">DocumentPoolEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">DocumentPoolEmbeddings</span></span>
<span><span class="va">WordEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">WordEmbeddings</span></span>
<span><span class="va">glove</span> <span class="op">=</span> <span class="fu">WordEmbeddings</span><span class="op">(</span><span class="st">'glove'</span><span class="op">)</span></span>
<span><span class="va">document_embeddings</span> <span class="op">=</span> <span class="fu">DocumentPoolEmbeddings</span><span class="op">(</span><span class="va">glove</span><span class="op">)</span></span></code></pre></div>
</div>
</div>
<div class="section level3">
<h3 id="initializing-the-textclassifier">Initializing the TextClassifier<a class="anchor" aria-label="anchor" href="#initializing-the-textclassifier"></a>
</h3>
<div style="text-align: justify;">
<div class="sourceCode" id="cb68"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># initiate TextClassifier</span></span>
<span><span class="va">TextClassifier</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_models.html">flair_models</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">TextClassifier</span></span>
<span><span class="va">classifier</span> <span class="op">&lt;-</span> <span class="fu">TextClassifier</span><span class="op">(</span><span class="va">document_embeddings</span>,</span>
<span>                             label_dictionary <span class="op">=</span> <span class="va">label_dict</span>,</span>
<span>                             label_type <span class="op">=</span> <span class="va">lbl_type</span><span class="op">)</span></span></code></pre></div>
<p><code>$to</code> allows you to set the device to use CPU, GPU, or
specific MPS devices on Mac (such as mps:0, mps:1, mps:2).</p>
<div class="sourceCode" id="cb69"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">classifier</span><span class="op">$</span><span class="fu">to</span><span class="op">(</span><span class="fu"><a href="../reference/flair_device.html">flair_device</a></span><span class="op">(</span><span class="st">"mps"</span><span class="op">)</span><span class="op">)</span> </span></code></pre></div>
<pre><code><span><span class="fu">TextClassifier</span><span class="op">(</span></span>
<span>  <span class="op">(</span><span class="va">embeddings</span><span class="op">)</span><span class="op">:</span> <span class="fu">DocumentPoolEmbeddings</span><span class="op">(</span></span>
<span>    fine_tune_mode<span class="op">=</span><span class="va">none</span>, pooling<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span></span>
<span>    <span class="op">(</span><span class="va">embeddings</span><span class="op">)</span><span class="op">:</span> <span class="fu">StackedEmbeddings</span><span class="op">(</span></span>
<span>      <span class="op">(</span><span class="va">list_embedding_0</span><span class="op">)</span><span class="op">:</span> <span class="fu">WordEmbeddings</span><span class="op">(</span></span>
<span>        <span class="st">'glove'</span></span>
<span>        <span class="op">(</span><span class="va">embedding</span><span class="op">)</span><span class="op">:</span> <span class="fu">Embedding</span><span class="op">(</span><span class="fl">400001</span>, <span class="fl">100</span><span class="op">)</span></span>
<span>      <span class="op">)</span></span>
<span>    <span class="op">)</span></span>
<span>  <span class="op">)</span></span>
<span>  <span class="op">(</span><span class="va">decoder</span><span class="op">)</span><span class="op">:</span> <span class="fu">Linear</span><span class="op">(</span>in_features<span class="op">=</span><span class="fl">100</span>, out_features<span class="op">=</span><span class="fl">3</span>, bias<span class="op">=</span><span class="va">True</span><span class="op">)</span></span>
<span>  <span class="op">(</span><span class="va">dropout</span><span class="op">)</span><span class="op">:</span> <span class="fu">Dropout</span><span class="op">(</span>p<span class="op">=</span><span class="fl">0.0</span>, inplace<span class="op">=</span><span class="va">False</span><span class="op">)</span></span>
<span>  <span class="op">(</span><span class="va">locked_dropout</span><span class="op">)</span><span class="op">:</span> <span class="fu">LockedDropout</span><span class="op">(</span>p<span class="op">=</span><span class="fl">0.0</span><span class="op">)</span></span>
<span>  <span class="op">(</span><span class="va">word_dropout</span><span class="op">)</span><span class="op">:</span> <span class="fu">WordDropout</span><span class="op">(</span>p<span class="op">=</span><span class="fl">0.0</span><span class="op">)</span></span>
<span>  <span class="op">(</span><span class="va">loss_function</span><span class="op">)</span><span class="op">:</span> <span class="fu">CrossEntropyLoss</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre>
</div>
</div>
<div class="section level3">
<h3 id="training-the-model">Training the Model<a class="anchor" aria-label="anchor" href="#training-the-model"></a>
</h3>
<div style="text-align: justify;">
<p>Training the text classifier model involves two simple steps:</p>
<ul>
<li>Defining the model trainer class by passing in the classifier model
and the corpus</li>
<li>Setting off the training process passing in the required training
hyper-parameters.</li>
</ul>
<p><strong>It is worth noting that the ‘L’ in numbers like 32L and 5L is
used in R to denote that the number is an integer. Without the ‘L’
suffix, numbers in R are treated as numeric, which are by default
double-precision floating-point numbers. In contrast, Python determines
the type based on the value of the number itself. Whole numbers (e.g., 5
or 32) are of type int, while numbers with decimal points (e.g., 5.0)
are of type float. Floating-point numbers in both languages are
representations of real numbers but can have some approximation due to
the way they are stored in memory.</strong></p>
<div class="sourceCode" id="cb71"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># initiate ModelTrainer</span></span>
<span><span class="va">ModelTrainer</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_trainers.html">flair_trainers</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">ModelTrainer</span></span>
<span></span>
<span><span class="co"># fit the model</span></span>
<span><span class="va">trainer</span> <span class="op">&lt;-</span> <span class="fu">ModelTrainer</span><span class="op">(</span><span class="va">classifier</span>, <span class="va">corpus</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># start to train</span></span>
<span><span class="co"># note: the 'L' in 32L is used in R to denote that the number is an integer.</span></span>
<span><span class="va">trainer</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="st">'classifier'</span>,</span>
<span>              learning_rate<span class="op">=</span><span class="fl">0.1</span>,</span>
<span>              mini_batch_size<span class="op">=</span><span class="fl">32L</span>,</span>
<span>              <span class="co"># specifies how embeddings are stored in RAM, ie."cpu", "cuda", "gpu", "mps".</span></span>
<span>              <span class="co"># embeddings_storage_mode = "mps",</span></span>
<span>              max_epochs<span class="op">=</span><span class="fl">10L</span><span class="op">)</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:50,027 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:50,027 Model: "TextClassifier(</span></span>
<span><span class="co">#&gt;   (embeddings): DocumentPoolEmbeddings(</span></span>
<span><span class="co">#&gt;     fine_tune_mode=none, pooling=mean</span></span>
<span><span class="co">#&gt;     (embeddings): StackedEmbeddings(</span></span>
<span><span class="co">#&gt;       (list_embedding_0): WordEmbeddings(</span></span>
<span><span class="co">#&gt;         'glove'</span></span>
<span><span class="co">#&gt;         (embedding): Embedding(400001, 100)</span></span>
<span><span class="co">#&gt;       )</span></span>
<span><span class="co">#&gt;     )</span></span>
<span><span class="co">#&gt;   )</span></span>
<span><span class="co">#&gt;   (decoder): Linear(in_features=100, out_features=2, bias=True)</span></span>
<span><span class="co">#&gt;   (dropout): Dropout(p=0.0, inplace=False)</span></span>
<span><span class="co">#&gt;   (locked_dropout): LockedDropout(p=0.0)</span></span>
<span><span class="co">#&gt;   (word_dropout): WordDropout(p=0.0)</span></span>
<span><span class="co">#&gt;   (loss_function): CrossEntropyLoss()</span></span>
<span><span class="co">#&gt;   (weights): None</span></span>
<span><span class="co">#&gt;   (weight_tensor) None</span></span>
<span><span class="co">#&gt; )"</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:50,027 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:50,027 Corpus: 2025 train + 225 dev + 250 test sentences</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:50,027 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:50,027 Train:  2025 sentences</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:50,027         (train_with_dev=False, train_with_test=False)</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:50,027 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:50,027 Training Params:</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:50,027  - learning_rate: "0.1" </span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:50,027  - mini_batch_size: "32"</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:50,027  - max_epochs: "10"</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:50,027  - shuffle: "True"</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:50,027 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:50,027 Plugins:</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:50,027  - AnnealOnPlateau | patience: '3', anneal_factor: '0.5', min_learning_rate: '0.0001'</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:50,027 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:50,027 Final evaluation on model from best epoch (best-model.pt)</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:50,027  - metric: "('micro avg', 'f1-score')"</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:50,027 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:50,027 Computation:</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:50,027  - compute on device: cpu</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:50,028  - embedding storage: cpu</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:50,028 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:50,028 Model training base path: "classifier"</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:50,028 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:50,028 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:50,641 epoch 1 - iter 6/64 - loss 0.91174122 - time (sec): 0.61 - samples/sec: 312.95 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:51,585 epoch 1 - iter 12/64 - loss 0.96861458 - time (sec): 1.56 - samples/sec: 246.63 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:52,264 epoch 1 - iter 18/64 - loss 0.97410540 - time (sec): 2.24 - samples/sec: 257.61 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:52,919 epoch 1 - iter 24/64 - loss 0.98070454 - time (sec): 2.89 - samples/sec: 265.65 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:53,806 epoch 1 - iter 30/64 - loss 0.96716065 - time (sec): 3.78 - samples/sec: 254.12 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:54,538 epoch 1 - iter 36/64 - loss 0.96751174 - time (sec): 4.51 - samples/sec: 255.42 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:55,271 epoch 1 - iter 42/64 - loss 0.95211656 - time (sec): 5.24 - samples/sec: 256.31 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:55,939 epoch 1 - iter 48/64 - loss 0.95586962 - time (sec): 5.91 - samples/sec: 259.83 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:56,846 epoch 1 - iter 54/64 - loss 0.95892662 - time (sec): 6.82 - samples/sec: 253.42 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:57,552 epoch 1 - iter 60/64 - loss 0.94931542 - time (sec): 7.52 - samples/sec: 255.17 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:58,034 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:58,034 EPOCH 1 done: loss 0.9538 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:58,962 DEV : loss 0.6814143061637878 - f1-score (micro avg)  0.5511</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:59,347  - 0 epochs without improvement</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:59,349 saving best model</span></span>
<span><span class="co">#&gt; 2024-11-28 18:36:59,695 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:00,765 epoch 2 - iter 6/64 - loss 0.88309590 - time (sec): 1.07 - samples/sec: 179.56 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:01,488 epoch 2 - iter 12/64 - loss 0.91989129 - time (sec): 1.79 - samples/sec: 214.17 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:02,213 epoch 2 - iter 18/64 - loss 0.90330828 - time (sec): 2.52 - samples/sec: 228.82 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:02,949 epoch 2 - iter 24/64 - loss 0.90877422 - time (sec): 3.25 - samples/sec: 236.03 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:03,693 epoch 2 - iter 30/64 - loss 0.90456812 - time (sec): 4.00 - samples/sec: 240.15 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:04,587 epoch 2 - iter 36/64 - loss 0.90967931 - time (sec): 4.89 - samples/sec: 235.52 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:05,276 epoch 2 - iter 42/64 - loss 0.91712019 - time (sec): 5.58 - samples/sec: 240.84 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:06,033 epoch 2 - iter 48/64 - loss 0.90972924 - time (sec): 6.34 - samples/sec: 242.36 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:06,755 epoch 2 - iter 54/64 - loss 0.89838140 - time (sec): 7.06 - samples/sec: 244.79 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:07,441 epoch 2 - iter 60/64 - loss 0.89817547 - time (sec): 7.75 - samples/sec: 247.89 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:07,948 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:07,948 EPOCH 2 done: loss 0.8927 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:09,090 DEV : loss 0.6868590712547302 - f1-score (micro avg)  0.5556</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:09,459  - 0 epochs without improvement</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:09,461 saving best model</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:09,730 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:10,525 epoch 3 - iter 6/64 - loss 0.92060424 - time (sec): 0.79 - samples/sec: 241.75 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:11,266 epoch 3 - iter 12/64 - loss 0.92075667 - time (sec): 1.54 - samples/sec: 250.13 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:12,192 epoch 3 - iter 18/64 - loss 0.91408771 - time (sec): 2.46 - samples/sec: 234.01 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:12,891 epoch 3 - iter 24/64 - loss 0.90718419 - time (sec): 3.16 - samples/sec: 243.04 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:13,628 epoch 3 - iter 30/64 - loss 0.90797503 - time (sec): 3.90 - samples/sec: 246.36 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:14,372 epoch 3 - iter 36/64 - loss 0.89226388 - time (sec): 4.64 - samples/sec: 248.22 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:15,117 epoch 3 - iter 42/64 - loss 0.88858810 - time (sec): 5.39 - samples/sec: 249.53 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:16,023 epoch 3 - iter 48/64 - loss 0.88652510 - time (sec): 6.29 - samples/sec: 244.11 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:16,743 epoch 3 - iter 54/64 - loss 0.88052210 - time (sec): 7.01 - samples/sec: 246.41 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:17,409 epoch 3 - iter 60/64 - loss 0.88498759 - time (sec): 7.68 - samples/sec: 250.06 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:17,908 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:17,908 EPOCH 3 done: loss 0.8869 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:18,861 DEV : loss 0.695308268070221 - f1-score (micro avg)  0.5644</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:19,230  - 0 epochs without improvement</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:19,232 saving best model</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:19,492 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:20,518 epoch 4 - iter 6/64 - loss 0.84620678 - time (sec): 1.03 - samples/sec: 187.31 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:21,242 epoch 4 - iter 12/64 - loss 0.85321340 - time (sec): 1.75 - samples/sec: 219.52 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:21,988 epoch 4 - iter 18/64 - loss 0.85991294 - time (sec): 2.50 - samples/sec: 230.78 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:22,699 epoch 4 - iter 24/64 - loss 0.88514151 - time (sec): 3.21 - samples/sec: 239.51 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:23,585 epoch 4 - iter 30/64 - loss 0.87729849 - time (sec): 4.09 - samples/sec: 234.55 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:24,311 epoch 4 - iter 36/64 - loss 0.86967195 - time (sec): 4.82 - samples/sec: 239.06 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:25,013 epoch 4 - iter 42/64 - loss 0.86305476 - time (sec): 5.52 - samples/sec: 243.45 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:25,734 epoch 4 - iter 48/64 - loss 0.87007949 - time (sec): 6.24 - samples/sec: 246.09 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:26,673 epoch 4 - iter 54/64 - loss 0.86743812 - time (sec): 7.18 - samples/sec: 240.64 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:27,346 epoch 4 - iter 60/64 - loss 0.85504824 - time (sec): 7.85 - samples/sec: 244.47 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:27,800 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:27,800 EPOCH 4 done: loss 0.8562 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:28,730 DEV : loss 0.6955223679542542 - f1-score (micro avg)  0.5733</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:29,103  - 0 epochs without improvement</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:29,105 saving best model</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:29,366 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:30,397 epoch 5 - iter 6/64 - loss 0.79712009 - time (sec): 1.03 - samples/sec: 186.33 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:31,131 epoch 5 - iter 12/64 - loss 0.83594465 - time (sec): 1.76 - samples/sec: 217.65 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:31,856 epoch 5 - iter 18/64 - loss 0.82634424 - time (sec): 2.49 - samples/sec: 231.38 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:32,825 epoch 5 - iter 24/64 - loss 0.84943491 - time (sec): 3.46 - samples/sec: 222.05 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:33,563 epoch 5 - iter 30/64 - loss 0.83855354 - time (sec): 4.20 - samples/sec: 228.76 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:34,284 epoch 5 - iter 36/64 - loss 0.83839231 - time (sec): 4.92 - samples/sec: 234.25 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:35,015 epoch 5 - iter 42/64 - loss 0.84415555 - time (sec): 5.65 - samples/sec: 237.91 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:35,702 epoch 5 - iter 48/64 - loss 0.84729431 - time (sec): 6.34 - samples/sec: 242.42 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:36,592 epoch 5 - iter 54/64 - loss 0.84063320 - time (sec): 7.23 - samples/sec: 239.15 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:37,315 epoch 5 - iter 60/64 - loss 0.84694536 - time (sec): 7.95 - samples/sec: 241.53 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:37,574 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:37,574 EPOCH 5 done: loss 0.8443 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:38,680 DEV : loss 0.6921943426132202 - f1-score (micro avg)  0.5822</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:39,070  - 0 epochs without improvement</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:39,072 saving best model</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:39,335 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:40,378 epoch 6 - iter 6/64 - loss 0.82745030 - time (sec): 1.04 - samples/sec: 184.04 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:41,139 epoch 6 - iter 12/64 - loss 0.83163678 - time (sec): 1.80 - samples/sec: 212.85 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:41,814 epoch 6 - iter 18/64 - loss 0.83676998 - time (sec): 2.48 - samples/sec: 232.42 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:42,555 epoch 6 - iter 24/64 - loss 0.83001364 - time (sec): 3.22 - samples/sec: 238.51 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:43,479 epoch 6 - iter 30/64 - loss 0.83004258 - time (sec): 4.14 - samples/sec: 231.67 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:44,132 epoch 6 - iter 36/64 - loss 0.83843245 - time (sec): 4.80 - samples/sec: 240.18 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:44,868 epoch 6 - iter 42/64 - loss 0.82743987 - time (sec): 5.53 - samples/sec: 242.91 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:45,617 epoch 6 - iter 48/64 - loss 0.82295681 - time (sec): 6.28 - samples/sec: 244.52 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:46,477 epoch 6 - iter 54/64 - loss 0.82645491 - time (sec): 7.14 - samples/sec: 241.95 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:47,178 epoch 6 - iter 60/64 - loss 0.82694597 - time (sec): 7.84 - samples/sec: 244.81 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:47,677 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:47,677 EPOCH 6 done: loss 0.8203 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:48,595 DEV : loss 1.2241613864898682 - f1-score (micro avg)  0.4533</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:48,970  - 1 epochs without improvement</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:48,972 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:50,013 epoch 7 - iter 6/64 - loss 0.99479450 - time (sec): 1.04 - samples/sec: 184.47 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:50,746 epoch 7 - iter 12/64 - loss 0.84938438 - time (sec): 1.77 - samples/sec: 216.44 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:51,487 epoch 7 - iter 18/64 - loss 0.86148151 - time (sec): 2.51 - samples/sec: 229.07 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:52,210 epoch 7 - iter 24/64 - loss 0.88028894 - time (sec): 3.24 - samples/sec: 237.16 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:52,894 epoch 7 - iter 30/64 - loss 0.85370332 - time (sec): 3.92 - samples/sec: 244.76 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:53,850 epoch 7 - iter 36/64 - loss 0.84295499 - time (sec): 4.88 - samples/sec: 236.17 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:54,588 epoch 7 - iter 42/64 - loss 0.83776256 - time (sec): 5.62 - samples/sec: 239.34 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:55,280 epoch 7 - iter 48/64 - loss 0.84896051 - time (sec): 6.31 - samples/sec: 243.49 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:55,999 epoch 7 - iter 54/64 - loss 0.82845668 - time (sec): 7.03 - samples/sec: 245.92 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:56,879 epoch 7 - iter 60/64 - loss 0.82379258 - time (sec): 7.91 - samples/sec: 242.82 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:57,169 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:57,170 EPOCH 7 done: loss 0.8225 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:58,327 DEV : loss 0.8091477751731873 - f1-score (micro avg)  0.5556</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:58,698  - 2 epochs without improvement</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:58,699 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:37:59,513 epoch 8 - iter 6/64 - loss 0.76075288 - time (sec): 0.81 - samples/sec: 235.95 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:00,481 epoch 8 - iter 12/64 - loss 0.78303260 - time (sec): 1.78 - samples/sec: 215.48 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:01,323 epoch 8 - iter 18/64 - loss 0.77066376 - time (sec): 2.62 - samples/sec: 219.53 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:02,033 epoch 8 - iter 24/64 - loss 0.75232087 - time (sec): 3.33 - samples/sec: 230.39 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:02,739 epoch 8 - iter 30/64 - loss 0.77737618 - time (sec): 4.04 - samples/sec: 237.66 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:03,452 epoch 8 - iter 36/64 - loss 0.77567254 - time (sec): 4.75 - samples/sec: 242.37 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:04,353 epoch 8 - iter 42/64 - loss 0.78603464 - time (sec): 5.65 - samples/sec: 237.73 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:05,045 epoch 8 - iter 48/64 - loss 0.78363027 - time (sec): 6.35 - samples/sec: 242.03 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:05,770 epoch 8 - iter 54/64 - loss 0.78447807 - time (sec): 7.07 - samples/sec: 244.37 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:06,470 epoch 8 - iter 60/64 - loss 0.78428696 - time (sec): 7.77 - samples/sec: 247.07 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:06,958 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:06,958 EPOCH 8 done: loss 0.7806 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:07,883 DEV : loss 0.619835615158081 - f1-score (micro avg)  0.6489</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:08,523  - 0 epochs without improvement</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:08,525 saving best model</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:08,788 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:09,500 epoch 9 - iter 6/64 - loss 0.81246133 - time (sec): 0.71 - samples/sec: 269.81 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:10,260 epoch 9 - iter 12/64 - loss 0.76851736 - time (sec): 1.47 - samples/sec: 260.99 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:11,006 epoch 9 - iter 18/64 - loss 0.76903655 - time (sec): 2.22 - samples/sec: 259.75 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:11,931 epoch 9 - iter 24/64 - loss 0.77481109 - time (sec): 3.14 - samples/sec: 244.37 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:12,659 epoch 9 - iter 30/64 - loss 0.77168306 - time (sec): 3.87 - samples/sec: 248.05 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:13,400 epoch 9 - iter 36/64 - loss 0.77215956 - time (sec): 4.61 - samples/sec: 249.81 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:14,074 epoch 9 - iter 42/64 - loss 0.76924835 - time (sec): 5.29 - samples/sec: 254.27 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:14,838 epoch 9 - iter 48/64 - loss 0.75900491 - time (sec): 6.05 - samples/sec: 253.89 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:15,682 epoch 9 - iter 54/64 - loss 0.77255435 - time (sec): 6.89 - samples/sec: 250.69 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:16,527 epoch 9 - iter 60/64 - loss 0.77476379 - time (sec): 7.74 - samples/sec: 248.13 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:16,804 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:16,804 EPOCH 9 done: loss 0.7797 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:17,926 DEV : loss 0.7148910760879517 - f1-score (micro avg)  0.5467</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:18,303  - 1 epochs without improvement</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:18,305 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:19,308 epoch 10 - iter 6/64 - loss 0.78717700 - time (sec): 1.00 - samples/sec: 191.53 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:20,028 epoch 10 - iter 12/64 - loss 0.74860297 - time (sec): 1.72 - samples/sec: 222.86 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:20,800 epoch 10 - iter 18/64 - loss 0.73957860 - time (sec): 2.49 - samples/sec: 230.91 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:21,473 epoch 10 - iter 24/64 - loss 0.76316903 - time (sec): 3.17 - samples/sec: 242.43 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:22,421 epoch 10 - iter 30/64 - loss 0.77254111 - time (sec): 4.12 - samples/sec: 233.25 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:23,175 epoch 10 - iter 36/64 - loss 0.77815889 - time (sec): 4.87 - samples/sec: 236.54 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:23,884 epoch 10 - iter 42/64 - loss 0.76594733 - time (sec): 5.58 - samples/sec: 240.92 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:24,573 epoch 10 - iter 48/64 - loss 0.76528511 - time (sec): 6.27 - samples/sec: 245.08 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:25,484 epoch 10 - iter 54/64 - loss 0.76086970 - time (sec): 7.18 - samples/sec: 240.71 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:26,179 epoch 10 - iter 60/64 - loss 0.76908471 - time (sec): 7.87 - samples/sec: 243.85 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:26,472 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:26,472 EPOCH 10 done: loss 0.7660 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:27,617 DEV : loss 0.6605172157287598 - f1-score (micro avg)  0.5956</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:27,991  - 2 epochs without improvement</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:28,254 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:28,254 Loading model from best epoch ...</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:29,628 </span></span>
<span><span class="co">#&gt; Results:</span></span>
<span><span class="co">#&gt; - F-score (micro) 0.628</span></span>
<span><span class="co">#&gt; - F-score (macro) 0.589</span></span>
<span><span class="co">#&gt; - Accuracy 0.628</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; By class:</span></span>
<span><span class="co">#&gt;               precision    recall  f1-score   support</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     NEGATIVE     0.5680    0.9669    0.7156       121</span></span>
<span><span class="co">#&gt;     POSITIVE     0.9091    0.3101    0.4624       129</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     accuracy                         0.6280       250</span></span>
<span><span class="co">#&gt;    macro avg     0.7385    0.6385    0.5890       250</span></span>
<span><span class="co">#&gt; weighted avg     0.7440    0.6280    0.5850       250</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:29,628 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; $test_score</span></span>
<span><span class="co">#&gt; [1] 0.628</span></span></code></pre></div>
</div>
</div>
<div class="section level3">
<h3 id="loading-and-using-the-classifiers">Loading and Using the Classifiers<a class="anchor" aria-label="anchor" href="#loading-and-using-the-classifiers"></a>
</h3>
<div style="text-align: justify;">
<p>After training the text classification model, the resulting
classifier will already be stored in memory as part of the classifier
variable. It is possible, however, that your Python session exited after
training. If so, you’ll need to load the model into memory with the
following:</p>
<div class="sourceCode" id="cb72"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">TextClassifier</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_models.html">flair_models</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">TextClassifier</span></span>
<span><span class="va">classifier</span> <span class="op">&lt;-</span> <span class="va">TextClassifier</span><span class="op">$</span><span class="fu">load</span><span class="op">(</span><span class="st">'classifier/best-model.pt'</span><span class="op">)</span></span></code></pre></div>
<p>We import the Sentence object. Now, we can generate predictions on
some example text inputs.</p>
<div class="sourceCode" id="cb73"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span></code></pre></div>
<div class="sourceCode" id="cb74"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">"great"</span><span class="op">)</span></span>
<span><span class="va">classifier</span><span class="op">$</span><span class="fu">predict</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="va">labels</span><span class="op">)</span></span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; 'Sentence[1]: "great"'/'POSITIVE' (1.0)</span></span></code></pre></div>
<div class="sourceCode" id="cb75"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">"sad"</span><span class="op">)</span></span>
<span><span class="va">classifier</span><span class="op">$</span><span class="fu">predict</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="va">labels</span><span class="op">)</span></span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; 'Sentence[1]: "sad"'/'NEGATIVE' (0.6436)</span></span></code></pre></div>
</div>
<hr>
</div>
</div>
<div class="section level2">
<h2 id="training-rnns">Training RNNs<a class="anchor" aria-label="anchor" href="#training-rnns"></a>
</h2>
<div style="text-align: justify;">
<p>Here, we train a sentiment analysis model to categorize text. In this
case, we also include a pipeline that implements the use of Recurrent
Neural Networks (RNN). This makes them particularly effective for tasks
involving sequential data. This section also show you how to implement
one of most powerful features in flaiR, stacked embeddings. You can
stack multiple embeddings with different layers and let the classifier
learn from different types of features. In Flair NLP, and with the
<strong>flaiR</strong> package, it’s very easy to accomplish this
task.</p>
</div>
<div class="section level3">
<h3 id="import-necessary-modules">Import Necessary Modules<a class="anchor" aria-label="anchor" href="#import-necessary-modules"></a>
</h3>
<div style="text-align: justify;">
<div class="sourceCode" id="cb76"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://davidycliao.github.io/flaiR">flaiR</a></span><span class="op">)</span></span>
<span><span class="va">WordEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">WordEmbeddings</span></span>
<span><span class="va">FlairEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">FlairEmbeddings</span></span>
<span><span class="va">DocumentRNNEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">DocumentRNNEmbeddings</span></span>
<span><span class="va">TextClassifier</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_models.html">flair_models</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">TextClassifier</span></span>
<span><span class="va">ModelTrainer</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_trainers.html">flair_trainers</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">ModelTrainer</span></span></code></pre></div>
</div>
</div>
<div class="section level3">
<h3 id="get-the-imdb-corpus">Get the IMDB Corpus<a class="anchor" aria-label="anchor" href="#get-the-imdb-corpus"></a>
</h3>
<div style="text-align: justify;">
<p>The IMDB movie review dataset is used here, which is a commonly
utilized dataset for sentiment analysis. <code>$downsample(0.1)</code>
method means only 10% of the dataset is used, allowing for a faster
demonstration.</p>
<div class="sourceCode" id="cb77"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># load the IMDB file and downsize it to 0.1</span></span>
<span><span class="va">IMDB</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_datasets.html">flair_datasets</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">IMDB</span></span>
<span><span class="va">corpus</span> <span class="op">&lt;-</span> <span class="fu">IMDB</span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="fu">downsample</span><span class="op">(</span><span class="fl">0.1</span><span class="op">)</span> </span>
<span><span class="co">#&gt; 2024-11-28 18:38:30,027 Reading data from /Users/yenchiehliao/.flair/datasets/imdb_v4-rebalanced</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:30,027 Train: /Users/yenchiehliao/.flair/datasets/imdb_v4-rebalanced/train.txt</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:30,027 Dev: None</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:30,027 Test: None</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:30,563 No test split found. Using 10% (i.e. 5000 samples) of the train split as test data</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:30,575 No dev split found. Using 10% (i.e. 4500 samples) of the train split as dev data</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:30,575 Initialized corpus /Users/yenchiehliao/.flair/datasets/imdb_v4-rebalanced (label type name is 'sentiment')</span></span>
<span><span class="co"># create the label dictionary</span></span>
<span><span class="va">lbl_type</span> <span class="op">&lt;-</span> <span class="st">'sentiment'</span></span>
<span><span class="va">label_dict</span> <span class="op">&lt;-</span> <span class="va">corpus</span><span class="op">$</span><span class="fu">make_label_dictionary</span><span class="op">(</span>label_type<span class="op">=</span><span class="va">lbl_type</span><span class="op">)</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:30,590 Computing label dictionary. Progress:</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:37,556 Dictionary created for label 'sentiment' with 2 values: POSITIVE (seen 2056 times), NEGATIVE (seen 1994 times)</span></span></code></pre></div>
</div>
</div>
<div class="section level3">
<h3 id="stacked-embeddings">Stacked Embeddings<a class="anchor" aria-label="anchor" href="#stacked-embeddings"></a>
</h3>
<div style="text-align: justify;">
<p>This is one of Flair’s most powerful features: it allows for the
integration of embeddings to enable the model to learn from more sparse
features. Three types of embeddings are utilized here: GloVe embeddings,
and two types of Flair embeddings (forward and backward). Word
embeddings are used to convert words into vectors.</p>
<div class="sourceCode" id="cb78"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># make a list of word embeddings</span></span>
<span><span class="va">word_embeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="fu">WordEmbeddings</span><span class="op">(</span><span class="st">'glove'</span><span class="op">)</span>,</span>
<span>                        <span class="fu">FlairEmbeddings</span><span class="op">(</span><span class="st">'news-forward-fast'</span><span class="op">)</span>,</span>
<span>                        <span class="fu">FlairEmbeddings</span><span class="op">(</span><span class="st">'news-backward-fast'</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># initialize the document embeddings</span></span>
<span><span class="va">document_embeddings</span> <span class="op">&lt;-</span> <span class="fu">DocumentRNNEmbeddings</span><span class="op">(</span><span class="va">word_embeddings</span>, </span>
<span>                                             hidden_size <span class="op">=</span> <span class="fl">512L</span>,</span>
<span>                                             reproject_words <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                                             reproject_words_dimension <span class="op">=</span> <span class="fl">256L</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb79"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># create a Text Classifier with the embeddings and label dictionary</span></span>
<span><span class="va">classifier</span> <span class="op">&lt;-</span> <span class="fu">TextClassifier</span><span class="op">(</span><span class="va">document_embeddings</span>, </span>
<span>                            label_dictionary<span class="op">=</span><span class="va">label_dict</span>, label_type<span class="op">=</span><span class="st">'class'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># initialize the text classifier trainer with our corpus</span></span>
<span><span class="va">trainer</span> <span class="op">&lt;-</span> <span class="fu">ModelTrainer</span><span class="op">(</span><span class="va">classifier</span>, <span class="va">corpus</span><span class="op">)</span></span></code></pre></div>
</div>
</div>
<div class="section level3">
<h3 id="start-the-training">Start the Training<a class="anchor" aria-label="anchor" href="#start-the-training"></a>
</h3>
<div style="text-align: justify;">
<p>For the sake of this example, setting max_epochs to 5. You might want
to increase this for better performance.</p>
<p>It is worth noting that the learning rate is a parameter that
determines the step size at each iteration while moving towards a
minimum of the loss function. A smaller learning rate could slow down
the learning process, but it could lead to more precise convergence.
<code>mini_batch_size</code> determines the number of samples that will
be used to compute the gradient at each step. The ‘L’ in 32L is used in
R to denote that the number is an integer.</p>
<p><code>patience</code> (aka early stop) is a hyper-parameter used in
conjunction with early stopping to avoid overfitting. It determines the
number of epochs the training process will tolerate without improvements
before stopping the training. Setting max_epochs to 5 means the
algorithm will make five passes through the dataset.</p>
<div class="sourceCode" id="cb80"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># note: the 'L' in 32L is used in R to denote that the number is an integer.</span></span>
<span><span class="va">trainer</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="st">'models/sentiment'</span>,</span>
<span>              learning_rate<span class="op">=</span><span class="fl">0.1</span>,</span>
<span>              mini_batch_size<span class="op">=</span><span class="fl">32L</span>,</span>
<span>              patience<span class="op">=</span><span class="fl">5L</span>,</span>
<span>              max_epochs<span class="op">=</span><span class="fl">5L</span><span class="op">)</span>  </span></code></pre></div>
</div>
</div>
<div class="section level3">
<h3 id="to-apply-the-trained-model-for-prediction">To Apply the Trained Model for Prediction<a class="anchor" aria-label="anchor" href="#to-apply-the-trained-model-for-prediction"></a>
</h3>
<div style="text-align: justify;">
<div class="sourceCode" id="cb81"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="st">"This movie was really exciting!"</span></span>
<span><span class="va">classifier</span><span class="op">$</span><span class="fu">predict</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence.labels</span><span class="op">)</span></span></code></pre></div>
</div>
<hr>
</div>
</div>
<div class="section level2">
<h2 id="finetune-transformers">Finetune Transformers<a class="anchor" aria-label="anchor" href="#finetune-transformers"></a>
</h2>
<div style="text-align: justify;">
<p>We use data from <em>The Temporal Focus of Campaign Communication
(2020 JOP)</em> as an example. Let’s assume we receive the data for
training from different times. First, suppose you have a dataset of 1000
entries called <code>cc_muller_old</code>. On another day, with the help
of nice friends, you receive another set of data, adding 2000 entries in
a dataset called <code>cc_muller_new</code>. Both subsets are from
<code>data(cc_muller)</code>. We will show how to fine-tune a
transformer model with <code>cc_muller_old</code>, and then continue
with another round of fine-tuning using <code>cc_muller_new</code>.</p>
<div class="sourceCode" id="cb82"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://davidycliao.github.io/flaiR">flaiR</a></span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="fine-tuning-a-transformers-model">Fine-tuning a Transformers Model<a class="anchor" aria-label="anchor" href="#fine-tuning-a-transformers-model"></a>
</h3>
<div style="text-align: justify;">
<p><u><strong>Step 1</strong></u> Load Necessary Modules from Flair</p>
<p>Load necessary classes from <code>flair</code> package.</p>
<div class="sourceCode" id="cb83"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Sentence is a class for holding a text sentence</span></span>
<span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span>
<span></span>
<span><span class="co"># Corpus is a class for text corpora</span></span>
<span><span class="va">Corpus</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Corpus</span></span>
<span></span>
<span><span class="co"># TransformerDocumentEmbeddings is a class for loading transformer </span></span>
<span><span class="va">TransformerDocumentEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">TransformerDocumentEmbeddings</span></span>
<span></span>
<span><span class="co"># TextClassifier is a class for text classification</span></span>
<span><span class="va">TextClassifier</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_models.html">flair_models</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">TextClassifier</span></span>
<span></span>
<span><span class="co"># ModelTrainer is a class for training and evaluating models</span></span>
<span><span class="va">ModelTrainer</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_trainers.html">flair_trainers</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">ModelTrainer</span></span></code></pre></div>
<p>We use purrr to help us split sentences using Sentence from
<code><a href="../reference/flair_data.html">flair_data()</a></code>, then use map2 to add labels, and finally use
<code>Corpus</code> to segment the data.</p>
<div class="sourceCode" id="cb84"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://purrr.tidyverse.org/" class="external-link">purrr</a></span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="va">cc_muller</span><span class="op">)</span></span>
<span><span class="va">cc_muller_old</span> <span class="op">&lt;-</span> <span class="va">cc_muller</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">1000</span>,<span class="op">]</span></span>
<span></span>
<span><span class="va">old_text</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://purrr.tidyverse.org/reference/map.html" class="external-link">map</a></span><span class="op">(</span><span class="va">cc_muller_old</span><span class="op">$</span><span class="va">text</span>, <span class="va">Sentence</span><span class="op">)</span></span>
<span><span class="va">old_labels</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/character.html" class="external-link">as.character</a></span><span class="op">(</span><span class="va">cc_muller_old</span><span class="op">$</span><span class="va">class</span><span class="op">)</span></span>
<span></span>
<span><span class="va">old_text</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://purrr.tidyverse.org/reference/map2.html" class="external-link">map2</a></span><span class="op">(</span><span class="va">old_text</span>, <span class="va">old_labels</span>, <span class="op">~</span> <span class="op">{</span></span>
<span>   </span>
<span>  <span class="va">.x</span><span class="op">$</span><span class="fu">add_label</span><span class="op">(</span><span class="st">"classification"</span>, <span class="va">.y</span><span class="op">)</span></span>
<span>  <span class="va">.x</span></span>
<span><span class="op">}</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb85"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">old_text</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 1000</span></span></code></pre></div>
<div class="sourceCode" id="cb86"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">2046</span><span class="op">)</span></span>
<span><span class="va">sample</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html" class="external-link">sample</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="cn">TRUE</span>, <span class="cn">FALSE</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">old_text</span><span class="op">)</span>, replace<span class="op">=</span><span class="cn">TRUE</span>, prob<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0.8</span>, <span class="fl">0.2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">old_train</span>  <span class="op">&lt;-</span> <span class="va">old_text</span><span class="op">[</span><span class="va">sample</span><span class="op">]</span></span>
<span><span class="va">old_test</span>   <span class="op">&lt;-</span> <span class="va">old_text</span><span class="op">[</span><span class="op">!</span><span class="va">sample</span><span class="op">]</span></span>
<span></span>
<span><span class="va">test_id</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html" class="external-link">sample</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="cn">TRUE</span>, <span class="cn">FALSE</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">old_test</span><span class="op">)</span>, replace<span class="op">=</span><span class="cn">TRUE</span>, prob<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0.5</span>, <span class="fl">0.5</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">old_test</span>   <span class="op">&lt;-</span> <span class="va">old_test</span><span class="op">[</span><span class="va">test_id</span><span class="op">]</span></span>
<span><span class="va">old_dev</span>   <span class="op">&lt;-</span> <span class="va">old_test</span><span class="op">[</span><span class="op">!</span><span class="va">test_id</span><span class="op">]</span></span></code></pre></div>
<p>If you do not provide a development set (dev set) while using Flair,
it will automatically split the training data into training and
development datasets. The test set is used for training the model and
evaluating its final performance, whereas the development set is used
for adjusting model parameters and preventing overfitting, or in other
words, for early stopping of the model.</p>
<div class="sourceCode" id="cb87"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">old_corpus</span> <span class="op">&lt;-</span> <span class="fu">Corpus</span><span class="op">(</span>train <span class="op">=</span> <span class="va">old_train</span>, test <span class="op">=</span> <span class="va">old_test</span><span class="op">)</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:39,938 No dev split found. Using 10% (i.e. 80 samples) of the train split as dev data</span></span></code></pre></div>
<p><u><strong>Step 3</strong></u> Load <code>distilbert</code>
Transformer</p>
<div class="sourceCode" id="cb88"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">document_embeddings</span> <span class="op">&lt;-</span> <span class="fu">TransformerDocumentEmbeddings</span><span class="op">(</span><span class="st">'distilbert-base-uncased'</span>, fine_tune<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<p>First, the <code>$make_label_dictionary</code> function is used to
automatically create a label dictionary for the classification task. The
label dictionary is a mapping from label to index, which is used to map
the labels to a tensor of label indices. Besides classification tasks,
flaiR also supports other label types for training custom model, such as
<code>ner</code>, <code>pos</code> and <code>sentiment</code>. From the
cc_muller dataset: Future (seen 423 times), Present (seen 262 times),
Past (seen 131 times)</p>
<div class="sourceCode" id="cb89"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">old_label_dict</span> <span class="op">&lt;-</span> <span class="va">old_corpus</span><span class="op">$</span><span class="fu">make_label_dictionary</span><span class="op">(</span>label_type<span class="op">=</span><span class="st">"classification"</span><span class="op">)</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:41,010 Computing label dictionary. Progress:</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:41,015 Dictionary created for label 'classification' with 3 values: Future (seen 380 times), Present (seen 232 times), Past (seen 111 times)</span></span></code></pre></div>
<p><code>TextClassifier</code> is used to create a text classifier. The
classifier takes the document embeddings (importing from
<code>'distilbert-base-uncased'</code> from Hugging Face) and the label
dictionary as input. The label type is also specified as
classification.</p>
<div class="sourceCode" id="cb90"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">old_classifier</span> <span class="op">&lt;-</span> <span class="fu">TextClassifier</span><span class="op">(</span><span class="va">document_embeddings</span>,</span>
<span>                                 label_dictionary <span class="op">=</span> <span class="va">old_label_dict</span>, </span>
<span>                                 label_type<span class="op">=</span><span class="st">'classification'</span><span class="op">)</span></span></code></pre></div>
<p><u><strong>Step 4</strong></u> Start Training</p>
<p><code>ModelTrainer</code> is used to train the model.</p>
<div class="sourceCode" id="cb91"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">old_trainer</span> <span class="op">&lt;-</span> <span class="fu">ModelTrainer</span><span class="op">(</span>model <span class="op">=</span> <span class="va">old_classifier</span>, corpus <span class="op">=</span> <span class="va">old_corpus</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb92"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">old_trainer</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="st">"vignettes/inst/muller-campaign-communication"</span>,  </span>
<span>                  learning_rate<span class="op">=</span><span class="fl">0.02</span>,              </span>
<span>                  mini_batch_size<span class="op">=</span><span class="fl">8L</span>,              </span>
<span>                  anneal_with_restarts <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                  save_final_model<span class="op">=</span><span class="cn">TRUE</span>,</span>
<span>                  max_epochs<span class="op">=</span><span class="fl">1L</span><span class="op">)</span>   </span>
<span><span class="co">#&gt; 2024-11-28 18:38:41,137 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:41,137 Model: "TextClassifier(</span></span>
<span><span class="co">#&gt;   (embeddings): TransformerDocumentEmbeddings(</span></span>
<span><span class="co">#&gt;     (model): DistilBertModel(</span></span>
<span><span class="co">#&gt;       (embeddings): Embeddings(</span></span>
<span><span class="co">#&gt;         (word_embeddings): Embedding(30523, 768, padding_idx=0)</span></span>
<span><span class="co">#&gt;         (position_embeddings): Embedding(512, 768)</span></span>
<span><span class="co">#&gt;         (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)</span></span>
<span><span class="co">#&gt;         (dropout): Dropout(p=0.1, inplace=False)</span></span>
<span><span class="co">#&gt;       )</span></span>
<span><span class="co">#&gt;       (transformer): Transformer(</span></span>
<span><span class="co">#&gt;         (layer): ModuleList(</span></span>
<span><span class="co">#&gt;           (0-5): 6 x TransformerBlock(</span></span>
<span><span class="co">#&gt;             (attention): MultiHeadSelfAttention(</span></span>
<span><span class="co">#&gt;               (dropout): Dropout(p=0.1, inplace=False)</span></span>
<span><span class="co">#&gt;               (q_lin): Linear(in_features=768, out_features=768, bias=True)</span></span>
<span><span class="co">#&gt;               (k_lin): Linear(in_features=768, out_features=768, bias=True)</span></span>
<span><span class="co">#&gt;               (v_lin): Linear(in_features=768, out_features=768, bias=True)</span></span>
<span><span class="co">#&gt;               (out_lin): Linear(in_features=768, out_features=768, bias=True)</span></span>
<span><span class="co">#&gt;             )</span></span>
<span><span class="co">#&gt;             (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)</span></span>
<span><span class="co">#&gt;             (ffn): FFN(</span></span>
<span><span class="co">#&gt;               (dropout): Dropout(p=0.1, inplace=False)</span></span>
<span><span class="co">#&gt;               (lin1): Linear(in_features=768, out_features=3072, bias=True)</span></span>
<span><span class="co">#&gt;               (lin2): Linear(in_features=3072, out_features=768, bias=True)</span></span>
<span><span class="co">#&gt;               (activation): GELUActivation()</span></span>
<span><span class="co">#&gt;             )</span></span>
<span><span class="co">#&gt;             (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)</span></span>
<span><span class="co">#&gt;           )</span></span>
<span><span class="co">#&gt;         )</span></span>
<span><span class="co">#&gt;       )</span></span>
<span><span class="co">#&gt;     )</span></span>
<span><span class="co">#&gt;   )</span></span>
<span><span class="co">#&gt;   (decoder): Linear(in_features=768, out_features=3, bias=True)</span></span>
<span><span class="co">#&gt;   (dropout): Dropout(p=0.0, inplace=False)</span></span>
<span><span class="co">#&gt;   (locked_dropout): LockedDropout(p=0.0)</span></span>
<span><span class="co">#&gt;   (word_dropout): WordDropout(p=0.0)</span></span>
<span><span class="co">#&gt;   (loss_function): CrossEntropyLoss()</span></span>
<span><span class="co">#&gt;   (weights): None</span></span>
<span><span class="co">#&gt;   (weight_tensor) None</span></span>
<span><span class="co">#&gt; )"</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:41,137 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:41,137 Corpus: 723 train + 80 dev + 85 test sentences</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:41,137 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:41,137 Train:  723 sentences</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:41,138         (train_with_dev=False, train_with_test=False)</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:41,138 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:41,138 Training Params:</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:41,138  - learning_rate: "0.02" </span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:41,138  - mini_batch_size: "8"</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:41,138  - max_epochs: "1"</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:41,138  - shuffle: "True"</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:41,138 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:41,138 Plugins:</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:41,138  - AnnealOnPlateau | patience: '3', anneal_factor: '0.5', min_learning_rate: '0.0001'</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:41,138 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:41,138 Final evaluation on model from best epoch (best-model.pt)</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:41,138  - metric: "('micro avg', 'f1-score')"</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:41,138 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:41,138 Computation:</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:41,138  - compute on device: cpu</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:41,138  - embedding storage: cpu</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:41,138 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:41,138 Model training base path: "vignettes/inst/muller-campaign-communication"</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:41,138 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:41,138 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:43,508 epoch 1 - iter 9/91 - loss 1.20928653 - time (sec): 2.37 - samples/sec: 30.38 - lr: 0.020000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:46,013 epoch 1 - iter 18/91 - loss 1.01873306 - time (sec): 4.87 - samples/sec: 29.54 - lr: 0.020000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:48,557 epoch 1 - iter 27/91 - loss 0.95770972 - time (sec): 7.42 - samples/sec: 29.12 - lr: 0.020000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:51,102 epoch 1 - iter 36/91 - loss 0.85857756 - time (sec): 9.96 - samples/sec: 28.91 - lr: 0.020000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:53,476 epoch 1 - iter 45/91 - loss 0.82960190 - time (sec): 12.34 - samples/sec: 29.18 - lr: 0.020000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:55,976 epoch 1 - iter 54/91 - loss 0.79808861 - time (sec): 14.84 - samples/sec: 29.12 - lr: 0.020000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:38:58,356 epoch 1 - iter 63/91 - loss 0.76187870 - time (sec): 17.22 - samples/sec: 29.27 - lr: 0.020000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:39:00,997 epoch 1 - iter 72/91 - loss 0.74997700 - time (sec): 19.86 - samples/sec: 29.01 - lr: 0.020000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:39:03,750 epoch 1 - iter 81/91 - loss 0.68867619 - time (sec): 22.61 - samples/sec: 28.66 - lr: 0.020000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:39:06,129 epoch 1 - iter 90/91 - loss 0.67390569 - time (sec): 24.99 - samples/sec: 28.81 - lr: 0.020000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:39:06,267 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:39:06,267 EPOCH 1 done: loss 0.6712 - lr: 0.020000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:39:07,263 DEV : loss 0.44294458627700806 - f1-score (micro avg)  0.875</span></span>
<span><span class="co">#&gt; 2024-11-28 18:39:07,264  - 0 epochs without improvement</span></span>
<span><span class="co">#&gt; 2024-11-28 18:39:07,266 saving best model</span></span>
<span><span class="co">#&gt; 2024-11-28 18:39:07,940 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:39:07,943 Loading model from best epoch ...</span></span>
<span><span class="co">#&gt; 2024-11-28 18:39:10,079 </span></span>
<span><span class="co">#&gt; Results:</span></span>
<span><span class="co">#&gt; - F-score (micro) 0.8471</span></span>
<span><span class="co">#&gt; - F-score (macro) 0.8356</span></span>
<span><span class="co">#&gt; - Accuracy 0.8471</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; By class:</span></span>
<span><span class="co">#&gt;               precision    recall  f1-score   support</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;       Future     0.8333    0.9302    0.8791        43</span></span>
<span><span class="co">#&gt;      Present     0.8696    0.7407    0.8000        27</span></span>
<span><span class="co">#&gt;         Past     0.8571    0.8000    0.8276        15</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     accuracy                         0.8471        85</span></span>
<span><span class="co">#&gt;    macro avg     0.8533    0.8237    0.8356        85</span></span>
<span><span class="co">#&gt; weighted avg     0.8490    0.8471    0.8449        85</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; 2024-11-28 18:39:10,079 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; $test_score</span></span>
<span><span class="co">#&gt; [1] 0.8470588</span></span></code></pre></div>
</div>
</div>
<div class="section level3">
<h3 id="continue-fine-tuning-with-new-dataset">Continue Fine-tuning with New Dataset<a class="anchor" aria-label="anchor" href="#continue-fine-tuning-with-new-dataset"></a>
</h3>
<div style="text-align: justify;">
<p>Now, we can continue to fine tune the already fine tuned model with
an additional 2000 pieces of data. First, let’s say we have another 2000
entries called <code>cc_muller_new</code>. We can fine-tune the previous
model with these 2000 entries. The steps are the same as before. For
this case, we don’t need to split the dataset again. We can use the
entire 2000 entries as the training set and use the
<code>old_test</code> set to evaluate how well our refined model
performs.</p>
<p><u><strong>Step 1</strong></u> Load the
<code>muller-campaign-communication</code> Model</p>
<p>Load the model (<code>old_model</code>) you have already fine tuned
from previous stage and let’s fine tune it with the new data,
<code>new_corpus</code>.</p>
<div class="sourceCode" id="cb93"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">old_model</span> <span class="op">&lt;-</span> <span class="va">TextClassifier</span><span class="op">$</span><span class="fu">load</span><span class="op">(</span><span class="st">"vignettes/inst/muller-campaign-communication/best-model.pt"</span><span class="op">)</span></span></code></pre></div>
<p><u><strong>Step 2</strong></u> Convert the New Data to Sentence and
Corpus</p>
<div class="sourceCode" id="cb94"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://purrr.tidyverse.org/" class="external-link">purrr</a></span><span class="op">)</span></span>
<span><span class="va">cc_muller_new</span> <span class="op">&lt;-</span> <span class="va">cc_muller</span><span class="op">[</span><span class="fl">1001</span><span class="op">:</span><span class="fl">3000</span>,<span class="op">]</span></span>
<span><span class="va">new_text</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://purrr.tidyverse.org/reference/map.html" class="external-link">map</a></span><span class="op">(</span><span class="va">cc_muller_new</span><span class="op">$</span><span class="va">text</span>, <span class="va">Sentence</span><span class="op">)</span></span>
<span><span class="va">new_labels</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/character.html" class="external-link">as.character</a></span><span class="op">(</span><span class="va">cc_muller_new</span><span class="op">$</span><span class="va">class</span><span class="op">)</span></span>
<span></span>
<span><span class="va">new_text</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://purrr.tidyverse.org/reference/map2.html" class="external-link">map2</a></span><span class="op">(</span><span class="va">new_text</span>, <span class="va">new_labels</span>, <span class="op">~</span> <span class="op">{</span></span>
<span>  <span class="va">.x</span><span class="op">$</span><span class="fu">add_label</span><span class="op">(</span><span class="st">"classification"</span>, <span class="va">.y</span><span class="op">)</span></span>
<span>  <span class="va">.x</span></span>
<span><span class="op">}</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb95"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">new_corpus</span> <span class="op">&lt;-</span> <span class="fu">Corpus</span><span class="op">(</span>train<span class="op">=</span><span class="va">new_text</span>, test<span class="op">=</span><span class="va">old_test</span><span class="op">)</span></span>
<span><span class="co">#&gt; 2024-11-28 18:39:11,624 No dev split found. Using 10% (i.e. 200 samples) of the train split as dev data</span></span></code></pre></div>
<p><u><strong>Step 3</strong></u> Create a New Model Trainer with the
Old Model and New Corpus</p>
<div class="sourceCode" id="cb96"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">new_trainer</span> <span class="op">&lt;-</span> <span class="fu">ModelTrainer</span><span class="op">(</span><span class="va">old_model</span>, <span class="va">new_corpus</span><span class="op">)</span></span></code></pre></div>
<p><u><strong>Step 4</strong></u> Train the New Model</p>
<div class="sourceCode" id="cb97"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">new_trainer</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="st">"vignettes/inst/new-muller-campaign-communication"</span>,</span>
<span>                  learning_rate<span class="op">=</span><span class="fl">0.002</span>, </span>
<span>                  mini_batch_size<span class="op">=</span><span class="fl">8L</span>,  </span>
<span>                  max_epochs<span class="op">=</span><span class="fl">1L</span><span class="op">)</span>    </span>
<span><span class="co">#&gt; 2024-11-28 18:39:11,707 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:39:11,708 Model: "TextClassifier(</span></span>
<span><span class="co">#&gt;   (embeddings): TransformerDocumentEmbeddings(</span></span>
<span><span class="co">#&gt;     (model): DistilBertModel(</span></span>
<span><span class="co">#&gt;       (embeddings): Embeddings(</span></span>
<span><span class="co">#&gt;         (word_embeddings): Embedding(30523, 768, padding_idx=0)</span></span>
<span><span class="co">#&gt;         (position_embeddings): Embedding(512, 768)</span></span>
<span><span class="co">#&gt;         (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)</span></span>
<span><span class="co">#&gt;         (dropout): Dropout(p=0.1, inplace=False)</span></span>
<span><span class="co">#&gt;       )</span></span>
<span><span class="co">#&gt;       (transformer): Transformer(</span></span>
<span><span class="co">#&gt;         (layer): ModuleList(</span></span>
<span><span class="co">#&gt;           (0-5): 6 x TransformerBlock(</span></span>
<span><span class="co">#&gt;             (attention): MultiHeadSelfAttention(</span></span>
<span><span class="co">#&gt;               (dropout): Dropout(p=0.1, inplace=False)</span></span>
<span><span class="co">#&gt;               (q_lin): Linear(in_features=768, out_features=768, bias=True)</span></span>
<span><span class="co">#&gt;               (k_lin): Linear(in_features=768, out_features=768, bias=True)</span></span>
<span><span class="co">#&gt;               (v_lin): Linear(in_features=768, out_features=768, bias=True)</span></span>
<span><span class="co">#&gt;               (out_lin): Linear(in_features=768, out_features=768, bias=True)</span></span>
<span><span class="co">#&gt;             )</span></span>
<span><span class="co">#&gt;             (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)</span></span>
<span><span class="co">#&gt;             (ffn): FFN(</span></span>
<span><span class="co">#&gt;               (dropout): Dropout(p=0.1, inplace=False)</span></span>
<span><span class="co">#&gt;               (lin1): Linear(in_features=768, out_features=3072, bias=True)</span></span>
<span><span class="co">#&gt;               (lin2): Linear(in_features=3072, out_features=768, bias=True)</span></span>
<span><span class="co">#&gt;               (activation): GELUActivation()</span></span>
<span><span class="co">#&gt;             )</span></span>
<span><span class="co">#&gt;             (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)</span></span>
<span><span class="co">#&gt;           )</span></span>
<span><span class="co">#&gt;         )</span></span>
<span><span class="co">#&gt;       )</span></span>
<span><span class="co">#&gt;     )</span></span>
<span><span class="co">#&gt;   )</span></span>
<span><span class="co">#&gt;   (decoder): Linear(in_features=768, out_features=3, bias=True)</span></span>
<span><span class="co">#&gt;   (dropout): Dropout(p=0.0, inplace=False)</span></span>
<span><span class="co">#&gt;   (locked_dropout): LockedDropout(p=0.0)</span></span>
<span><span class="co">#&gt;   (word_dropout): WordDropout(p=0.0)</span></span>
<span><span class="co">#&gt;   (loss_function): CrossEntropyLoss()</span></span>
<span><span class="co">#&gt;   (weights): None</span></span>
<span><span class="co">#&gt;   (weight_tensor) None</span></span>
<span><span class="co">#&gt; )"</span></span>
<span><span class="co">#&gt; 2024-11-28 18:39:11,708 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:39:11,708 Corpus: 1800 train + 200 dev + 85 test sentences</span></span>
<span><span class="co">#&gt; 2024-11-28 18:39:11,708 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:39:11,708 Train:  1800 sentences</span></span>
<span><span class="co">#&gt; 2024-11-28 18:39:11,708         (train_with_dev=False, train_with_test=False)</span></span>
<span><span class="co">#&gt; 2024-11-28 18:39:11,708 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:39:11,708 Training Params:</span></span>
<span><span class="co">#&gt; 2024-11-28 18:39:11,708  - learning_rate: "0.002" </span></span>
<span><span class="co">#&gt; 2024-11-28 18:39:11,708  - mini_batch_size: "8"</span></span>
<span><span class="co">#&gt; 2024-11-28 18:39:11,708  - max_epochs: "1"</span></span>
<span><span class="co">#&gt; 2024-11-28 18:39:11,708  - shuffle: "True"</span></span>
<span><span class="co">#&gt; 2024-11-28 18:39:11,708 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:39:11,708 Plugins:</span></span>
<span><span class="co">#&gt; 2024-11-28 18:39:11,708  - AnnealOnPlateau | patience: '3', anneal_factor: '0.5', min_learning_rate: '0.0001'</span></span>
<span><span class="co">#&gt; 2024-11-28 18:39:11,708 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:39:11,708 Final evaluation on model from best epoch (best-model.pt)</span></span>
<span><span class="co">#&gt; 2024-11-28 18:39:11,708  - metric: "('micro avg', 'f1-score')"</span></span>
<span><span class="co">#&gt; 2024-11-28 18:39:11,708 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:39:11,708 Computation:</span></span>
<span><span class="co">#&gt; 2024-11-28 18:39:11,708  - compute on device: cpu</span></span>
<span><span class="co">#&gt; 2024-11-28 18:39:11,708  - embedding storage: cpu</span></span>
<span><span class="co">#&gt; 2024-11-28 18:39:11,708 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:39:11,709 Model training base path: "vignettes/inst/new-muller-campaign-communication"</span></span>
<span><span class="co">#&gt; 2024-11-28 18:39:11,709 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:39:11,709 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:39:18,581 epoch 1 - iter 22/225 - loss 0.50694836 - time (sec): 6.87 - samples/sec: 25.61 - lr: 0.002000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:39:25,069 epoch 1 - iter 44/225 - loss 0.46326165 - time (sec): 13.36 - samples/sec: 26.35 - lr: 0.002000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:39:31,083 epoch 1 - iter 66/225 - loss 0.47215962 - time (sec): 19.37 - samples/sec: 27.25 - lr: 0.002000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:39:37,161 epoch 1 - iter 88/225 - loss 0.42928298 - time (sec): 25.45 - samples/sec: 27.66 - lr: 0.002000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:39:43,058 epoch 1 - iter 110/225 - loss 0.42027618 - time (sec): 31.35 - samples/sec: 28.07 - lr: 0.002000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:39:49,055 epoch 1 - iter 132/225 - loss 0.41898603 - time (sec): 37.35 - samples/sec: 28.28 - lr: 0.002000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:39:55,671 epoch 1 - iter 154/225 - loss 0.40655317 - time (sec): 43.96 - samples/sec: 28.02 - lr: 0.002000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:40:03,002 epoch 1 - iter 176/225 - loss 0.40555196 - time (sec): 51.29 - samples/sec: 27.45 - lr: 0.002000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:40:09,632 epoch 1 - iter 198/225 - loss 0.40328091 - time (sec): 57.92 - samples/sec: 27.35 - lr: 0.002000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:40:16,328 epoch 1 - iter 220/225 - loss 0.39744571 - time (sec): 64.62 - samples/sec: 27.24 - lr: 0.002000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:40:17,686 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:40:17,686 EPOCH 1 done: loss 0.3989 - lr: 0.002000</span></span>
<span><span class="co">#&gt; 2024-11-28 18:40:20,414 DEV : loss 0.44265758991241455 - f1-score (micro avg)  0.85</span></span>
<span><span class="co">#&gt; 2024-11-28 18:40:20,417  - 0 epochs without improvement</span></span>
<span><span class="co">#&gt; 2024-11-28 18:40:20,418 saving best model</span></span>
<span><span class="co">#&gt; 2024-11-28 18:40:21,072 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-11-28 18:40:21,073 Loading model from best epoch ...</span></span>
<span><span class="co">#&gt; 2024-11-28 18:40:23,248 </span></span>
<span><span class="co">#&gt; Results:</span></span>
<span><span class="co">#&gt; - F-score (micro) 0.8824</span></span>
<span><span class="co">#&gt; - F-score (macro) 0.8804</span></span>
<span><span class="co">#&gt; - Accuracy 0.8824</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; By class:</span></span>
<span><span class="co">#&gt;               precision    recall  f1-score   support</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;       Future     0.9268    0.8837    0.9048        43</span></span>
<span><span class="co">#&gt;      Present     0.7812    0.9259    0.8475        27</span></span>
<span><span class="co">#&gt;         Past     1.0000    0.8000    0.8889        15</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     accuracy                         0.8824        85</span></span>
<span><span class="co">#&gt;    macro avg     0.9027    0.8699    0.8804        85</span></span>
<span><span class="co">#&gt; weighted avg     0.8935    0.8824    0.8838        85</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; 2024-11-28 18:40:23,248 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; $test_score</span></span>
<span><span class="co">#&gt; [1] 0.8823529</span></span></code></pre></div>
<div class="section level3">
<h3 id="model-performance-metrics-pre-and-post-fine-tuning">Model Performance Metrics: Pre and Post Fine-tuning<a class="anchor" aria-label="anchor" href="#model-performance-metrics-pre-and-post-fine-tuning"></a>
</h3>
<p>After fine-tuning for 1 epoch, the model showed improved performance
on the same test set.</p>
<table class="table">
<thead><tr class="header">
<th>Evaluation Metric</th>
<th>Pre-finetune</th>
<th>Post-finetune</th>
<th>Improvement</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>F-score (micro)</td>
<td>0.7294</td>
<td>0.8471</td>
<td>+0.1177</td>
</tr>
<tr class="even">
<td>F-score (macro)</td>
<td>0.7689</td>
<td>0.8583</td>
<td>+0.0894</td>
</tr>
<tr class="odd">
<td>Accuracy</td>
<td>0.7294</td>
<td>0.8471</td>
<td>+0.1177</td>
</tr>
</tbody>
</table>
<p>More R tutorial and documentation see <a href="https://github.com/davidycliao/flaiR" class="external-link">here</a>.</p>
</div>
</div>
</div>
</div>

  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



   </div>
  <footer><div class="container">
  <div class="pkgdown-footer-left">
  <p>Developed by <a href="https://davidycliao.github.io" class="external-link">Yen-Chieh Liao</a>, <a href="https://muellerstefan.net" class="external-link">Stefan Müller</a>, Akbik Alan, Blythe Duncan, Vollgraf Roland.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

  </div></footer>
</body>
</html>
