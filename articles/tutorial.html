<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="flaiR">
<title>Tutorials • flaiR</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/Source_Sans_Pro-0.4.9/font.css" rel="stylesheet">
<link href="../deps/Source_Code_Pro-0.4.9/font.css" rel="stylesheet">
<!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.11/clipboard.min.js" integrity="sha512-7O5pXpc0oCRrxk8RUfDYFgn0nO1t+jLuIOQdOMRp4APB7uZ4vSjspzp5y6YDtDs4VzUSTbWzBFZ/LKJhnyFOKw==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Tutorials">
<meta property="og:description" content="flaiR">
<meta property="og:image" content="https://davidycliao.github.io/flaiR/logo.png">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]--><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=G-3ZG40PPG98"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-3ZG40PPG98');
</script><script defer data-domain="{YOUR DOMAIN},all.tidyverse.org" src="https://plausible.io/js/plausible.js"></script>
</head>
<body>
    <a href="#container" class="visually-hidden-focusable">Skip to content</a>
    

    <nav class="navbar fixed-top navbar-inverse navbar-expand-lg bg-none" data-bs-theme="inverse"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">flaiR</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Released version">0.0.6</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../index.html">
    <span class="fa fa-home"></span>
     
  </a>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown--quick-start">
    <span class="fa fa-rocket"></span>
     
    Quick Start
  </a>
  <div class="dropdown-menu" aria-labelledby="dropdown--quick-start">
    <a class="dropdown-item" href="../articles/quickstart.html#why-write-flaiR-to-access-fair-nlp-in-python">Why flaiR</a>
    <a class="dropdown-item" href="../articles/quickstart.html#install-flair-with-using-remotes">Install flaiR via GitHub</a>
    <a class="dropdown-item" href="../articles/quickstart.html#class-and-ojbect-in-r-via-flair">Class and Ojbect in R</a>
    <a class="dropdown-item" href="../articles/quickstart.html#embeddings">FlaiR Embeddings</a>
    <a class="dropdown-item" href="../articles/quickstart.html#expanded-feats-for-nlp-tasks-in-flair">Feats in flaiR</a>
    <a class="dropdown-item" href="../articles/quickstart.html#how-to-contribute">How to Contribute</a>
  </div>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown--tutorials">
    <span class="fa fa-project-diagram"></span>
     
    Tutorials
  </a>
  <div class="dropdown-menu" aria-labelledby="dropdown--tutorials">
    <a class="dropdown-item" href="../articles/tutorial.html#introduction">Introduction</a>
    <a class="dropdown-item" href="../articles/tutorial.html#sentence-and-token">Sentence and Token</a>
    <a class="dropdown-item" href="../articles/tutorial.html#sequence-taggings">Sequence Taggings</a>
    <a class="dropdown-item" href="../articles/tutorial.html#performing-ner-tasks">Performing NER Tasks</a>
    <a class="dropdown-item" href="../articles/tutorial.html#flair-embedding">Flair Embeddings</a>
    <a class="dropdown-item" href="../articles/tutorial.html#training-a-binary-classifier">Training a Binary Classifier</a>
    <a class="dropdown-item" href="../articles/tutorial.html#training-rnns">Training RNNs</a>
    <a class="dropdown-item" href="../articles/tutorial.html#finetune-transformers">Finetune Transformers </a>
  </div>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown--expanded-feats">
    <span class="fa fa-newspaper-o"></span>
     
    Expanded Feats
  </a>
  <div class="dropdown-menu" aria-labelledby="dropdown--expanded-feats">
    <a class="dropdown-item" href="../articles/get_pos.html">Part-of-speech Tagging</a>
    <a class="dropdown-item" href="../articles/get_entities.html">Named Entity Recognition</a>
    <a class="dropdown-item" href="../articles/get_sentiments.html">Tagging Sentiment</a>
    <a class="dropdown-item" href="../articles/highlight_text.html">The Coloring Entities</a>
    <h6 class="dropdown-header" data-toc-skip>Visualizing the Sentiments (in progress)</h6>
  </div>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown--reference">
    <span class="fa fa-file-code-o"></span>
     
    Reference
  </a>
  <div class="dropdown-menu" aria-labelledby="dropdown--reference">
    <a class="dropdown-item" href="../reference/index.html">All Function Reference</a>
  </div>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown--news">
    <span class="fa fa-newspaper-o"></span>
     
    News
  </a>
  <div class="dropdown-menu" aria-labelledby="dropdown--news">
    <a class="dropdown-item" href="../news/index.html#flair-006-2023-10-29">0.0.6</a>
    <a class="dropdown-item" href="../news/index.html#flair-005-2023-10-01">0.0.5</a>
    <a class="dropdown-item" href="../news/index.html#flair-003-2023-09-10">0.0.3</a>
    <a class="dropdown-item" href="../news/index.html#flair-001-development-version">0.0.1</a>
  </div>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/davidycliao/flaiR">
    <span class="fa fa-github fa-lg"></span>
     
    GitHub
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article" id="container">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>Tutorials</h1>
                        <h4 data-toc-skip class="author">David
(Yen-Chieh) Liao</h4>
            <address class="author_afil">
      Postdoc at Text &amp; Policy Research Group and SPIRe in
UCD<br><small class="dont-index">Source: <a href="https://github.com/davidycliao/flaiR/blob/HEAD/vignettes/tutorial.Rmd" class="external-link"><code>vignettes/tutorial.Rmd</code></a></small>
      <div class="d-none name"><code>tutorial.Rmd</code></div>
    </address>
</div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<div style="text-align: justify">
<p><strong>Flair NLP</strong> is an open-source library for Natural
Language Processing (NLP) developed by <a href="https://github.com/zalandoresearch/" class="external-link">Zalando Research</a>. Known
for its state-of-the-art solutions for NLP tasks like Named Entity
Recognition (NER), Part-of-Speech tagging (POS), and more, it has
garnered attention in the NLP community for its ease of use and powerful
functionalities. Developed in Python, and built on the PyTorch
framework, it offers a flexible and dynamic approach to deal with
textual data. On the other hand, {flaiR} in R aims to continue the
framework established by Flair in Python by creating a framework for R,
thereby extending Flair’s capabilities to the R programming
environment.</p>
<p>One of the hallmark features of Flair is its <strong>contextual
string embeddings</strong>, which is crucial in discerning the meaning
of words in different contextual usages. Traditional embeddings assign a
fixed vector to a word, without considering its context, which can be a
limitation when trying to understand the nuances in the word’s usage
across different sentences. On the contrary, Flair’s contextual
embeddings generate word vectors by considering the surrounding text,
thus capturing the word’s context and semantics more accurately. This is
particularly impactful in scenarios where a word can have different
meanings based on its usage.</p>
<p>Flair offers pre-trained models for various languages and tasks,
providing a solid foundation for various NLP applications such as text
classification, sentiment analysis, and entity recognition, etc. For
instance, if you’re involved in a project that requires identifying
persons, organizations, or locations from text, Flair has pre-trained
NER models that can simplify this task.</p>
</div>
<div class="section level3">
<h3 id="oop-in-r-when-introducing-python-module">OOP in R when Introducing Python Module<a class="anchor" aria-label="anchor" href="#oop-in-r-when-introducing-python-module"></a>
</h3>
<div style="text-align: justify">
<p>Object-Oriented Programming (OOP) is a programming paradigm that uses
objects, which contain both data (attributes) and functions (methods),
to design applications and software. The idea is to bind the data and
the methods that operate on that data into one single unit, an object.
Before the advent of R6, OOP was not very common in the early stages of
R. To my knowledge, R6 is relatively rare; aside from {<a href="https://mlr3.mlr-org.com" class="external-link">mlr3</a>}, which is written in R6, most
packages are accomplished in <a href="http://adv-r.had.co.nz/OO-essentials.html#s3" class="external-link">S4</a> and <a href="http://adv-r.had.co.nz/OO-essentials.html#s4" class="external-link">S3</a> (to my
personal experience), which, of course, may be greatly related to the
habits and tasks of R users. However, the purpose of
{<code>flaiR</code>} is to standardize wrapping the ‘{flair NLP}’ Python
functionality in R and to provide more convenient access for R users to
utilize flair NLP features. Most usage of Flair NLP within the {flaiR}
employs concepts of objects and classes, which are similar to <a href="https://r6.r-lib.org/articles/Introduction.html" class="external-link">R6</a> or .</p>
<p>However, these features are packaged in {reticulate} from Python. In
other words, some functionalities imported into R essentially belong to
Python classes or modules. In the {<code>flairR</code>} architecture, we
use the simplest <a href="http://adv-r.had.co.nz/OO-essentials.html#s3" class="external-link">S3</a> method to
wrap both modules and methods within those modules, allowing R users to
conveniently access and use Python functionalities.</p>
<!-- In addition, tensors serve as a fundamental building block for creating and training neural networks and for conducting various numerical computations in Python. For all of Flair's NLP tasks in Python on PyTorch, there are numerous extensive functionalities for tensor operations, including element-wise operations, matrix multiplications, and reshaping. In this tutorial, we also cover how to work with tensors in R and how to convert tensors into matrices in the R environment. This is particularly important when using Flair word embeddings in R environment. -->
</div>
<p> </p>
</div>
<div class="section level3">
<h3 id="the-overview">The Overview<a class="anchor" aria-label="anchor" href="#the-overview"></a>
</h3>
<div style="text-align: justify">
<p>The following tutorial is mainly based on Tadej Magajna’s ‘<a href="https://github.com/PacktPublishing/Natural-Language-Processing-with-Flair" class="external-link"><strong>Natural
Language Processing with Flair: A Practical Guide to Understanding and
Solving NLP Problems</strong></a>’, as well as the official <a href="https://flairnlp.github.io/docs/intro" class="external-link">Flair NLP</a> Python
tutorial and blog. both are written in Python. If you utilize the
examples from {flaiR} in R , I welcome you to cite this R repository,
but you should also cite their works. Except when necessary, everything
will be accomplished within the R environment, utilizing several uesful
R packages, such as {<a href="https://quanteda.org" class="external-link">quanteda</a>}, {<a href="https://bnosac.github.io/udpipe/en/" class="external-link"><code>udpipe</code></a>}, and
{<a href="https://cran.r-project.org/web/packages/tidytext/index.html" class="external-link">tidytext</a>},
to complete the following topics:</p>
<ul>
<li><p><strong><a href="#tutorial.html#introduction-to-flair-nlp">Introduction to flaiR in
R</a></strong></p></li>
<li><p><strong><a href="">Text Scaling Method with
NER</a></strong></p></li>
<li><p><strong><a href="">Text Scaling Method with
POS</a></strong></p></li>
<li><p><strong><a href="#tutorial.html#sentence-and-token">Sentence and
Token Object</a></strong></p></li>
<li><p><strong><a href="#tutorial.html#sequence-taggings">Sequence
Taggings</a></strong></p></li>
<li><p><strong><a href="#tutorial.html#flair-embedding">Embedding in
flaiR</a></strong></p></li>
<li><p><strong><a href="#tutorial.html#performing-ner-tasks">Performing
NER Tasks</a></strong></p></li>
<li><p><strong><a href="#tutorial.html#training-a-binary-classifier-in-flair">Training a
Binary Classifier in flaiR</a></strong></p></li>
<li><p><strong><a href="#tutorial.html#training-rnns">Training
RNNa</a></strong></p></li>
<li><p><strong><a href="#tutorial.html#finetune-transformers">Finetune
BERT</a></strong></p></li>
</ul>
</div>
<p> </p>
<hr>
</div>
</div>
<div class="section level2">
<h2 id="sentence-and-token">Sentence and Token<a class="anchor" aria-label="anchor" href="#sentence-and-token"></a>
</h2>
<div style="text-align: justify">
<p>Sentence and Token are fundamental classes.</p>
<div class="section level3">
<h3 id="sentence">
<strong>Sentence</strong><a class="anchor" aria-label="anchor" href="#sentence"></a>
</h3>
<p>A Sentence in Flair is an object that contains a sequence of Token
objects, and it can be annotated with labels, such as named entities,
part-of-speech tags, and more. It also can store embeddings for the
sentence as a whole and different kinds of linguistic annotations.</p>
<p>Here’s a simple example of how you create a Sentence:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Creating a Sentence object</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va">flaiR</span><span class="op">)</span></span>
<span><span class="va">string</span> <span class="op">&lt;-</span> <span class="st">"What I see in UCD today, what I have seen of UCD in its impact on my own life and the life of Ireland."</span></span>
<span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span>
<span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="va">string</span><span class="op">)</span></span></code></pre></div>
<p><code>Sentence[26]</code> means that there are a total of 26 tokens
in the sentence.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="co">#&gt; Sentence[26]: "What I see in UCD today, what I have seen of UCD in its impact on my own life and the life of Ireland."</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="token">
<strong>Token</strong><a class="anchor" aria-label="anchor" href="#token"></a>
</h3>
<p>When you use Flair to handle text data,<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content="&lt;p&gt;Flair is built on PyTorch, which is a library in
Python.&lt;/p&gt;"><sup>1</sup></a> <code>Sentence</code>
and <code>Token</code> objects often play central roles in many use
cases. When you create a Sentence object, it usually automatically
decomposes the internal raw text into multiple Token objects. In other
words, the Sentence object automatically handles the text tokenization
work, so you usually don’t need to create Token objects manually.</p>
<p>Unlike R, which indexes from 1, Python indexes from 0. Therefore,
when I use a for loop, I use <code>seq_along(sentence) - 1</code>. The
output should be something like:</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># The Sentence object has automatically created and contains multiple Token objects</span></span>
<span><span class="co"># We can iterate through the Sentence object to view each Token.</span></span>
<span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq_along</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="co">#&gt; Token[0]: "What"</span></span>
<span><span class="co">#&gt; Token[1]: "I"</span></span>
<span><span class="co">#&gt; Token[2]: "see"</span></span>
<span><span class="co">#&gt; Token[3]: "in"</span></span>
<span><span class="co">#&gt; Token[4]: "UCD"</span></span>
<span><span class="co">#&gt; Token[5]: "today"</span></span>
<span><span class="co">#&gt; Token[6]: ","</span></span>
<span><span class="co">#&gt; Token[7]: "what"</span></span>
<span><span class="co">#&gt; Token[8]: "I"</span></span>
<span><span class="co">#&gt; Token[9]: "have"</span></span>
<span><span class="co">#&gt; Token[10]: "seen"</span></span>
<span><span class="co">#&gt; Token[11]: "of"</span></span>
<span><span class="co">#&gt; Token[12]: "UCD"</span></span>
<span><span class="co">#&gt; Token[13]: "in"</span></span>
<span><span class="co">#&gt; Token[14]: "its"</span></span>
<span><span class="co">#&gt; Token[15]: "impact"</span></span>
<span><span class="co">#&gt; Token[16]: "on"</span></span>
<span><span class="co">#&gt; Token[17]: "my"</span></span>
<span><span class="co">#&gt; Token[18]: "own"</span></span>
<span><span class="co">#&gt; Token[19]: "life"</span></span>
<span><span class="co">#&gt; Token[20]: "and"</span></span>
<span><span class="co">#&gt; Token[21]: "the"</span></span>
<span><span class="co">#&gt; Token[22]: "life"</span></span>
<span><span class="co">#&gt; Token[23]: "of"</span></span>
<span><span class="co">#&gt; Token[24]: "Ireland"</span></span>
<span><span class="co">#&gt; Token[25]: "."</span></span></code></pre></div>
<p>Or you can directly use <code>$tokens</code> method to print all
tokens.</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="va">tokens</span><span class="op">)</span></span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Token[0]: "What"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[2]]</span></span>
<span><span class="co">#&gt; Token[1]: "I"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[3]]</span></span>
<span><span class="co">#&gt; Token[2]: "see"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[4]]</span></span>
<span><span class="co">#&gt; Token[3]: "in"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[5]]</span></span>
<span><span class="co">#&gt; Token[4]: "UCD"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[6]]</span></span>
<span><span class="co">#&gt; Token[5]: "today"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[7]]</span></span>
<span><span class="co">#&gt; Token[6]: ","</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[8]]</span></span>
<span><span class="co">#&gt; Token[7]: "what"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[9]]</span></span>
<span><span class="co">#&gt; Token[8]: "I"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[10]]</span></span>
<span><span class="co">#&gt; Token[9]: "have"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[11]]</span></span>
<span><span class="co">#&gt; Token[10]: "seen"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[12]]</span></span>
<span><span class="co">#&gt; Token[11]: "of"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[13]]</span></span>
<span><span class="co">#&gt; Token[12]: "UCD"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[14]]</span></span>
<span><span class="co">#&gt; Token[13]: "in"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[15]]</span></span>
<span><span class="co">#&gt; Token[14]: "its"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[16]]</span></span>
<span><span class="co">#&gt; Token[15]: "impact"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[17]]</span></span>
<span><span class="co">#&gt; Token[16]: "on"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[18]]</span></span>
<span><span class="co">#&gt; Token[17]: "my"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[19]]</span></span>
<span><span class="co">#&gt; Token[18]: "own"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[20]]</span></span>
<span><span class="co">#&gt; Token[19]: "life"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[21]]</span></span>
<span><span class="co">#&gt; Token[20]: "and"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[22]]</span></span>
<span><span class="co">#&gt; Token[21]: "the"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[23]]</span></span>
<span><span class="co">#&gt; Token[22]: "life"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[24]]</span></span>
<span><span class="co">#&gt; Token[23]: "of"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[25]]</span></span>
<span><span class="co">#&gt; Token[24]: "Ireland"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[26]]</span></span>
<span><span class="co">#&gt; Token[25]: "."</span></span></code></pre></div>
<p><strong>Retrieve the Token</strong></p>
<p>To comprehend the string representation format of the Sentence
object, tagging at least one token is adequate. The
<code>get_token(n)</code> method, a Python <code>method</code>, allows
us to retrieve the Token object for a particular token. Additionally, we
can use <strong><code>[]</code></strong> to index a specific token. It
is noteworthy that Python indexes from 0, whereas R starts indexing from
1.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># method in Python</span></span>
<span><span class="va">sentence</span><span class="op">$</span><span class="fu">get_token</span><span class="op">(</span><span class="fl">5</span><span class="op">)</span></span>
<span><span class="co">#&gt; Token[4]: "UCD"</span></span></code></pre></div>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># indexing in R </span></span>
<span><span class="va">sentence</span><span class="op">[</span><span class="fl">4</span><span class="op">]</span></span>
<span><span class="co">#&gt; Token[4]: "UCD"</span></span></code></pre></div>
<p>Each word (and punctuation) in the sentence is treated as an
individual Token object. These Token objects store text information and
other possible linguistic information (such as part-of-speech tags or
named entity tags) and embeddings (if you used a model to generate
them).</p>
<p>Even though in most cases you do not need to create Token objects
manually, understanding how to manage these objects manually is still
useful in some situations, such as when you want fine-grained control
over the tokenization process. For example, you can control the
exactness of tokenization by adding manually created Token objects to a
Sentence object.</p>
<p>This design pattern in Flair allows users to handle text data in a
very flexible way. Users can use the automatic tokenization feature for
rapid development, and also perform finer-grained control to accommodate
more use cases.</p>
<p><strong>Annotate POS tag and NER tag</strong></p>
<p>The <code>add_label(label_type, value)</code> method can be employed
to assign a label to the token. We manually add a tag in this
preliminary tutorial, so usually, in Universal POS tags, if
<code>sentence[10]</code> is ‘see’, ‘seen’ might be tagged as
<code>VERB</code>, indicating it is a past participle form of a
verb.</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">sentence</span><span class="op">[</span><span class="fl">10</span><span class="op">]</span><span class="op">$</span><span class="fu">add_label</span><span class="op">(</span><span class="st">'manual-pos'</span>, <span class="st">'VERB'</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">[</span><span class="fl">10</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="co">#&gt; Token[10]: "seen" → VERB (1.0)</span></span></code></pre></div>
<p>We can also add a NER (Named Entity Recognition) tag to
<code>sentence[4]</code>, “UCD”, identifying it as a university in
Dublin.</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">sentence</span><span class="op">[</span><span class="fl">4</span><span class="op">]</span><span class="op">$</span><span class="fu">add_label</span><span class="op">(</span><span class="st">'ner'</span>, <span class="st">'ORG'</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">[</span><span class="fl">4</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="co">#&gt; Token[4]: "UCD" → ORG (1.0)</span></span></code></pre></div>
<p>If we print the sentence object, <code>Sentence[50]</code> provides
information for 50 tokens → [‘in’/ORG, ‘seen’/VERB], thus displaying two
tagging pieces of information.</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="co">#&gt; Sentence[26]: "What I see in UCD today, what I have seen of UCD in its impact on my own life and the life of Ireland." → ["UCD"/ORG, "seen"/VERB]</span></span></code></pre></div>
</div>
</div>
<div class="section level3">
<h3 id="corpus">
<strong>Corpus</strong><a class="anchor" aria-label="anchor" href="#corpus"></a>
</h3>
<div style="text-align: justify">
<p>The Corpus object in Flair is a fundamental data structure that
represents a dataset containing text samples, usually comprising of a
training set, a development set (or validation set), and a test set.
It’s designed to work smoothly with Flair’s models for tasks like named
entity recognition, text classification, and more.</p>
<p><strong>Attributes:</strong></p>
<ul>
<li>
<code>train</code>: A list of sentences (List[Sentence]) that form
the training dataset.</li>
<li>
<code>dev</code> (or development): A list of sentences
(List[Sentence]) that form the development (or validation) dataset.</li>
<li>
<code>test</code>: A list of sentences (List[Sentence]) that form
the test dataset.</li>
</ul>
<p><strong>Important Methods:</strong></p>
<ul>
<li>
<code>downsample</code>: This method allows you to downsample
(reduce) the number of sentences in the train, dev, and test splits.
obtain_statistics: This method gives a quick overview of the statistics
of the corpus, including the number of sentences and the distribution of
labels.</li>
<li>
<code>make_vocab_dictionary</code>: Used to create a vocabulary
dictionary from the corpus.</li>
</ul>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va">flaiR</span><span class="op">)</span></span>
<span><span class="va">Corpus</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Corpus</span></span>
<span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span></code></pre></div>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Create some example sentences</span></span>
<span><span class="va">train</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="fu">Sentence</span><span class="op">(</span><span class="st">'This is a training example.'</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">dev</span> <span class="op">&lt;-</span>  <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="fu">Sentence</span><span class="op">(</span><span class="st">'This is a validation example.'</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">test</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="fu">Sentence</span><span class="op">(</span><span class="st">'This is a test example.'</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Create a corpus using the custom data splits</span></span>
<span><span class="va">corpus</span> <span class="op">&lt;-</span>  <span class="fu">Corpus</span><span class="op">(</span>train <span class="op">=</span> <span class="va">train</span>, dev <span class="op">=</span> <span class="va">dev</span>, test <span class="op">=</span> <span class="va">test</span><span class="op">)</span></span></code></pre></div>
<p><code>$obtain_statistics()</code> method of the Corpus object in the
Flair library provides an overview of the dataset statistics. The method
returns a <a href="https://www.w3schools.com/python/python_dictionaries.asp" class="external-link">Python’s
dictionary</a> with details about the training, validation
(development), and test datasets that make up the corpus. In R, you can
use the jsonlite package to format JSON.</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://jeroen.r-universe.dev/jsonlite" class="external-link">jsonlite</a></span><span class="op">)</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/jsonlite/man/fromJSON.html" class="external-link">fromJSON</a></span><span class="op">(</span><span class="va">corpus</span><span class="op">$</span><span class="fu">obtain_statistics</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">formatted_str</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/jsonlite/man/fromJSON.html" class="external-link">toJSON</a></span><span class="op">(</span><span class="va">data</span>, pretty<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">formatted_str</span><span class="op">)</span></span>
<span><span class="co">#&gt; {</span></span>
<span><span class="co">#&gt;   "TRAIN": {</span></span>
<span><span class="co">#&gt;     "dataset": ["TRAIN"],</span></span>
<span><span class="co">#&gt;     "total_number_of_documents": [1],</span></span>
<span><span class="co">#&gt;     "number_of_documents_per_class": {},</span></span>
<span><span class="co">#&gt;     "number_of_tokens_per_tag": {},</span></span>
<span><span class="co">#&gt;     "number_of_tokens": {</span></span>
<span><span class="co">#&gt;       "total": [6],</span></span>
<span><span class="co">#&gt;       "min": [6],</span></span>
<span><span class="co">#&gt;       "max": [6],</span></span>
<span><span class="co">#&gt;       "avg": [6]</span></span>
<span><span class="co">#&gt;     }</span></span>
<span><span class="co">#&gt;   },</span></span>
<span><span class="co">#&gt;   "TEST": {</span></span>
<span><span class="co">#&gt;     "dataset": ["TEST"],</span></span>
<span><span class="co">#&gt;     "total_number_of_documents": [1],</span></span>
<span><span class="co">#&gt;     "number_of_documents_per_class": {},</span></span>
<span><span class="co">#&gt;     "number_of_tokens_per_tag": {},</span></span>
<span><span class="co">#&gt;     "number_of_tokens": {</span></span>
<span><span class="co">#&gt;       "total": [6],</span></span>
<span><span class="co">#&gt;       "min": [6],</span></span>
<span><span class="co">#&gt;       "max": [6],</span></span>
<span><span class="co">#&gt;       "avg": [6]</span></span>
<span><span class="co">#&gt;     }</span></span>
<span><span class="co">#&gt;   },</span></span>
<span><span class="co">#&gt;   "DEV": {</span></span>
<span><span class="co">#&gt;     "dataset": ["DEV"],</span></span>
<span><span class="co">#&gt;     "total_number_of_documents": [1],</span></span>
<span><span class="co">#&gt;     "number_of_documents_per_class": {},</span></span>
<span><span class="co">#&gt;     "number_of_tokens_per_tag": {},</span></span>
<span><span class="co">#&gt;     "number_of_tokens": {</span></span>
<span><span class="co">#&gt;       "total": [6],</span></span>
<span><span class="co">#&gt;       "min": [6],</span></span>
<span><span class="co">#&gt;       "max": [6],</span></span>
<span><span class="co">#&gt;       "avg": [6]</span></span>
<span><span class="co">#&gt;     }</span></span>
<span><span class="co">#&gt;   }</span></span>
<span><span class="co">#&gt; }</span></span></code></pre></div>
<p><strong>In R</strong></p>
<p>Below, we use data from the article <a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;ved=2ahUKEwjz_bS3p5KCAxWEWEEAHcuVAi4QFnoECA8QAQ&amp;url=https%3A%2F%2Fwww.journals.uchicago.edu%2Fdoi%2Ffull%2F10.1086%2F715165&amp;usg=AOvVaw3f_J3sXTrym2ZR64pF3ZtN&amp;opi=89978449" class="external-link"><em>The
Temporal Focus of Campaign Communication</em></a> by <a href="https://muellerstefan.net" class="external-link">Stefan Muller</a>, published in
<em>Journal of Politics</em> in 2020, as an example.</p>
<p>First, I vectorize the <code>cc_muller$text</code> using the Sentence
function to transform it into a list object. Then, we reformat
<code>cc_muller$class_pro_retro</code> as a factor. It’s essential to
note that R handles numerical values differently than Python. In R,
numerical values are represented with a floating point, so it’s
advisable to convert them into factors or strings. Lastly, I employ the
map function from the purrr package to assign labels to each sentence
corpus using the $add_label method.</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://purrr.tidyverse.org/" class="external-link">purrr</a></span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Attaching package: 'purrr'</span></span>
<span><span class="co">#&gt; The following object is masked from 'package:jsonlite':</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     flatten</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="va">cc_muller</span><span class="op">)</span></span>
<span><span class="co"># The `Sentence` object tokenizes text </span></span>
<span><span class="va">text</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html" class="external-link">lapply</a></span><span class="op">(</span> <span class="va">cc_muller</span><span class="op">$</span><span class="va">text</span>, <span class="va">Sentence</span><span class="op">)</span></span>
<span><span class="co"># split sentence object to train and test. </span></span>
<span><span class="va">labels</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html" class="external-link">as.factor</a></span><span class="op">(</span><span class="va">cc_muller</span><span class="op">$</span><span class="va">class_pro_retro</span><span class="op">)</span></span>
<span><span class="co"># `$add_label` method assigns the corresponding coded type to each Sentence corpus.</span></span>
<span><span class="va">text</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://purrr.tidyverse.org/reference/map2.html" class="external-link">map2</a></span><span class="op">(</span><span class="va">text</span>, <span class="va">labels</span>, <span class="op">~</span> <span class="va">.x</span><span class="op">$</span><span class="fu">add_label</span><span class="op">(</span><span class="st">"classification"</span>, <span class="va">.y</span><span class="op">)</span>, .progress <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<p>To perform a train-test split using base R, we can follow these
steps:</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">2046</span><span class="op">)</span></span>
<span><span class="va">sample</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html" class="external-link">sample</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="cn">TRUE</span>, <span class="cn">FALSE</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">text</span><span class="op">)</span>, replace<span class="op">=</span><span class="cn">TRUE</span>, prob<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0.8</span>, <span class="fl">0.2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">train</span>  <span class="op">&lt;-</span> <span class="va">text</span><span class="op">[</span><span class="va">sample</span><span class="op">]</span></span>
<span><span class="va">test</span>   <span class="op">&lt;-</span> <span class="va">text</span><span class="op">[</span><span class="op">!</span><span class="va">sample</span><span class="op">]</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html" class="external-link">sprintf</a></span><span class="op">(</span><span class="st">"Corpus object sizes - Train: %d |  Test: %d"</span>, <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">train</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">test</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "Corpus object sizes - Train: 4710 |  Test: 1148"</span></span></code></pre></div>
<p>If you don’t provide a dev set, Flair won’t force you to carve out a
portion of your test set to serve as a dev set. However, in some cases
when only the train and test sets are provided without a dev set, Flair
might automatically take a fraction of the train set (e.g., 10%) to use
as a dev set (<a href="https://github.com/flairNLP/flair/issues/2259#issuecomment-830040253" class="external-link">#2259</a>).
This is to offer a mechanism for model selection and early stopping to
prevent the model from overfitting on the train set.</p>
<p>In the “Corpus” function, there is a random selection of “dev.” To
ensure reproducibility, we need to set the seed in the Flair framework.
We can accomplish this by calling the top-level module “flair” from via
{flaiR} and using <code>$set_seed(1964L)</code> to set the seed.</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">flair</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/import_flair.html">import_flair</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">flair</span><span class="op">$</span><span class="fu">set_seed</span><span class="op">(</span><span class="fl">1964L</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">corpus</span> <span class="op">&lt;-</span> <span class="fu">Corpus</span><span class="op">(</span>train<span class="op">=</span><span class="va">train</span>, </span>
<span>                 <span class="co"># dev=test,</span></span>
<span>                 test<span class="op">=</span><span class="va">test</span><span class="op">)</span></span>
<span><span class="co">#&gt; 2024-05-02 01:46:34,570 No dev split found. Using 0% (i.e. 471 samples) of the train split as dev data</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html" class="external-link">sprintf</a></span><span class="op">(</span><span class="st">"Corpus object sizes - Train: %d | Test: %d | Dev: %d"</span>, </span>
<span>        <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">corpus</span><span class="op">$</span><span class="va">train</span><span class="op">)</span>, </span>
<span>        <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">corpus</span><span class="op">$</span><span class="va">test</span><span class="op">)</span>,</span>
<span>        <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">corpus</span><span class="op">$</span><span class="va">dev</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "Corpus object sizes - Train: 4239 | Test: 1148 | Dev: 471"</span></span></code></pre></div>
<p>In the later sections, there will be more similar processing using
the <code>Corpus</code>. Following that, we will focus on advanced NLP
applications.</p>
</div>
<p> </p>
<hr>
</div>
</div>
<div class="section level2">
<h2 id="sequence-taggings">Sequence Taggings<a class="anchor" aria-label="anchor" href="#sequence-taggings"></a>
</h2>
<div class="section level3">
<h3 id="tag-entities-in-text">
<strong>Tag Entities in Text</strong><a class="anchor" aria-label="anchor" href="#tag-entities-in-text"></a>
</h3>
<div style="text-align: justify">
<p>Let’s run named entity recognition over the following example
sentence: “I love Berlin and New York”. To do this, all you need is to
make a Sentence for this text, load a pre-trained model and use it to
predict tags for the sentence object.</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># attach flaiR in R</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va">flaiR</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># make a sentence</span></span>
<span></span>
<span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span>
<span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">'I love Berlin and New York.'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># load the NER tagger</span></span>
<span><span class="va">Classifier</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_nn.html">flair_nn</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Classifier</span></span>
<span><span class="va">tagger</span> <span class="op">&lt;-</span> <span class="va">Classifier</span><span class="op">$</span><span class="fu">load</span><span class="op">(</span><span class="st">'ner'</span><span class="op">)</span></span>
<span><span class="co">#&gt; 2024-05-02 01:46:36,594 SequenceTagger predicts: Dictionary with 20 tags: &lt;unk&gt;, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, &lt;START&gt;, &lt;STOP&gt;</span></span>
<span></span>
<span><span class="co"># run NER over sentence</span></span>
<span><span class="va">tagger</span><span class="op">$</span><span class="fu">predict</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span></code></pre></div>
<p>This should print:</p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># print the sentence with all annotations</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="co">#&gt; Sentence[7]: "I love Berlin and New York." → ["Berlin"/LOC, "New York"/LOC]</span></span></code></pre></div>
<p>Use a for loop to print out each POS tag. It’s important to note that
Python is indexed from 0. Therefore, in an R environment, we must use
<code>seq_along(sentence$get_labels()) - 1</code>.</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq_along</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="fu">get_labels</span><span class="op">(</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="fu">get_labels</span><span class="op">(</span><span class="op">)</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span><span class="co">#&gt; 'Span[2:3]: "Berlin"'/'LOC' (0.9812)</span></span>
<span><span class="co">#&gt; 'Span[4:6]: "New York"'/'LOC' (0.9957)</span></span></code></pre></div>
</div>
<p> </p>
</div>
<div class="section level3">
<h3 id="tag-part-of-speech-in-text">
<strong>Tag Part-of-Speech in Text</strong><a class="anchor" aria-label="anchor" href="#tag-part-of-speech-in-text"></a>
</h3>
<div style="text-align: justify">
<p>We use flair/pos-english for POS tagging in the standard models on
Hugging Face.</p>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># attach flaiR in R</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va">flaiR</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># make a sentence</span></span>
<span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span>
<span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">'I love Berlin and New York.'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># load the NER tagger</span></span>
<span><span class="va">Classifier</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_nn.html">flair_nn</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Classifier</span></span>
<span><span class="va">tagger</span> <span class="op">&lt;-</span> <span class="va">Classifier</span><span class="op">$</span><span class="fu">load</span><span class="op">(</span><span class="st">'pos'</span><span class="op">)</span></span>
<span><span class="co">#&gt; 2024-05-02 01:46:38,055 SequenceTagger predicts: Dictionary with 53 tags: &lt;unk&gt;, O, UH, ,, VBD, PRP, VB, PRP$, NN, RB, ., DT, JJ, VBP, VBG, IN, CD, NNS, NNP, WRB, VBZ, WDT, CC, TO, MD, VBN, WP, :, RP, EX, JJR, FW, XX, HYPH, POS, RBR, JJS, PDT, NNPS, RBS, AFX, WP$, -LRB-, -RRB-, ``, '', LS, $, SYM, ADD</span></span>
<span></span>
<span><span class="co"># run NER over sentence</span></span>
<span><span class="va">tagger</span><span class="op">$</span><span class="fu">predict</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span></code></pre></div>
<p>This should print:</p>
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># print the sentence with all annotations</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="co">#&gt; Sentence[7]: "I love Berlin and New York." → ["I"/PRP, "love"/VBP, "Berlin"/NNP, "and"/CC, "New"/NNP, "York"/NNP, "."/.]</span></span></code></pre></div>
<p>Use a for loop to print out each pos tag.</p>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq_along</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="fu">get_labels</span><span class="op">(</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="fu">get_labels</span><span class="op">(</span><span class="op">)</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span><span class="co">#&gt; 'Token[0]: "I"'/'PRP' (1.0)</span></span>
<span><span class="co">#&gt; 'Token[1]: "love"'/'VBP' (1.0)</span></span>
<span><span class="co">#&gt; 'Token[2]: "Berlin"'/'NNP' (0.9999)</span></span>
<span><span class="co">#&gt; 'Token[3]: "and"'/'CC' (1.0)</span></span>
<span><span class="co">#&gt; 'Token[4]: "New"'/'NNP' (1.0)</span></span>
<span><span class="co">#&gt; 'Token[5]: "York"'/'NNP' (1.0)</span></span>
<span><span class="co">#&gt; 'Token[6]: "."'/'.' (1.0)</span></span></code></pre></div>
</div>
<p> </p>
</div>
<div class="section level3">
<h3 id="detect-sentiment">
<strong>Detect Sentiment</strong><a class="anchor" aria-label="anchor" href="#detect-sentiment"></a>
</h3>
<div style="text-align: justify">
<p>Let’s run sentiment analysis over the same sentence to determine
whether it is POSITIVE or NEGATIVE.</p>
<p>You can do this with essentially the same code as above. Just instead
of loading the ‘ner’ model, you now load the ‘sentiment’ model:</p>
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># attach flaiR in R</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va">flaiR</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># make a sentence</span></span>
<span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span>
<span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">'I love Berlin and New York.'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># load the Classifier tagger from flair.nn module</span></span>
<span><span class="va">Classifier</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_nn.html">flair_nn</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Classifier</span></span>
<span><span class="va">tagger</span> <span class="op">&lt;-</span> <span class="va">Classifier</span><span class="op">$</span><span class="fu">load</span><span class="op">(</span><span class="st">'sentiment'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># run sentiment analysis over sentence</span></span>
<span><span class="va">tagger</span><span class="op">$</span><span class="fu">predict</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># print the sentence with all annotations</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="co">#&gt; Sentence[7]: "I love Berlin and New York." → POSITIVE (0.9982)</span></span></code></pre></div>
</div>
<p> </p>
<p> </p>
<hr>
</div>
</div>
<div class="section level2">
<h2 id="performing-ner-tasks">Performing NER Tasks<a class="anchor" aria-label="anchor" href="#performing-ner-tasks"></a>
</h2>
<div style="text-align: justify">
<p>Flair NLP also provides a set of functions to perform NLP tasks, such
as named entity recognition, sentiment analysis, and part-of-speech
tagging.</p>
<p>First, we load the data and the model to perform NER task on the text
below.</p>
<blockquote>
<p><em>Yesterday, Dr. Jane Smith spoke at the United Nations in New
York. She discussed climate change and its impact on global economies.
The event was attended by representatives from various countries
including France and Japan. Dr. Smith mentioned that by 2050, the world
could see a rise in sea level by approximately 2 feet. The World Health
Organization (WHO) has pledged $50 million to combat the health effects
of global warming. In an interview with The New York Times, Dr. Smith
emphasized the urgent need for action. Later that day, she flew back to
London, arriving at 10:00 PM GMT.</em></p>
</blockquote>
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Classifier</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_nn.html">flair_nn</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Classifier</span></span>
<span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span>
<span></span>
<span><span class="co"># load the model flair NLP already trained for us</span></span>
<span><span class="va">tagger</span> <span class="op">&lt;-</span> <span class="va">Classifier</span><span class="op">$</span><span class="fu">load</span><span class="op">(</span><span class="st">'ner'</span><span class="op">)</span></span>
<span><span class="co">#&gt; 2024-05-02 01:46:43,229 SequenceTagger predicts: Dictionary with 20 tags: &lt;unk&gt;, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, &lt;START&gt;, &lt;STOP&gt;</span></span>
<span></span>
<span><span class="co"># make a sentence object</span></span>
<span><span class="va">text</span> <span class="op">&lt;-</span> <span class="st">"Yesterday, Dr. Jane Smith spoke at the United Nations in New York. She discussed climate change and its impact on global economies. The event was attended by representatives from various countries including France and Japan. Dr. Smith mentioned that by 2050, the world could see a rise in sea level by approximately 2 feet. The World Health Organization (WHO) has pledged $50 million to combat the health effects of global warming. In an interview with The New York Times, Dr. Smith emphasized the urgent need for action. Later that day, she flew back to London, arriving at 10:00 PM GMT."</span></span>
<span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="va">text</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># predict NER tags</span></span>
<span><span class="va">tagger</span><span class="op">$</span><span class="fu">predict</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># print sentence with predicted tags</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="co">#&gt; Sentence[115]: "Yesterday, Dr. Jane Smith spoke at the United Nations in New York. She discussed climate change and its impact on global economies. The event was attended by representatives from various countries including France and Japan. Dr. Smith mentioned that by 2050, the world could see a rise in sea level by approximately 2 feet. The World Health Organization (WHO) has pledged $50 million to combat the health effects of global warming. In an interview with The New York Times, Dr. Smith emphasized the urgent need for action. Later that day, she flew back to London, arriving at 10:00 PM GMT." → ["Jane Smith"/PER, "United Nations"/ORG, "New York"/LOC, "France"/LOC, "Japan"/LOC, "Smith"/PER, "World Health Organization"/ORG, "WHO"/ORG, "The New York Times"/ORG, "Smith"/PER, "London"/LOC, "GMT"/MISC]</span></span></code></pre></div>
<p>Alternatively, to facilitate more efficient use for social science
research, {<code>flairR</code>} expands {<code>flairNLP/flair</code>}’s
core functionality for working with three major functions to extract
features in a tidy and fast format– <a href="https://cran.r-project.org/web/packages/data.table/index.html" class="external-link">data.table</a>
in R.</p>
<p>The expanded features in <code>flaiR</code> can be used to perform
and extract features from the sentence object in a tidy format.</p>
<ul>
<li><a href="https://davidycliao.github.io/flaiR/articles/get_entities.html"><strong>named
entity recognition</strong></a></li>
<li><a href="https://davidycliao.github.io/flaiR/articles/get_sentiments.html"><strong>transformer-based
sentiment analysis</strong></a></li>
<li><a href="https://davidycliao.github.io/flaiR/articles/get_pos.html"><strong>part-of-speech
tagging</strong></a></li>
</ul>
<p>For example, we can use the <code>get_entities</code> function and
<code>load_tagger_ner("ner")</code>in flaiR to extract the named
entities from the sentence object in a tidy format.</p>
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">tagger_ner</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/load_tagger_ner.html">load_tagger_ner</a></span><span class="op">(</span><span class="st">"ner"</span><span class="op">)</span></span>
<span><span class="co">#&gt; 2024-05-02 01:46:46,031 SequenceTagger predicts: Dictionary with 20 tags: &lt;unk&gt;, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, &lt;START&gt;, &lt;STOP&gt;</span></span>
<span><span class="va">results</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/get_entities.html">get_entities</a></span><span class="op">(</span>text <span class="op">=</span> <span class="va">text</span>, </span>
<span>                        doc_ids <span class="op">=</span> <span class="st">"example text"</span>,</span>
<span>                        <span class="va">tagger_ner</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">results</span><span class="op">)</span></span>
<span><span class="co">#&gt;           doc_id                    entity    tag</span></span>
<span><span class="co">#&gt;           &lt;char&gt;                    &lt;char&gt; &lt;char&gt;</span></span>
<span><span class="co">#&gt;  1: example text                Jane Smith    PER</span></span>
<span><span class="co">#&gt;  2: example text            United Nations    ORG</span></span>
<span><span class="co">#&gt;  3: example text                  New York    LOC</span></span>
<span><span class="co">#&gt;  4: example text                    France    LOC</span></span>
<span><span class="co">#&gt;  5: example text                     Japan    LOC</span></span>
<span><span class="co">#&gt;  6: example text                     Smith    PER</span></span>
<span><span class="co">#&gt;  7: example text World Health Organization    ORG</span></span>
<span><span class="co">#&gt;  8: example text                       WHO    ORG</span></span>
<span><span class="co">#&gt;  9: example text        The New York Times    ORG</span></span>
<span><span class="co">#&gt; 10: example text                     Smith    PER</span></span>
<span><span class="co">#&gt; 11: example text                    London    LOC</span></span>
<span><span class="co">#&gt; 12: example text                       GMT   MISC</span></span></code></pre></div>
<p>In most cases, we need to extract the named entities from a large
corpus. For example, we can use Stefan’s data from <strong><em>The
Temporal Focus of Campaign Communication</em></strong> (JOP 2022) as an
example.</p>
<div class="sourceCode" id="cb29"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va">flaiR</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="va">cc_muller</span><span class="op">)</span></span>
<span><span class="va">examples</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">cc_muller</span>, <span class="fl">10</span><span class="op">)</span></span>
<span><span class="va">examples</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"text"</span>, <span class="st">"countryname"</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="co">#&gt;                                                                                                                                                                                                                                            text</span></span>
<span><span class="co">#&gt; 1                                                                                                                                                   And to boost the housing we need, we will start to build a new generation of garden cities.</span></span>
<span><span class="co">#&gt; 2                                                                                                                                                 In many cases, their value to society in economic, social and environmental terms is dubious.</span></span>
<span><span class="co">#&gt; 3  However, requests for Standing Order 31 adjournments of Dáil business, which are read out on the Order of Business, are being misused to ventilate issues which clearly have no prospect of meeting the criteria set out in Standing Orders.</span></span>
<span><span class="co">#&gt; 4                                                                                          We will work with the Pig Industry Stakeholder group to enhance areas such as food safety, quality, animal welfare and environmental sustainability.</span></span>
<span><span class="co">#&gt; 5                                    The legacy of the Celtic Tiger includes 'ghost' housing estates, negative equity, an unprecedented banking crisis, unemployment, a personal debt crisis, badly planned towns and a huge Exchequer deficit.</span></span>
<span><span class="co">#&gt; 6                                                                                                                                       We must not allow ISIS to hold a safe haven from which it can pursue an agenda of anarchy and violence.</span></span>
<span><span class="co">#&gt; 7                                                                                 The declaration of the G20 as the premier forum for international economic cooperation represents the most significant shift in global governance in decades.</span></span>
<span><span class="co">#&gt; 8                                                         This funding represents the next instalment (Round Five, Phase One) of the Rudd Labor Government's highly successful 10 year $2. 5 billion Trades Training Centres in Schools program</span></span>
<span><span class="co">#&gt; 9         We'll provide free after-school care and holiday programmes for every child at decile 1 to 4 schools, and we will expand access to Out of School Care and Recreation (OSCAR) low income subsidies to children at decile 5-10 schools.</span></span>
<span><span class="co">#&gt; 10                                                                                                This will properly manage the adverse environmental effects of activities such as oil and gas exploration in our vast Exclusive Economic Zone</span></span>
<span><span class="co">#&gt;       countryname</span></span>
<span><span class="co">#&gt; 1  United Kingdom</span></span>
<span><span class="co">#&gt; 2         Ireland</span></span>
<span><span class="co">#&gt; 3         Ireland</span></span>
<span><span class="co">#&gt; 4         Ireland</span></span>
<span><span class="co">#&gt; 5         Ireland</span></span>
<span><span class="co">#&gt; 6          Canada</span></span>
<span><span class="co">#&gt; 7       Australia</span></span>
<span><span class="co">#&gt; 8       Australia</span></span>
<span><span class="co">#&gt; 9     New Zealand</span></span>
<span><span class="co">#&gt; 10    New Zealand</span></span></code></pre></div>
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">tagger_ner</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/load_tagger_ner.html">load_tagger_ner</a></span><span class="op">(</span><span class="st">"ner"</span><span class="op">)</span></span>
<span><span class="co">#&gt; 2024-05-02 01:46:49,240 SequenceTagger predicts: Dictionary with 20 tags: &lt;unk&gt;, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, &lt;START&gt;, &lt;STOP&gt;</span></span>
<span><span class="va">results</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/get_entities.html">get_entities</a></span><span class="op">(</span>text <span class="op">=</span> <span class="va">examples</span><span class="op">$</span><span class="va">text</span>, </span>
<span>                        doc_ids <span class="op">=</span> <span class="va">examples</span><span class="op">$</span><span class="va">countryname</span>,</span>
<span>                        <span class="va">tagger_ner</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">results</span><span class="op">)</span></span>
<span><span class="co">#&gt;             doc_id                   entity    tag</span></span>
<span><span class="co">#&gt;             &lt;char&gt;                   &lt;char&gt; &lt;char&gt;</span></span>
<span><span class="co">#&gt;  1: United Kingdom                     &lt;NA&gt;   &lt;NA&gt;</span></span>
<span><span class="co">#&gt;  2:        Ireland                     &lt;NA&gt;   &lt;NA&gt;</span></span>
<span><span class="co">#&gt;  3:        Ireland                     Dáil    ORG</span></span>
<span><span class="co">#&gt;  4:        Ireland        Order of Business    ORG</span></span>
<span><span class="co">#&gt;  5:        Ireland          Standing Orders   MISC</span></span>
<span><span class="co">#&gt;  6:        Ireland Pig Industry Stakeholder    ORG</span></span>
<span><span class="co">#&gt;  7:        Ireland             Celtic Tiger    ORG</span></span>
<span><span class="co">#&gt;  8:         Canada                     ISIS    ORG</span></span>
<span><span class="co">#&gt;  9:      Australia                      G20    ORG</span></span>
<span><span class="co">#&gt; 10:      Australia               Round Five   MISC</span></span>
<span><span class="co">#&gt; 11:      Australia                Phase One   MISC</span></span>
<span><span class="co">#&gt; 12:      Australia    Rudd Labor Government    ORG</span></span>
<span><span class="co">#&gt; 13:    New Zealand                    OSCAR    ORG</span></span>
<span><span class="co">#&gt; 14:    New Zealand  Exclusive Economic Zone   MISC</span></span></code></pre></div>
<p>In addition, to handle the load on RAM when dealing with larger
corpus, {<code>flairR</code>} supports batch processing to handle texts
in batches, which is especially useful when dealing with large datasets,
to optimize memory usage and performance. The implementation of batch
processing can also utilize GPU acceleration for faster
computations.</p>
</div>
<p> </p>
<hr>
</div>
<div class="section level2">
<h2 id="flair-embedding">Flair Embedding<a class="anchor" aria-label="anchor" href="#flair-embedding"></a>
</h2>
<div style="text-align: justify">
<p>Flair is a very popular natural language processing library,
providing a variety of embedding methods for text representation through
Flair. Flair Embeddings is a word embedding framowork in Natural
Language Processing, developed by the <a href="https://engineering.zalando.com/posts/2018/11/zalando-research-releases-flair.html" class="external-link">Zalando</a>.
Flair focuses on word-level representation and can capture contextual
information of words, meaning that the same word can have different
embeddings in different contexts. Unlike traditional word embeddings
(such as Word2Vec or GloVe), Flair can dynamically generate word
embeddings based on context and has achieved excellent results in
various NLP tasks. Below are some key points about Flair Embeddings:</p>
<p><strong>Context-Aware</strong></p>
<p>Flair can understand the context of a word in a sentence and
dynamically generate word embeddings based on this context. This is
different from static embeddings, where the embedding of a word does not
consider its context in a sentence.</p>
<p>Flair is a dynamic word embedding technique that can understand the
meaning of words based on context. In contrast, static word embeddings,
such as Word2Vec or GloVe, provide a fixed embedding for each word
without considering its context in a sentence.</p>
<p>Therefore, context-sensitive embedding techniques, such as Flair, can
capture the meaning of words in specific sentences more accurately, thus
enhancing the performance of language models in various tasks.</p>
<p>Example:</p>
<p>Consider the following two English sentences:</p>
<ul>
<li>“I am interested in the bank of the river.”</li>
<li>“I need to go to the bank to withdraw money.”</li>
</ul>
<p>Here, the word “bank” has two different meanings. In the first
sentence, it refers to the edge or shore of a river. In the second
sentence, it refers to a financial institution.</p>
<p>For static embeddings, the word “bank” might have an embedding that
lies somewhere between these two meanings because it doesn’t consider
context. But for dynamic embeddings like Flair, “bank” in the first
sentence will have an embedding related to rivers, and in the second
sentence, it will have an embedding related to finance.</p>
<div class="sourceCode" id="cb31"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">FlairEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">FlairEmbeddings</span></span>
<span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span>
<span></span>
<span></span>
<span><span class="co"># Initialize Flair embeddings</span></span>
<span><span class="va">flair_embedding_forward</span> <span class="op">&lt;-</span> <span class="fu">FlairEmbeddings</span><span class="op">(</span><span class="st">'news-forward'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Define the two sentences</span></span>
<span><span class="va">sentence1</span> <span class="op">&lt;-</span>  <span class="fu">Sentence</span><span class="op">(</span><span class="st">"I am interested in the bank of the river."</span><span class="op">)</span></span>
<span><span class="va">sentence2</span> <span class="op">&lt;-</span>  <span class="fu">Sentence</span><span class="op">(</span><span class="st">"I need to go to the bank to withdraw money."</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Get the embeddings</span></span>
<span></span>
<span><span class="va">flair_embedding_forward</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence1</span><span class="op">)</span></span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Sentence[10]: "I am interested in the bank of the river."</span></span>
<span><span class="va">flair_embedding_forward</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence2</span><span class="op">)</span></span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Sentence[11]: "I need to go to the bank to withdraw money."</span></span>
<span></span>
<span><span class="co"># Extract the embedding for "bank" from the sentences</span></span>
<span><span class="va">bank_embedding_sentence1</span> <span class="op">=</span> <span class="va">sentence1</span><span class="op">[</span><span class="fl">5</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span>  <span class="co"># "bank" is the seventh word</span></span>
<span><span class="va">bank_embedding_sentence2</span> <span class="op">=</span> <span class="va">sentence2</span><span class="op">[</span><span class="fl">6</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span>  <span class="co"># "bank" is the sixth word</span></span></code></pre></div>
<p>Same word, similar vector representation, but essentially different.
In this way, you can see how the dynamic embeddings for “bank” in the
two sentences differ based on context. Although we printed the
embeddings here, in reality, they would be high-dimensional vectors, so
you might see a lot of numbers. If you want a more intuitive view of the
differences, you could compute the cosine similarity or other metrics
between the two embeddings.</p>
<p>This is just a simple demonstration. In practice, you can also
combine multiple embedding techniques, such as
<code>WordEmbeddings</code> and <code>FlairEmbeddings</code>, to get
richer word vectors.</p>
<div class="sourceCode" id="cb32"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va">lsa</span><span class="op">)</span></span>
<span><span class="co">#&gt; Loading required package: SnowballC</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/lsa/man/cosine.html" class="external-link">cosine</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/numeric.html" class="external-link">as.numeric</a></span><span class="op">(</span> <span class="va">bank_embedding_sentence1</span><span class="op">$</span><span class="fu">numpy</span><span class="op">(</span><span class="op">)</span><span class="op">)</span>, </span>
<span>       <span class="fu"><a href="https://rdrr.io/r/base/numeric.html" class="external-link">as.numeric</a></span><span class="op">(</span> <span class="va">bank_embedding_sentence2</span><span class="op">$</span><span class="fu">numpy</span><span class="op">(</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt;           [,1]</span></span>
<span><span class="co">#&gt; [1,] 0.7329551</span></span></code></pre></div>
<p><strong>Character-Based</strong></p>
<p>Flair uses a character-level language model, meaning it can generate
embeddings for rare words or even misspelled words. This is an important
feature because it allows the model to understand and process words that
have never appeared in the training data. Flair uses a bidirectional
LSTM (Long Short-Term Memory) network that operates at a character
level. This means that it feeds individual characters into the LSTM
instead of words.</p>
<p><strong>Multilingual Support</strong></p>
<p>Flair provides various pre-trained character-level language models,
supporting contextual word embeddings for multiple languages. Flair
allows you to easily combine different word embeddings (e.g., Flair
Embeddings, Word2Vec, GloVe, etc.) to create powerful stacked
embeddings.</p>
</div>
<div class="section level3">
<h3 id="classic-wordembeddings">Classic Wordembeddings<a class="anchor" aria-label="anchor" href="#classic-wordembeddings"></a>
</h3>
<div style="text-align: justify">
<p>In Flair, the simplest form of embedding that still contains semantic
information about the word is called classic word embeddings. These
embeddings are pre-trained and non-contextual. Let’s retrieve a few word
embeddings. Then, we can utilize FastText embeddings with the following
code. To use them, we simply instantiate a WordEmbeddings class by
passing in the ID of the embedding of our choice. Then, we simply wrap
our text into a Sentence object, and call the embed(sentence) method on
our WordEmbeddings class.</p>
<div class="sourceCode" id="cb33"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">embedding</span> <span class="op">=</span> <span class="fu"><a href="../reference/flair_embeddings.WordEmbeddings.html">flair_embeddings.WordEmbeddings</a></span><span class="op">(</span><span class="st">'crawl'</span><span class="op">)</span> </span>
<span><span class="va">sentence</span> <span class="op">=</span> <span class="fu"><a href="../reference/flair_data.Sentence.html">flair_data.Sentence</a></span><span class="op">(</span><span class="st">"one two three one"</span><span class="op">)</span> </span>
<span><span class="va">embedding</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span> </span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Sentence[4]: "one two three one"</span></span>
<span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq_along</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="va">tokens</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="va">tokens</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">)</span>, n <span class="op">=</span><span class="fl">5</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="co">#&gt; tensor([-0.0535, -0.0368, -0.2851, -0.0381, -0.0486,  0.2383])</span></span>
<span><span class="co">#&gt; tensor([ 0.0282, -0.0786, -0.1236,  0.1756, -0.1199,  0.0964])</span></span>
<span><span class="co">#&gt; tensor([-0.0920, -0.0690, -0.1475,  0.2313, -0.0872,  0.0799])</span></span>
<span><span class="co">#&gt; tensor([-0.0535, -0.0368, -0.2851, -0.0381, -0.0486,  0.2383])</span></span></code></pre></div>
<p>Flair supports a range of classic word embeddings, each offering
unique features and application scopes. Below is an overview, detailing
the ID required to load each embedding and its corresponding
language.</p>
<table class="table">
<thead><tr class="header">
<th>Embedding Type</th>
<th>ID</th>
<th>Language</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>GloVe</td>
<td>glove</td>
<td>English</td>
</tr>
<tr class="even">
<td>Komninos</td>
<td>extvec</td>
<td>English</td>
</tr>
<tr class="odd">
<td>Twitter</td>
<td>twitter</td>
<td>English</td>
</tr>
<tr class="even">
<td>Turian (small)</td>
<td>turian</td>
<td>English</td>
</tr>
<tr class="odd">
<td>FastText (crawl)</td>
<td>crawl</td>
<td>English</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>ar</td>
<td>Arabic</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>bg</td>
<td>Bulgarian</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>ca</td>
<td>Catalan</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>cz</td>
<td>Czech</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>da</td>
<td>Danish</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>de</td>
<td>German</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>es</td>
<td>Spanish</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>en</td>
<td>English</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>eu</td>
<td>Basque</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>fa</td>
<td>Persian</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>fi</td>
<td>Finnish</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>fr</td>
<td>French</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>he</td>
<td>Hebrew</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>hi</td>
<td>Hindi</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>hr</td>
<td>Croatian</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>id</td>
<td>Indonesian</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>it</td>
<td>Italian</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>ja</td>
<td>Japanese</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>ko</td>
<td>Korean</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>nl</td>
<td>Dutch</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>no</td>
<td>Norwegian</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>pl</td>
<td>Polish</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>pt</td>
<td>Portuguese</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>ro</td>
<td>Romanian</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>ru</td>
<td>Russian</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>si</td>
<td>Slovenian</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>sk</td>
<td>Slovak</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>sr</td>
<td>Serbian</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>sv</td>
<td>Swedish</td>
</tr>
<tr class="odd">
<td>FastText (news &amp; Wikipedia)</td>
<td>tr</td>
<td>Turkish</td>
</tr>
<tr class="even">
<td>FastText (news &amp; Wikipedia)</td>
<td>zh</td>
<td>Chinese</td>
</tr>
</tbody>
</table>
</div>
<p> </p>
<hr>
</div>
<div class="section level3">
<h3 id="contexual-embeddings">Contexual Embeddings<a class="anchor" aria-label="anchor" href="#contexual-embeddings"></a>
</h3>
<div style="text-align: justify">
<p>Understanding the contextuality of Flair embeddings The idea behind
contextual string embeddings is that each word embedding should be
defined by not only its syntactic-semantic meaning but also the context
it appears in. What this means is that each word will have a different
embedding for every context it appears in. Each pre-trained Flair model
offers a <strong>forward</strong> version and a
<strong>backward</strong> version. Let’s assume you are processing a
language that, just like this book, uses the left-to-right script. The
forward version takes into account the context that happens before the
word – on the left-hand side. The backward version works in the opposite
direction. It takes into account the context after the word – on the
right-hand side of the word. If this is true, then two same words that
appear at the beginning of two different sentences should have identical
forward embeddings, because their context is null. Let’s test this
out:</p>
<p>Because we are using a forward model, it only takes into account the
context that occurs before a word. Additionally, since our word has no
context on the left-hand side of its position in the sentence, the two
embeddings are identical, and the code assumes they are identical,
indeed output is <strong>True</strong>.</p>
<div class="sourceCode" id="cb34"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">embedding</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.FlairEmbeddings.html">flair_embeddings.FlairEmbeddings</a></span><span class="op">(</span><span class="st">'news-forward'</span><span class="op">)</span></span>
<span><span class="va">s1</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.Sentence.html">flair_data.Sentence</a></span><span class="op">(</span><span class="st">"nice shirt"</span><span class="op">)</span> </span>
<span><span class="va">s2</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.Sentence.html">flair_data.Sentence</a></span><span class="op">(</span><span class="st">"nice pants"</span><span class="op">)</span> </span>
<span></span>
<span><span class="va">embedding</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">s1</span><span class="op">)</span> </span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Sentence[2]: "nice shirt"</span></span>
<span><span class="va">embedding</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">s2</span><span class="op">)</span> </span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Sentence[2]: "nice pants"</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">" s1 sentence:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste</a></span><span class="op">(</span><span class="va">s1</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span>, sep <span class="op">=</span> <span class="st">""</span><span class="op">)</span>, <span class="st">"\n"</span>, <span class="st">"s2 sentence:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste</a></span><span class="op">(</span><span class="va">s2</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span>, sep <span class="op">=</span> <span class="st">""</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt;  s1 sentence: Token[0]: "nice" </span></span>
<span><span class="co">#&gt;  s2 sentence: Token[0]: "nice"</span></span></code></pre></div>
<p>We test whether the sum of the two 2048 embeddings of ‘nice’ is equal
to 2048. If it is true, it indicates that the embedding results are
consistent, which should theoretically be the case.</p>
<div class="sourceCode" id="cb35"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">s1</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">numpy</span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">==</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">s1</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">numpy</span><span class="op">(</span><span class="op">)</span> <span class="op">==</span>  <span class="va">s2</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">numpy</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] TRUE</span></span></code></pre></div>
<p>Now we separately add a few more words, <code>very</code> and
<code>pretty</code>, into two sentence objects.</p>
<div class="sourceCode" id="cb36"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">embedding</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.FlairEmbeddings.html">flair_embeddings.FlairEmbeddings</a></span><span class="op">(</span><span class="st">'news-forward'</span><span class="op">)</span></span>
<span><span class="va">s1</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.Sentence.html">flair_data.Sentence</a></span><span class="op">(</span><span class="st">"nice shirt"</span><span class="op">)</span> </span>
<span><span class="va">s2</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.Sentence.html">flair_data.Sentence</a></span><span class="op">(</span><span class="st">"nice pants"</span><span class="op">)</span> </span></code></pre></div>
<div class="sourceCode" id="cb37"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">embedding</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.FlairEmbeddings.html">flair_embeddings.FlairEmbeddings</a></span><span class="op">(</span><span class="st">'news-forward'</span><span class="op">)</span></span>
<span><span class="va">s1</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.Sentence.html">flair_data.Sentence</a></span><span class="op">(</span><span class="st">"very nice shirt"</span><span class="op">)</span> </span>
<span><span class="va">s2</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.Sentence.html">flair_data.Sentence</a></span><span class="op">(</span><span class="st">"pretty nice pants"</span><span class="op">)</span> </span>
<span></span>
<span><span class="va">embedding</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">s1</span><span class="op">)</span> </span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Sentence[3]: "very nice shirt"</span></span>
<span><span class="va">embedding</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">s2</span><span class="op">)</span> </span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; Sentence[3]: "pretty nice pants"</span></span></code></pre></div>
<p>The two sets of embeddings are not identical because the words are
different, so it returns <strong>False</strong>.</p>
<div class="sourceCode" id="cb38"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">s1</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">numpy</span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">==</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">s1</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">numpy</span><span class="op">(</span><span class="op">)</span> <span class="op">==</span>  <span class="va">s2</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">numpy</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] FALSE</span></span></code></pre></div>
<p>The measure of similarity between two vectors in an inner product
space is known as cosine similarity. The formula for calculating cosine
similarity between two vectors, such as vectors A and B, is as
follows:</p>
<p><span class="math inline">\(Cosine Similarity = \frac{\sum_{i} (A_i
\cdot B_i)}{\sqrt{\sum_{i} (A_i^2)} \cdot \sqrt{\sum_{i}
(B_i^2)}}\)</span></p>
<div class="sourceCode" id="cb39"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va">lsa</span><span class="op">)</span></span>
<span><span class="va">vector1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html" class="external-link">as.numeric</a></span><span class="op">(</span><span class="va">s1</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">numpy</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">vector2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html" class="external-link">as.numeric</a></span><span class="op">(</span><span class="va">s2</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">numpy</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>We can observe that the similarity between the two words is 0.55.</p>
<div class="sourceCode" id="cb40"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cosine_similarity</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/lsa/man/cosine.html" class="external-link">cosine</a></span><span class="op">(</span><span class="va">vector1</span>, <span class="va">vector2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">cosine_similarity</span><span class="op">)</span></span>
<span><span class="co">#&gt;           [,1]</span></span>
<span><span class="co">#&gt; [1,] 0.5571664</span></span></code></pre></div>
</div>
<p> </p>
<hr>
</div>
<div class="section level3">
<h3 id="extracting-embeddings-from-bert">Extracting Embeddings from BERT<a class="anchor" aria-label="anchor" href="#extracting-embeddings-from-bert"></a>
</h3>
<div style="text-align: justify">
<p>First, we utilize the flair.embeddings.TransformerWordEmbeddings
function to download BERT, and more transformer models can also be found
on <a href="https://huggingface.co/flair" class="external-link">Flair NLP’s Hugging
Face</a>.</p>
<div class="sourceCode" id="cb41"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">TransformerWordEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.TransformerWordEmbeddings.html">flair_embeddings.TransformerWordEmbeddings</a></span><span class="op">(</span><span class="st">"bert-base-uncased"</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb42"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">embedding</span> <span class="op">&lt;-</span> <span class="va">TransformerWordEmbeddings</span><span class="op">$</span><span class="fu">embed</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span></code></pre></div>
<p>Traverse each token in the sentence and print them. To view each
token, it’s necessary to use<code>reticulate::py_str(token)</code> since
the sentence is a Python object.</p>
<div class="sourceCode" id="cb43"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Iterate through each token in the sentence, printing them. </span></span>
<span><span class="co"># Utilize reticulate::py_str(token) to view each token, given that the sentence is a Python object.</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq_along</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="va">tokens</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"Token: "</span>, <span class="fu">reticulate</span><span class="fu">::</span><span class="fu"><a href="https://rstudio.github.io/reticulate/reference/py_str.html" class="external-link">py_str</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="va">tokens</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span><span class="op">)</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span>  <span class="co"># Access the embedding of the token, converting it to an R object, </span></span>
<span>  <span class="co"># and print the first 10 elements of the vector.</span></span>
<span>  <span class="va">token_embedding</span> <span class="op">&lt;-</span> <span class="va">sentence</span><span class="op">$</span><span class="va">tokens</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span><span class="op">$</span><span class="va">embedding</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">token_embedding</span>, <span class="fl">10</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="co">#&gt; Token:  Token[0]: "one" </span></span>
<span><span class="co">#&gt; tensor([-0.0535, -0.0368, -0.2851, -0.0381, -0.0486,  0.2383, -0.1200,  0.2620,</span></span>
<span><span class="co">#&gt;         -0.0575,  0.0228])</span></span>
<span><span class="co">#&gt; Token:  Token[1]: "two" </span></span>
<span><span class="co">#&gt; tensor([ 0.0282, -0.0786, -0.1236,  0.1756, -0.1199,  0.0964, -0.1327,  0.4449,</span></span>
<span><span class="co">#&gt;         -0.0264, -0.1168])</span></span>
<span><span class="co">#&gt; Token:  Token[2]: "three" </span></span>
<span><span class="co">#&gt; tensor([-0.0920, -0.0690, -0.1475,  0.2313, -0.0872,  0.0799, -0.0901,  0.4403,</span></span>
<span><span class="co">#&gt;         -0.0103, -0.1494])</span></span>
<span><span class="co">#&gt; Token:  Token[3]: "one" </span></span>
<span><span class="co">#&gt; tensor([-0.0535, -0.0368, -0.2851, -0.0381, -0.0486,  0.2383, -0.1200,  0.2620,</span></span>
<span><span class="co">#&gt;         -0.0575,  0.0228])</span></span></code></pre></div>
</div>
<p> </p>
<hr>
</div>
</div>
<div class="section level2">
<h2 id="training-a-binary-classifier">Training a Binary Classifier<a class="anchor" aria-label="anchor" href="#training-a-binary-classifier"></a>
</h2>
<div style="text-align: justify">
<p>In this section, we’ll train a sentiment analysis model that can
categorize text as either positive or negative. This case study is
adapted from pages 116 to 130 of Tadej Magajna’s book, ‘<a href="https://www.packtpub.com/product/natural-language-processing-with-flair/9781801072311" class="external-link">Natural
Language Processing with Flair</a>’. The process for training text
classifiers in Flair mirrors the sequence followed for sequence labeling
models. Specifically, the steps to train text classifiers are:</p>
<ul>
<li>Load a tagged corpus and compute the label dictionary map.</li>
<li>Prepare the document embeddings.</li>
<li>Initialize the <code>TextClassifier</code> class.</li>
<li>Train the model.</li>
</ul>
</div>
<div class="section level3">
<h3 id="loading-a-tagged-corpus">Loading a Tagged Corpus<a class="anchor" aria-label="anchor" href="#loading-a-tagged-corpus"></a>
</h3>
<div style="text-align: justify">
<p>Training text classification models requires a set of text documents
(typically, sentences or paragraphs) where each document is associated
with one or more classification labels. To train our sentiment analysis
text classification model, we will be using the famous Internet Movie
Database (IMDb) dataset, which contains 50,000 movie reviews from IMDB,
where each review is labeled as either positive or negative. References
to this dataset are already baked into Flair, so loading the dataset
couldn’t be easier:</p>
<div class="sourceCode" id="cb44"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va">flaiR</span><span class="op">)</span></span>
<span><span class="co"># load IMDB from flair_datasets module</span></span>
<span><span class="va">Corpus</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Corpus</span></span>
<span><span class="va">IMDB</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_datasets.html">flair_datasets</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">IMDB</span></span></code></pre></div>
<div class="sourceCode" id="cb45"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># downsize to 0.05</span></span>
<span><span class="va">corpus</span> <span class="op">=</span> <span class="fu">IMDB</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:06,511 Reading data from /Users/yenchiehliao/.flair/datasets/imdb_v4-rebalanced</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:06,511 Train: /Users/yenchiehliao/.flair/datasets/imdb_v4-rebalanced/train.txt</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:06,511 Dev: None</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:06,511 Test: None</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:07,468 No test split found. Using 0% (i.e. 5000 samples) of the train split as test data</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:07,490 No dev split found. Using 0% (i.e. 4500 samples) of the train split as dev data</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:07,490 Initialized corpus /Users/yenchiehliao/.flair/datasets/imdb_v4-rebalanced (label type name is 'sentiment')</span></span>
<span><span class="va">corpus</span><span class="op">$</span><span class="fu">downsample</span><span class="op">(</span><span class="fl">0.05</span><span class="op">)</span></span>
<span><span class="co">#&gt; &lt;flair.datasets.document_classification.IMDB object at 0x377caa380&gt;</span></span></code></pre></div>
<p>Print the sizes in the corpus object as follows - test: %d | train:
%d | dev: %d”</p>
<div class="sourceCode" id="cb46"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">test_size</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">corpus</span><span class="op">$</span><span class="va">test</span><span class="op">)</span></span>
<span><span class="va">train_size</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">corpus</span><span class="op">$</span><span class="va">train</span><span class="op">)</span></span>
<span><span class="va">dev_size</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">corpus</span><span class="op">$</span><span class="va">dev</span><span class="op">)</span></span>
<span><span class="va">output</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sprintf.html" class="external-link">sprintf</a></span><span class="op">(</span><span class="st">"Corpus object sizes - Test: %d | Train: %d | Dev: %d"</span>, <span class="va">test_size</span>, <span class="va">train_size</span>, <span class="va">dev_size</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">output</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "Corpus object sizes - Test: 250 | Train: 2025 | Dev: 225"</span></span></code></pre></div>
<div class="sourceCode" id="cb47"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">lbl_type</span> <span class="op">=</span> <span class="st">'sentiment'</span></span>
<span><span class="va">label_dict</span> <span class="op">=</span> <span class="va">corpus</span><span class="op">$</span><span class="fu">make_label_dictionary</span><span class="op">(</span>label_type<span class="op">=</span><span class="va">lbl_type</span><span class="op">)</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:07,672 Computing label dictionary. Progress:</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:13,632 Dictionary created for label 'sentiment' with 2 values: POSITIVE (seen 1014 times), NEGATIVE (seen 1011 times)</span></span></code></pre></div>
</div>
</div>
<div class="section level3">
<h3 id="loading-the-embeddings">Loading the Embeddings<a class="anchor" aria-label="anchor" href="#loading-the-embeddings"></a>
</h3>
<div style="text-align: justify">
<p>In flair, it covers all the different types of document embeddings
that we can use. Here, we simply use
<code>DocumentPoolEmbeddings</code>. They require no training prior to
training the classification model itself:</p>
<div class="sourceCode" id="cb48"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">DocumentPoolEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">DocumentPoolEmbeddings</span></span>
<span><span class="va">WordEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">WordEmbeddings</span></span>
<span><span class="va">glove</span> <span class="op">=</span> <span class="fu">WordEmbeddings</span><span class="op">(</span><span class="st">'glove'</span><span class="op">)</span></span>
<span><span class="va">document_embeddings</span> <span class="op">=</span> <span class="fu">DocumentPoolEmbeddings</span><span class="op">(</span><span class="va">glove</span><span class="op">)</span></span></code></pre></div>
</div>
</div>
<div class="section level3">
<h3 id="initializing-the-textclassifier-class">Initializing the TextClassifier Class<a class="anchor" aria-label="anchor" href="#initializing-the-textclassifier-class"></a>
</h3>
<div style="text-align: justify">
<div class="sourceCode" id="cb49"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># initiate TextClassifier</span></span>
<span><span class="va">TextClassifier</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_models.html">flair_models</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">TextClassifier</span></span>
<span><span class="va">classifier</span> <span class="op">&lt;-</span> <span class="fu">TextClassifier</span><span class="op">(</span><span class="va">document_embeddings</span>,</span>
<span>                             label_dictionary <span class="op">=</span> <span class="va">label_dict</span>,</span>
<span>                             label_type <span class="op">=</span> <span class="va">lbl_type</span><span class="op">)</span></span></code></pre></div>
<p><code>$to</code> allows you to set the device to use CPU, GPU, or
specific MPS devices on Mac (such as mps:0, mps:1, mps:2).</p>
<div class="sourceCode" id="cb50"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">classifier</span><span class="op">$</span><span class="fu">to</span><span class="op">(</span><span class="fu"><a href="../reference/flair_device.html">flair_device</a></span><span class="op">(</span><span class="st">"mps"</span><span class="op">)</span><span class="op">)</span> </span></code></pre></div>
<pre><code><span><span class="fu">TextClassifier</span><span class="op">(</span></span>
<span>  <span class="op">(</span><span class="va">embeddings</span><span class="op">)</span><span class="op">:</span> <span class="fu">DocumentPoolEmbeddings</span><span class="op">(</span></span>
<span>    fine_tune_mode<span class="op">=</span><span class="va">none</span>, pooling<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span></span>
<span>    <span class="op">(</span><span class="va">embeddings</span><span class="op">)</span><span class="op">:</span> <span class="fu">StackedEmbeddings</span><span class="op">(</span></span>
<span>      <span class="op">(</span><span class="va">list_embedding_0</span><span class="op">)</span><span class="op">:</span> <span class="fu">WordEmbeddings</span><span class="op">(</span></span>
<span>        <span class="st">'glove'</span></span>
<span>        <span class="op">(</span><span class="va">embedding</span><span class="op">)</span><span class="op">:</span> <span class="fu">Embedding</span><span class="op">(</span><span class="fl">400001</span>, <span class="fl">100</span><span class="op">)</span></span>
<span>      <span class="op">)</span></span>
<span>    <span class="op">)</span></span>
<span>  <span class="op">)</span></span>
<span>  <span class="op">(</span><span class="va">decoder</span><span class="op">)</span><span class="op">:</span> <span class="fu">Linear</span><span class="op">(</span>in_features<span class="op">=</span><span class="fl">100</span>, out_features<span class="op">=</span><span class="fl">3</span>, bias<span class="op">=</span><span class="va">True</span><span class="op">)</span></span>
<span>  <span class="op">(</span><span class="va">dropout</span><span class="op">)</span><span class="op">:</span> <span class="fu">Dropout</span><span class="op">(</span>p<span class="op">=</span><span class="fl">0.0</span>, inplace<span class="op">=</span><span class="va">False</span><span class="op">)</span></span>
<span>  <span class="op">(</span><span class="va">locked_dropout</span><span class="op">)</span><span class="op">:</span> <span class="fu">LockedDropout</span><span class="op">(</span>p<span class="op">=</span><span class="fl">0.0</span><span class="op">)</span></span>
<span>  <span class="op">(</span><span class="va">word_dropout</span><span class="op">)</span><span class="op">:</span> <span class="fu">WordDropout</span><span class="op">(</span>p<span class="op">=</span><span class="fl">0.0</span><span class="op">)</span></span>
<span>  <span class="op">(</span><span class="va">loss_function</span><span class="op">)</span><span class="op">:</span> <span class="fu">CrossEntropyLoss</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre>
</div>
</div>
<div class="section level3">
<h3 id="training-the-model">Training the Model<a class="anchor" aria-label="anchor" href="#training-the-model"></a>
</h3>
<div style="text-align: justify">
<p>Training the text classifier model involves two simple steps: -
Defining the model trainer class by passing in the classifier model and
the corpus - Setting off the training process passing in the required
training hyperparameters.</p>
<p><strong>It is worth noting that the ‘L’ in numbers like 32L and 5L is
used in R to denote that the number is an integer. Without the ‘L’
suffix, numbers in R are treated as numeric, which are by default
double-precision floating-point numbers. In contrast, Python determines
the type based on the value of the number itself. Whole numbers (e.g., 5
or 32) are of type int, while numbers with decimal points (e.g., 5.0)
are of type float. Floating-point numbers in both languages are
representations of real numbers but can have some approximation due to
the way they are stored in memory.</strong></p>
<div class="sourceCode" id="cb52"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># initiate ModelTrainer</span></span>
<span><span class="va">ModelTrainer</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_trainers.html">flair_trainers</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">ModelTrainer</span></span>
<span></span>
<span><span class="co"># fit the model</span></span>
<span><span class="va">trainer</span> <span class="op">&lt;-</span> <span class="fu">ModelTrainer</span><span class="op">(</span><span class="va">classifier</span>, <span class="va">corpus</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># start to train</span></span>
<span><span class="co"># note: the 'L' in 32L is used in R to denote that the number is an integer.</span></span>
<span><span class="va">trainer</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="st">'classifier'</span>,</span>
<span>              learning_rate<span class="op">=</span><span class="fl">0.1</span>,</span>
<span>              mini_batch_size<span class="op">=</span><span class="fl">32L</span>,</span>
<span>              <span class="co"># specifies how embeddings are stored in RAM, ie."cpu", "cuda", "gpu", "mps".</span></span>
<span>              <span class="co"># embeddings_storage_mode = "mps",</span></span>
<span>              max_epochs<span class="op">=</span><span class="fl">10L</span><span class="op">)</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:16,265 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:16,266 Model: "TextClassifier(</span></span>
<span><span class="co">#&gt;   (embeddings): DocumentPoolEmbeddings(</span></span>
<span><span class="co">#&gt;     fine_tune_mode=none, pooling=mean</span></span>
<span><span class="co">#&gt;     (embeddings): StackedEmbeddings(</span></span>
<span><span class="co">#&gt;       (list_embedding_0): WordEmbeddings(</span></span>
<span><span class="co">#&gt;         'glove'</span></span>
<span><span class="co">#&gt;         (embedding): Embedding(400001, 100)</span></span>
<span><span class="co">#&gt;       )</span></span>
<span><span class="co">#&gt;     )</span></span>
<span><span class="co">#&gt;   )</span></span>
<span><span class="co">#&gt;   (decoder): Linear(in_features=100, out_features=2, bias=True)</span></span>
<span><span class="co">#&gt;   (dropout): Dropout(p=0.0, inplace=False)</span></span>
<span><span class="co">#&gt;   (locked_dropout): LockedDropout(p=0.0)</span></span>
<span><span class="co">#&gt;   (word_dropout): WordDropout(p=0.0)</span></span>
<span><span class="co">#&gt;   (loss_function): CrossEntropyLoss()</span></span>
<span><span class="co">#&gt;   (weights): None</span></span>
<span><span class="co">#&gt;   (weight_tensor) None</span></span>
<span><span class="co">#&gt; )"</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:16,266 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:16,266 Corpus: 2025 train + 225 dev + 250 test sentences</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:16,266 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:16,266 Train:  2025 sentences</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:16,266         (train_with_dev=False, train_with_test=False)</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:16,266 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:16,266 Training Params:</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:16,267  - learning_rate: "0.1" </span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:16,267  - mini_batch_size: "32"</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:16,267  - max_epochs: "10"</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:16,267  - shuffle: "True"</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:16,267 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:16,267 Plugins:</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:16,267  - AnnealOnPlateau | patience: '3', anneal_factor: '0.5', min_learning_rate: '0.0001'</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:16,267 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:16,267 Final evaluation on model from best epoch (best-model.pt)</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:16,267  - metric: "('micro avg', 'f1-score')"</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:16,267 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:16,267 Computation:</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:16,267  - compute on device: cpu</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:16,268  - embedding storage: cpu</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:16,268 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:16,268 Model training base path: "classifier"</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:16,268 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:16,268 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:17,742 epoch 1 - iter 6/64 - loss 0.88905144 - time (sec): 1.47 - samples/sec: 130.24 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:19,165 epoch 1 - iter 12/64 - loss 0.89519293 - time (sec): 2.90 - samples/sec: 132.56 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:20,687 epoch 1 - iter 18/64 - loss 0.89878009 - time (sec): 4.42 - samples/sec: 130.34 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:22,321 epoch 1 - iter 24/64 - loss 0.89973966 - time (sec): 6.05 - samples/sec: 126.88 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:23,752 epoch 1 - iter 30/64 - loss 0.91613695 - time (sec): 7.48 - samples/sec: 128.28 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:25,714 epoch 1 - iter 36/64 - loss 0.91676073 - time (sec): 9.45 - samples/sec: 121.96 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:27,310 epoch 1 - iter 42/64 - loss 0.90450261 - time (sec): 11.04 - samples/sec: 121.71 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:28,878 epoch 1 - iter 48/64 - loss 0.89435618 - time (sec): 12.61 - samples/sec: 121.81 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:30,350 epoch 1 - iter 54/64 - loss 0.88822153 - time (sec): 14.08 - samples/sec: 122.71 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:31,962 epoch 1 - iter 60/64 - loss 0.89721280 - time (sec): 15.69 - samples/sec: 122.34 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:32,928 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:32,928 EPOCH 1 done: loss 0.8901 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:34,930 DEV : loss 0.834987998008728 - f1-score (micro avg)  0.4533</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:36,013  - 0 epochs without improvement</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:36,014 saving best model</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:36,656 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:37,789 epoch 2 - iter 6/64 - loss 0.90765432 - time (sec): 1.13 - samples/sec: 169.45 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:39,267 epoch 2 - iter 12/64 - loss 0.88809363 - time (sec): 2.61 - samples/sec: 147.09 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:40,947 epoch 2 - iter 18/64 - loss 0.87966466 - time (sec): 4.29 - samples/sec: 134.26 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:42,528 epoch 2 - iter 24/64 - loss 0.88046305 - time (sec): 5.87 - samples/sec: 130.79 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:43,968 epoch 2 - iter 30/64 - loss 0.86499112 - time (sec): 7.31 - samples/sec: 131.30 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:45,427 epoch 2 - iter 36/64 - loss 0.86619079 - time (sec): 8.77 - samples/sec: 131.34 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:46,892 epoch 2 - iter 42/64 - loss 0.87442547 - time (sec): 10.24 - samples/sec: 131.30 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:48,484 epoch 2 - iter 48/64 - loss 0.87625918 - time (sec): 11.83 - samples/sec: 129.86 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:50,073 epoch 2 - iter 54/64 - loss 0.87751761 - time (sec): 13.42 - samples/sec: 128.80 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:51,874 epoch 2 - iter 60/64 - loss 0.87576020 - time (sec): 15.22 - samples/sec: 126.17 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:52,540 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:52,540 EPOCH 2 done: loss 0.8769 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:54,688 DEV : loss 0.8388614058494568 - f1-score (micro avg)  0.4533</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:55,351  - 1 epochs without improvement</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:55,352 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:56,753 epoch 3 - iter 6/64 - loss 1.02308557 - time (sec): 1.40 - samples/sec: 137.07 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:47:58,495 epoch 3 - iter 12/64 - loss 0.92214941 - time (sec): 3.14 - samples/sec: 122.22 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:48:00,020 epoch 3 - iter 18/64 - loss 0.87441330 - time (sec): 4.67 - samples/sec: 123.40 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:48:01,531 epoch 3 - iter 24/64 - loss 0.88802047 - time (sec): 6.18 - samples/sec: 124.31 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:48:03,073 epoch 3 - iter 30/64 - loss 0.87721623 - time (sec): 7.72 - samples/sec: 124.34 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:48:04,549 epoch 3 - iter 36/64 - loss 0.85884527 - time (sec): 9.20 - samples/sec: 125.27 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:48:06,040 epoch 3 - iter 42/64 - loss 0.85893823 - time (sec): 10.69 - samples/sec: 125.76 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:48:07,536 epoch 3 - iter 48/64 - loss 0.85024869 - time (sec): 12.18 - samples/sec: 126.07 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:48:09,019 epoch 3 - iter 54/64 - loss 0.84597292 - time (sec): 13.67 - samples/sec: 126.44 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:48:10,585 epoch 3 - iter 60/64 - loss 0.84663184 - time (sec): 15.23 - samples/sec: 126.05 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:48:11,386 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:48:11,386 EPOCH 3 done: loss 0.8436 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:48:13,198 DEV : loss 0.842860758304596 - f1-score (micro avg)  0.4533</span></span>
<span><span class="co">#&gt; 2024-05-02 01:48:14,227  - 2 epochs without improvement</span></span>
<span><span class="co">#&gt; 2024-05-02 01:48:14,229 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:48:15,413 epoch 4 - iter 6/64 - loss 0.87586416 - time (sec): 1.18 - samples/sec: 162.10 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:48:16,844 epoch 4 - iter 12/64 - loss 0.85521715 - time (sec): 2.62 - samples/sec: 146.84 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:48:18,527 epoch 4 - iter 18/64 - loss 0.86854706 - time (sec): 4.30 - samples/sec: 134.02 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:48:20,428 epoch 4 - iter 24/64 - loss 0.85131466 - time (sec): 6.20 - samples/sec: 123.89 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:48:21,818 epoch 4 - iter 30/64 - loss 0.84007824 - time (sec): 7.59 - samples/sec: 126.50 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:48:23,362 epoch 4 - iter 36/64 - loss 0.83846245 - time (sec): 9.13 - samples/sec: 126.13 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:48:24,874 epoch 4 - iter 42/64 - loss 0.83343096 - time (sec): 10.64 - samples/sec: 126.26 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:48:26,313 epoch 4 - iter 48/64 - loss 0.81893471 - time (sec): 12.08 - samples/sec: 127.11 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:48:27,723 epoch 4 - iter 54/64 - loss 0.82114615 - time (sec): 13.49 - samples/sec: 128.05 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:48:29,139 epoch 4 - iter 60/64 - loss 0.82118241 - time (sec): 14.91 - samples/sec: 128.77 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:48:30,081 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:48:30,081 EPOCH 4 done: loss 0.8206 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:48:32,025 DEV : loss 0.8624953031539917 - f1-score (micro avg)  0.4533</span></span>
<span><span class="co">#&gt; 2024-05-02 01:48:33,063  - 3 epochs without improvement</span></span>
<span><span class="co">#&gt; 2024-05-02 01:48:33,064 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:48:34,183 epoch 5 - iter 6/64 - loss 0.90471396 - time (sec): 1.12 - samples/sec: 171.72 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:48:35,664 epoch 5 - iter 12/64 - loss 0.85042832 - time (sec): 2.60 - samples/sec: 147.75 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:48:37,064 epoch 5 - iter 18/64 - loss 0.83970328 - time (sec): 4.00 - samples/sec: 144.01 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:48:38,549 epoch 5 - iter 24/64 - loss 0.85223064 - time (sec): 5.48 - samples/sec: 140.03 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:48:40,128 epoch 5 - iter 30/64 - loss 0.84393760 - time (sec): 7.06 - samples/sec: 135.91 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:48:41,619 epoch 5 - iter 36/64 - loss 0.83461233 - time (sec): 8.55 - samples/sec: 134.67 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:48:43,198 epoch 5 - iter 42/64 - loss 0.82826268 - time (sec): 10.13 - samples/sec: 132.64 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:48:45,006 epoch 5 - iter 48/64 - loss 0.82183057 - time (sec): 11.94 - samples/sec: 128.63 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:48:46,508 epoch 5 - iter 54/64 - loss 0.82378480 - time (sec): 13.44 - samples/sec: 128.54 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:48:48,022 epoch 5 - iter 60/64 - loss 0.82457873 - time (sec): 14.96 - samples/sec: 128.36 - lr: 0.100000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:48:48,855 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:48:48,855 EPOCH 5 done: loss 0.8291 - lr: 0.100000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:48:50,759 DEV : loss 0.8600366115570068 - f1-score (micro avg)  0.4533</span></span>
<span><span class="co">#&gt; 2024-05-02 01:48:51,745  - 4 epochs without improvement (above 'patience')-&gt; annealing learning_rate to [0.05]</span></span>
<span><span class="co">#&gt; 2024-05-02 01:48:51,746 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:48:53,053 epoch 6 - iter 6/64 - loss 0.65543083 - time (sec): 1.31 - samples/sec: 146.97 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:48:54,567 epoch 6 - iter 12/64 - loss 0.66841915 - time (sec): 2.82 - samples/sec: 136.17 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:48:56,141 epoch 6 - iter 18/64 - loss 0.67260585 - time (sec): 4.39 - samples/sec: 131.08 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:48:57,574 epoch 6 - iter 24/64 - loss 0.69273287 - time (sec): 5.83 - samples/sec: 131.78 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:48:59,137 epoch 6 - iter 30/64 - loss 0.69270405 - time (sec): 7.39 - samples/sec: 129.90 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:49:00,633 epoch 6 - iter 36/64 - loss 0.68977858 - time (sec): 8.89 - samples/sec: 129.64 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:49:02,185 epoch 6 - iter 42/64 - loss 0.68499941 - time (sec): 10.44 - samples/sec: 128.75 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:49:03,675 epoch 6 - iter 48/64 - loss 0.67988836 - time (sec): 11.93 - samples/sec: 128.77 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:49:05,123 epoch 6 - iter 54/64 - loss 0.67491026 - time (sec): 13.38 - samples/sec: 129.18 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:49:06,916 epoch 6 - iter 60/64 - loss 0.67874009 - time (sec): 15.17 - samples/sec: 126.57 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:49:07,501 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:49:07,502 EPOCH 6 done: loss 0.6764 - lr: 0.050000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:49:09,681 DEV : loss 0.6151502728462219 - f1-score (micro avg)  0.68</span></span>
<span><span class="co">#&gt; 2024-05-02 01:49:10,334  - 0 epochs without improvement</span></span>
<span><span class="co">#&gt; 2024-05-02 01:49:10,335 saving best model</span></span>
<span><span class="co">#&gt; 2024-05-02 01:49:10,797 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:49:12,046 epoch 7 - iter 6/64 - loss 0.66998419 - time (sec): 1.25 - samples/sec: 153.77 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:49:13,884 epoch 7 - iter 12/64 - loss 0.65563834 - time (sec): 3.09 - samples/sec: 124.44 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:49:15,502 epoch 7 - iter 18/64 - loss 0.65905811 - time (sec): 4.70 - samples/sec: 122.46 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:49:16,898 epoch 7 - iter 24/64 - loss 0.65355667 - time (sec): 6.10 - samples/sec: 125.91 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:49:18,441 epoch 7 - iter 30/64 - loss 0.65179015 - time (sec): 7.64 - samples/sec: 125.60 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:49:19,873 epoch 7 - iter 36/64 - loss 0.65320787 - time (sec): 9.07 - samples/sec: 126.94 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:49:21,489 epoch 7 - iter 42/64 - loss 0.65325464 - time (sec): 10.69 - samples/sec: 125.71 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:49:22,871 epoch 7 - iter 48/64 - loss 0.65109974 - time (sec): 12.07 - samples/sec: 127.22 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:49:24,378 epoch 7 - iter 54/64 - loss 0.65223570 - time (sec): 13.58 - samples/sec: 127.25 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:49:25,933 epoch 7 - iter 60/64 - loss 0.65245476 - time (sec): 15.14 - samples/sec: 126.86 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:49:26,911 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:49:26,912 EPOCH 7 done: loss 0.6543 - lr: 0.050000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:49:28,774 DEV : loss 0.6376797556877136 - f1-score (micro avg)  0.6089</span></span>
<span><span class="co">#&gt; 2024-05-02 01:49:29,818  - 1 epochs without improvement</span></span>
<span><span class="co">#&gt; 2024-05-02 01:49:29,820 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:49:30,933 epoch 8 - iter 6/64 - loss 0.64463797 - time (sec): 1.11 - samples/sec: 172.55 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:49:32,288 epoch 8 - iter 12/64 - loss 0.65566841 - time (sec): 2.47 - samples/sec: 155.62 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:49:34,409 epoch 8 - iter 18/64 - loss 0.63872668 - time (sec): 4.59 - samples/sec: 125.53 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:49:36,079 epoch 8 - iter 24/64 - loss 0.63630920 - time (sec): 6.26 - samples/sec: 122.71 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:49:37,649 epoch 8 - iter 30/64 - loss 0.64660525 - time (sec): 7.83 - samples/sec: 122.63 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:49:38,951 epoch 8 - iter 36/64 - loss 0.65062172 - time (sec): 9.13 - samples/sec: 126.16 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:49:40,471 epoch 8 - iter 42/64 - loss 0.64101514 - time (sec): 10.65 - samples/sec: 126.19 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:49:42,081 epoch 8 - iter 48/64 - loss 0.65310105 - time (sec): 12.26 - samples/sec: 125.27 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:49:43,651 epoch 8 - iter 54/64 - loss 0.65527810 - time (sec): 13.83 - samples/sec: 124.94 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:49:45,085 epoch 8 - iter 60/64 - loss 0.65732408 - time (sec): 15.27 - samples/sec: 125.78 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:49:46,022 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:49:46,022 EPOCH 8 done: loss 0.6558 - lr: 0.050000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:49:47,962 DEV : loss 0.6833770275115967 - f1-score (micro avg)  0.5556</span></span>
<span><span class="co">#&gt; 2024-05-02 01:49:49,015  - 2 epochs without improvement</span></span>
<span><span class="co">#&gt; 2024-05-02 01:49:49,017 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:49:50,199 epoch 9 - iter 6/64 - loss 0.65765223 - time (sec): 1.18 - samples/sec: 162.38 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:49:51,526 epoch 9 - iter 12/64 - loss 0.62341080 - time (sec): 2.51 - samples/sec: 153.03 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:49:53,036 epoch 9 - iter 18/64 - loss 0.63934545 - time (sec): 4.02 - samples/sec: 143.30 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:49:54,575 epoch 9 - iter 24/64 - loss 0.64706338 - time (sec): 5.56 - samples/sec: 138.18 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:49:56,217 epoch 9 - iter 30/64 - loss 0.64979049 - time (sec): 7.20 - samples/sec: 133.34 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:49:57,715 epoch 9 - iter 36/64 - loss 0.64552120 - time (sec): 8.70 - samples/sec: 132.44 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:49:59,219 epoch 9 - iter 42/64 - loss 0.64046718 - time (sec): 10.20 - samples/sec: 131.74 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:01,054 epoch 9 - iter 48/64 - loss 0.64160677 - time (sec): 12.04 - samples/sec: 127.60 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:02,548 epoch 9 - iter 54/64 - loss 0.64063221 - time (sec): 13.53 - samples/sec: 127.70 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:04,195 epoch 9 - iter 60/64 - loss 0.63914926 - time (sec): 15.18 - samples/sec: 126.50 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:04,910 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:04,910 EPOCH 9 done: loss 0.6377 - lr: 0.050000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:06,860 DEV : loss 0.6505366563796997 - f1-score (micro avg)  0.6089</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:07,839  - 3 epochs without improvement</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:07,841 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:08,750 epoch 10 - iter 6/64 - loss 0.63918026 - time (sec): 0.91 - samples/sec: 211.17 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:10,447 epoch 10 - iter 12/64 - loss 0.61172795 - time (sec): 2.61 - samples/sec: 147.34 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:11,930 epoch 10 - iter 18/64 - loss 0.63438003 - time (sec): 4.09 - samples/sec: 140.86 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:13,449 epoch 10 - iter 24/64 - loss 0.64797637 - time (sec): 5.61 - samples/sec: 136.94 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:15,076 epoch 10 - iter 30/64 - loss 0.65468905 - time (sec): 7.23 - samples/sec: 132.70 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:16,621 epoch 10 - iter 36/64 - loss 0.65349375 - time (sec): 8.78 - samples/sec: 131.21 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:18,100 epoch 10 - iter 42/64 - loss 0.64762172 - time (sec): 10.26 - samples/sec: 131.01 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:19,679 epoch 10 - iter 48/64 - loss 0.64453953 - time (sec): 11.84 - samples/sec: 129.76 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:21,122 epoch 10 - iter 54/64 - loss 0.64228234 - time (sec): 13.28 - samples/sec: 130.11 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:22,706 epoch 10 - iter 60/64 - loss 0.64496682 - time (sec): 14.87 - samples/sec: 129.16 - lr: 0.050000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:23,605 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:23,606 EPOCH 10 done: loss 0.6451 - lr: 0.050000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:25,585 DEV : loss 0.6141707897186279 - f1-score (micro avg)  0.6444</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:26,622  - 4 epochs without improvement (above 'patience')-&gt; annealing learning_rate to [0.025]</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:27,085 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:27,085 Loading model from best epoch ...</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:29,120 </span></span>
<span><span class="co">#&gt; Results:</span></span>
<span><span class="co">#&gt; - F-score (micro) 0.668</span></span>
<span><span class="co">#&gt; - F-score (macro) 0.647</span></span>
<span><span class="co">#&gt; - Accuracy 0.668</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; By class:</span></span>
<span><span class="co">#&gt;               precision    recall  f1-score   support</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     POSITIVE     0.6264    0.8837    0.7331       129</span></span>
<span><span class="co">#&gt;     NEGATIVE     0.7794    0.4380    0.5608       121</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     accuracy                         0.6680       250</span></span>
<span><span class="co">#&gt;    macro avg     0.7029    0.6609    0.6470       250</span></span>
<span><span class="co">#&gt; weighted avg     0.7004    0.6680    0.6497       250</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:29,120 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; $test_score</span></span>
<span><span class="co">#&gt; [1] 0.668</span></span></code></pre></div>
</div>
</div>
<div class="section level3">
<h3 id="loading-and-using-the-classifiers">Loading and Using the Classifiers<a class="anchor" aria-label="anchor" href="#loading-and-using-the-classifiers"></a>
</h3>
<div style="text-align: justify">
<p>After training the text classification model, the resulting
classifier will already be stored in memory as part of the classifier
variable. It is possible, however, that your Python session exited after
training. If so, you’ll need to load the model into memory with the
following:</p>
<div class="sourceCode" id="cb53"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">TextClassifier</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_models.html">flair_models</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">TextClassifier</span></span>
<span><span class="va">classifier</span> <span class="op">&lt;-</span> <span class="va">TextClassifier</span><span class="op">$</span><span class="fu">load</span><span class="op">(</span><span class="st">'classifier/best-model.pt'</span><span class="op">)</span></span></code></pre></div>
<p>We import the Sentence object. Now, we can generate predictions on
some example text inputs.</p>
<div class="sourceCode" id="cb54"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span></code></pre></div>
<div class="sourceCode" id="cb55"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">"great"</span><span class="op">)</span></span>
<span><span class="va">classifier</span><span class="op">$</span><span class="fu">predict</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="va">labels</span><span class="op">)</span></span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; 'Sentence[1]: "great"'/'POSITIVE' (0.9999)</span></span></code></pre></div>
<div class="sourceCode" id="cb56"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="fu">Sentence</span><span class="op">(</span><span class="st">"sad"</span><span class="op">)</span></span>
<span><span class="va">classifier</span><span class="op">$</span><span class="fu">predict</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">$</span><span class="va">labels</span><span class="op">)</span></span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; 'Sentence[1]: "sad"'/'NEGATIVE' (0.8492)</span></span></code></pre></div>
</div>
<p> </p>
<hr>
</div>
</div>
<div class="section level2">
<h2 id="training-rnns">Training RNNs<a class="anchor" aria-label="anchor" href="#training-rnns"></a>
</h2>
<div style="text-align: justify">
<p>Here, we train a sentiment analysis model to categorize text. In this
case, we also include a pipeline that implements the use of Recurrent
Neural Networks (RNN). This makes them particularly effective for tasks
involving sequential data. This section also show you how to implent one
of most powerful feature in featrue, stacked Embeddings. You can stack
multiple embeddings with different layers and let the classifier learn
from different types of features. In Flair NLP, and with the
{<code>flaiR</code>} package, it’s very easy to accomplish this
task.</p>
</div>
<div class="section level3">
<h3 id="import-necessary-modules">Import Necessary Modules<a class="anchor" aria-label="anchor" href="#import-necessary-modules"></a>
</h3>
<div class="sourceCode" id="cb57"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va">flaiR</span><span class="op">)</span></span>
<span><span class="va">WordEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">WordEmbeddings</span></span>
<span><span class="va">FlairEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">FlairEmbeddings</span></span>
<span><span class="va">DocumentRNNEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">DocumentRNNEmbeddings</span></span>
<span><span class="va">TextClassifier</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_models.html">flair_models</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">TextClassifier</span></span>
<span><span class="va">ModelTrainer</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_trainers.html">flair_trainers</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">ModelTrainer</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="get-the-imdb-corpus">Get the IMDB Corpus<a class="anchor" aria-label="anchor" href="#get-the-imdb-corpus"></a>
</h3>
<div style="text-align: justify">
<p>The IMDB movie review dataset is used here, which is a commonly
utilized dataset for sentiment analysis. <code>$downsample(0.1)</code>
method means only 10% of the dataset is used, allowing for a faster
demonstration</p>
<div class="sourceCode" id="cb58"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># load the IMDB file and downsize it to 0.1</span></span>
<span><span class="va">IMDB</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_datasets.html">flair_datasets</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">IMDB</span></span>
<span><span class="va">corpus</span> <span class="op">&lt;-</span> <span class="fu">IMDB</span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="fu">downsample</span><span class="op">(</span><span class="fl">0.1</span><span class="op">)</span> </span>
<span><span class="co">#&gt; 2024-05-02 01:50:29,887 Reading data from /Users/yenchiehliao/.flair/datasets/imdb_v4-rebalanced</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:29,887 Train: /Users/yenchiehliao/.flair/datasets/imdb_v4-rebalanced/train.txt</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:29,887 Dev: None</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:29,887 Test: None</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:30,792 No test split found. Using 0% (i.e. 5000 samples) of the train split as test data</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:30,814 No dev split found. Using 0% (i.e. 4500 samples) of the train split as dev data</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:30,814 Initialized corpus /Users/yenchiehliao/.flair/datasets/imdb_v4-rebalanced (label type name is 'sentiment')</span></span>
<span><span class="co"># create the label dictionary</span></span>
<span><span class="va">lbl_type</span> <span class="op">&lt;-</span> <span class="st">'sentiment'</span></span>
<span><span class="va">label_dict</span> <span class="op">&lt;-</span> <span class="va">corpus</span><span class="op">$</span><span class="fu">make_label_dictionary</span><span class="op">(</span>label_type<span class="op">=</span><span class="va">lbl_type</span><span class="op">)</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:30,840 Computing label dictionary. Progress:</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:43,037 Dictionary created for label 'sentiment' with 2 values: POSITIVE (seen 2056 times), NEGATIVE (seen 1994 times)</span></span></code></pre></div>
</div>
</div>
<div class="section level3">
<h3 id="stacked-embeddings">Stacked Embeddings<a class="anchor" aria-label="anchor" href="#stacked-embeddings"></a>
</h3>
<div style="text-align: justify">
<p>This is one of Flair’s most powerful features: it allows for the
integration of embeddings to enable the model to learn from more sparse
features. Three types of embeddings are utilized here: GloVe embeddings,
and two types of Flair embeddings (forward and backward). Word
embeddings are used to convert words into vectors.</p>
<div class="sourceCode" id="cb59"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># make a list of word embeddings</span></span>
<span><span class="va">word_embeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="fu">WordEmbeddings</span><span class="op">(</span><span class="st">'glove'</span><span class="op">)</span>,</span>
<span>                        <span class="fu">FlairEmbeddings</span><span class="op">(</span><span class="st">'news-forward-fast'</span><span class="op">)</span>,</span>
<span>                        <span class="fu">FlairEmbeddings</span><span class="op">(</span><span class="st">'news-backward-fast'</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># initialize the document embeddings</span></span>
<span><span class="va">document_embeddings</span> <span class="op">&lt;-</span> <span class="fu">DocumentRNNEmbeddings</span><span class="op">(</span><span class="va">word_embeddings</span>, </span>
<span>                                             hidden_size <span class="op">=</span> <span class="fl">512L</span>,</span>
<span>                                             reproject_words <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                                             reproject_words_dimension <span class="op">=</span> <span class="fl">256L</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb60"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># create a Text Classifier with the embeddings and label dictionary</span></span>
<span><span class="va">classifier</span> <span class="op">&lt;-</span> <span class="fu">TextClassifier</span><span class="op">(</span><span class="va">document_embeddings</span>, </span>
<span>                            label_dictionary<span class="op">=</span><span class="va">label_dict</span>, label_type<span class="op">=</span><span class="st">'class'</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># initialize the text classifier trainer with our corpus</span></span>
<span><span class="va">trainer</span> <span class="op">&lt;-</span> <span class="fu">ModelTrainer</span><span class="op">(</span><span class="va">classifier</span>, <span class="va">corpus</span><span class="op">)</span></span></code></pre></div>
</div>
</div>
<div class="section level3">
<h3 id="start-the-training">Start the Training<a class="anchor" aria-label="anchor" href="#start-the-training"></a>
</h3>
<div style="text-align: justify">
<p>For the sake of this example, setting max_epochs to 5. You might want
to increase this for better performance.</p>
<p>It is worth noting that thelearning rate is a parameter that
determines the step size at each iteration while moving towards a
minimum of the loss function. A smaller learning rate could slow down
the learning process, but it could lead to more precise convergence.
<code>mini_batch_size</code> determines the number of samples that will
be used to compute the gradient at each step. The ‘L’ in 32L is used in
R to denote that the number is an integer.</p>
<p><code>patience</code> (aka early stop) is a hyperparameter used in
conjunction with early stopping to avoid overfitting. It determines the
number of epochs the training process will tolerate without improvements
before stopping the training. Setting max_epochs to 5 means the
algorithm will make five passes through the dataset.</p>
<div class="sourceCode" id="cb61"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># note: the 'L' in 32L is used in R to denote that the number is an integer.</span></span>
<span><span class="va">trainer</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="st">'models/sentiment'</span>,</span>
<span>              learning_rate<span class="op">=</span><span class="fl">0.1</span>,</span>
<span>              mini_batch_size<span class="op">=</span><span class="fl">32L</span>,</span>
<span>              patience<span class="op">=</span><span class="fl">5L</span>,</span>
<span>              max_epochs<span class="op">=</span><span class="fl">5L</span><span class="op">)</span>  </span></code></pre></div>
</div>
</div>
<div class="section level3">
<h3 id="to-apply-the-trained-model-for-prediction">To Apply the Trained Model for Prediction<a class="anchor" aria-label="anchor" href="#to-apply-the-trained-model-for-prediction"></a>
</h3>
<div style="text-align: justify">
<div class="sourceCode" id="cb62"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">sentence</span> <span class="op">&lt;-</span> <span class="st">"This movie was really exciting!"</span></span>
<span><span class="va">classifier</span><span class="op">$</span><span class="fu">predict</span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">sentence.labels</span><span class="op">)</span></span></code></pre></div>
</div>
<p> </p>
<hr>
</div>
</div>
<div class="section level2">
<h2 id="finetune-transformers">Finetune Transformers<a class="anchor" aria-label="anchor" href="#finetune-transformers"></a>
</h2>
<div style="text-align: justify">
<p>We use Stefan’s data from <strong><em>Finetune
Transformers</em></strong> as an example. Let’s assume we receive the
data for training from different times. First, suppose you have a
dataset of 1000 entries called <code>cc_muller_old</code>. Another days,
with nice colleage share anothe set of data andyou have another 2000
entries in a dataset called <code>cc_muller_new.</code> Both subsets are
from . We will show how to fine-tune a transformer model with , and then
continue with another round of fine-tuning using .</p>
<div class="sourceCode" id="cb63"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va">flaiR</span><span class="op">)</span></span></code></pre></div>
<div class="section level3">
<h3 id="fine-tuning-a-transformers-model">Fine-tuning a Transformers Model<a class="anchor" aria-label="anchor" href="#fine-tuning-a-transformers-model"></a>
</h3>
<p><u><strong>Step 1</strong></u> Load Necessary Modules from Flair</p>
<p>Load necessary classes from <code>flair</code> package.</p>
<div class="sourceCode" id="cb64"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Sentence is a class for holding a text sentence</span></span>
<span><span class="va">Sentence</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Sentence</span></span>
<span></span>
<span><span class="co"># Corpus is a class for text corpora</span></span>
<span><span class="va">Corpus</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_data.html">flair_data</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">Corpus</span></span>
<span></span>
<span><span class="co"># TransformerDocumentEmbeddings is a class for loading transformer </span></span>
<span><span class="va">TransformerDocumentEmbeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_embeddings.html">flair_embeddings</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">TransformerDocumentEmbeddings</span></span>
<span></span>
<span><span class="co"># TextClassifier is a class for text classification</span></span>
<span><span class="va">TextClassifier</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_models.html">flair_models</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">TextClassifier</span></span>
<span></span>
<span><span class="co"># ModelTrainer is a class for training and evaluating models</span></span>
<span><span class="va">ModelTrainer</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/flair_trainers.html">flair_trainers</a></span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">ModelTrainer</span></span></code></pre></div>
<p>We use purrr to help us split sentences using Sentence from
<code><a href="../reference/flair_data.html">flair_data()</a></code>, then use map2 to add labels, and finally use
<code>Corpus</code> to segment the data.</p>
<div class="sourceCode" id="cb65"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://purrr.tidyverse.org/" class="external-link">purrr</a></span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="va">cc_muller</span><span class="op">)</span></span>
<span><span class="va">cc_muller_old</span> <span class="op">&lt;-</span> <span class="va">cc_muller</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">1000</span>,<span class="op">]</span></span>
<span></span>
<span><span class="va">old_text</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://purrr.tidyverse.org/reference/map.html" class="external-link">map</a></span><span class="op">(</span><span class="va">cc_muller_old</span><span class="op">$</span><span class="va">text</span>, <span class="va">Sentence</span><span class="op">)</span></span>
<span><span class="va">old_labels</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/character.html" class="external-link">as.character</a></span><span class="op">(</span><span class="va">cc_muller_old</span><span class="op">$</span><span class="va">class</span><span class="op">)</span></span>
<span></span>
<span><span class="va">old_text</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://purrr.tidyverse.org/reference/map2.html" class="external-link">map2</a></span><span class="op">(</span><span class="va">old_text</span>, <span class="va">old_labels</span>, <span class="op">~</span> <span class="op">{</span></span>
<span>   </span>
<span>  <span class="va">.x</span><span class="op">$</span><span class="fu">add_label</span><span class="op">(</span><span class="st">"classification"</span>, <span class="va">.y</span><span class="op">)</span></span>
<span>  <span class="va">.x</span></span>
<span><span class="op">}</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb66"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">old_text</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 1000</span></span></code></pre></div>
<div class="sourceCode" id="cb67"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">2046</span><span class="op">)</span></span>
<span><span class="va">sample</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html" class="external-link">sample</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="cn">TRUE</span>, <span class="cn">FALSE</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">old_text</span><span class="op">)</span>, replace<span class="op">=</span><span class="cn">TRUE</span>, prob<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0.8</span>, <span class="fl">0.2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">old_train</span>  <span class="op">&lt;-</span> <span class="va">old_text</span><span class="op">[</span><span class="va">sample</span><span class="op">]</span></span>
<span><span class="va">old_test</span>   <span class="op">&lt;-</span> <span class="va">old_text</span><span class="op">[</span><span class="op">!</span><span class="va">sample</span><span class="op">]</span></span>
<span></span>
<span><span class="va">test_id</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html" class="external-link">sample</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="cn">TRUE</span>, <span class="cn">FALSE</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">old_test</span><span class="op">)</span>, replace<span class="op">=</span><span class="cn">TRUE</span>, prob<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0.5</span>, <span class="fl">0.5</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">old_test</span>   <span class="op">&lt;-</span> <span class="va">old_test</span><span class="op">[</span><span class="va">test_id</span><span class="op">]</span></span>
<span><span class="va">old_dev</span>   <span class="op">&lt;-</span> <span class="va">old_test</span><span class="op">[</span><span class="op">!</span><span class="va">test_id</span><span class="op">]</span></span></code></pre></div>
<p>If you do not provide a development split (dev split) while using
Flair, it will automatically split the training data into training and
development datasets. The test set is used for training the model and
evaluating its final performance, whereas the development set (dev set)
is used for adjusting model parameters and preventing overfitting, or in
other words, for early stopping of the model.</p>
<div class="sourceCode" id="cb68"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">old_corpus</span> <span class="op">&lt;-</span> <span class="fu">Corpus</span><span class="op">(</span>train <span class="op">=</span> <span class="va">old_train</span>, test <span class="op">=</span> <span class="va">old_test</span><span class="op">)</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:46,423 No dev split found. Using 0% (i.e. 80 samples) of the train split as dev data</span></span></code></pre></div>
<p><u><strong>Step 3</strong></u> Load <code>distilbert</code>
Transformer</p>
<div class="sourceCode" id="cb69"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">document_embeddings</span> <span class="op">&lt;-</span> <span class="fu">TransformerDocumentEmbeddings</span><span class="op">(</span><span class="st">'distilbert-base-uncased'</span>, fine_tune<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<p>First, <code>$make_label_dictionary</code> function is used to
automatically create a label dictionary for the classification task. The
label dictionary is a mapping from label to index, which is used to map
the labels to a tensor of label indices. expcept classifcation task,
flair also supports other label types for training custom model, such as
<code>ner</code>, <code>pos</code> and <code>sentiment</code>. From the
cc_muller dataset: Future (seen 423 times), Present (seen 262 times),
Past (seen 131 times)</p>
<div class="sourceCode" id="cb70"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">old_label_dict</span> <span class="op">&lt;-</span> <span class="va">old_corpus</span><span class="op">$</span><span class="fu">make_label_dictionary</span><span class="op">(</span>label_type<span class="op">=</span><span class="st">"classification"</span><span class="op">)</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:48,023 Computing label dictionary. Progress:</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:48,033 Dictionary created for label 'classification' with 3 values: Future (seen 380 times), Present (seen 232 times), Past (seen 111 times)</span></span></code></pre></div>
<p><code>TextClassifier</code> is used to create a text classifier. The
classifier takes the document embeddings (importing from
<code>'distilbert-base-uncased'</code> from HugginFace) and the label
dictionary as input. The label type is also specified as
classification.</p>
<div class="sourceCode" id="cb71"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">old_classifier</span> <span class="op">&lt;-</span> <span class="fu">TextClassifier</span><span class="op">(</span><span class="va">document_embeddings</span>,</span>
<span>                                 label_dictionary <span class="op">=</span> <span class="va">old_label_dict</span>, </span>
<span>                                 label_type<span class="op">=</span><span class="st">'classification'</span><span class="op">)</span></span></code></pre></div>
<p><u><strong>Step 4</strong></u> Start Training</p>
<p><code>ModelTrainer</code> is used to train the model.</p>
<div class="sourceCode" id="cb72"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">old_trainer</span> <span class="op">&lt;-</span> <span class="fu">ModelTrainer</span><span class="op">(</span>model <span class="op">=</span> <span class="va">old_classifier</span>, corpus <span class="op">=</span> <span class="va">old_corpus</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb73"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">old_trainer</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="st">"vignettes/inst/muller-campaign-communication"</span>,  </span>
<span>                  learning_rate<span class="op">=</span><span class="fl">0.02</span>,              </span>
<span>                  mini_batch_size<span class="op">=</span><span class="fl">8L</span>,              </span>
<span>                  anneal_with_restarts <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                  save_final_model<span class="op">=</span><span class="cn">TRUE</span>,</span>
<span>                  max_epochs<span class="op">=</span><span class="fl">1L</span><span class="op">)</span>   </span>
<span><span class="co">#&gt; 2024-05-02 01:50:48,276 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:48,276 Model: "TextClassifier(</span></span>
<span><span class="co">#&gt;   (embeddings): TransformerDocumentEmbeddings(</span></span>
<span><span class="co">#&gt;     (model): DistilBertModel(</span></span>
<span><span class="co">#&gt;       (embeddings): Embeddings(</span></span>
<span><span class="co">#&gt;         (word_embeddings): Embedding(30523, 768)</span></span>
<span><span class="co">#&gt;         (position_embeddings): Embedding(512, 768)</span></span>
<span><span class="co">#&gt;         (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)</span></span>
<span><span class="co">#&gt;         (dropout): Dropout(p=0.1, inplace=False)</span></span>
<span><span class="co">#&gt;       )</span></span>
<span><span class="co">#&gt;       (transformer): Transformer(</span></span>
<span><span class="co">#&gt;         (layer): ModuleList(</span></span>
<span><span class="co">#&gt;           (0-5): 6 x TransformerBlock(</span></span>
<span><span class="co">#&gt;             (attention): MultiHeadSelfAttention(</span></span>
<span><span class="co">#&gt;               (dropout): Dropout(p=0.1, inplace=False)</span></span>
<span><span class="co">#&gt;               (q_lin): Linear(in_features=768, out_features=768, bias=True)</span></span>
<span><span class="co">#&gt;               (k_lin): Linear(in_features=768, out_features=768, bias=True)</span></span>
<span><span class="co">#&gt;               (v_lin): Linear(in_features=768, out_features=768, bias=True)</span></span>
<span><span class="co">#&gt;               (out_lin): Linear(in_features=768, out_features=768, bias=True)</span></span>
<span><span class="co">#&gt;             )</span></span>
<span><span class="co">#&gt;             (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)</span></span>
<span><span class="co">#&gt;             (ffn): FFN(</span></span>
<span><span class="co">#&gt;               (dropout): Dropout(p=0.1, inplace=False)</span></span>
<span><span class="co">#&gt;               (lin1): Linear(in_features=768, out_features=3072, bias=True)</span></span>
<span><span class="co">#&gt;               (lin2): Linear(in_features=3072, out_features=768, bias=True)</span></span>
<span><span class="co">#&gt;               (activation): GELUActivation()</span></span>
<span><span class="co">#&gt;             )</span></span>
<span><span class="co">#&gt;             (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)</span></span>
<span><span class="co">#&gt;           )</span></span>
<span><span class="co">#&gt;         )</span></span>
<span><span class="co">#&gt;       )</span></span>
<span><span class="co">#&gt;     )</span></span>
<span><span class="co">#&gt;   )</span></span>
<span><span class="co">#&gt;   (decoder): Linear(in_features=768, out_features=3, bias=True)</span></span>
<span><span class="co">#&gt;   (dropout): Dropout(p=0.0, inplace=False)</span></span>
<span><span class="co">#&gt;   (locked_dropout): LockedDropout(p=0.0)</span></span>
<span><span class="co">#&gt;   (word_dropout): WordDropout(p=0.0)</span></span>
<span><span class="co">#&gt;   (loss_function): CrossEntropyLoss()</span></span>
<span><span class="co">#&gt;   (weights): None</span></span>
<span><span class="co">#&gt;   (weight_tensor) None</span></span>
<span><span class="co">#&gt; )"</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:48,276 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:48,277 Corpus: 723 train + 80 dev + 85 test sentences</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:48,277 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:48,277 Train:  723 sentences</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:48,277         (train_with_dev=False, train_with_test=False)</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:48,277 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:48,277 Training Params:</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:48,277  - learning_rate: "0.02" </span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:48,277  - mini_batch_size: "8"</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:48,277  - max_epochs: "1"</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:48,277  - shuffle: "True"</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:48,277 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:48,277 Plugins:</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:48,277  - AnnealOnPlateau | patience: '3', anneal_factor: '0.5', min_learning_rate: '0.0001'</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:48,277 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:48,277 Final evaluation on model from best epoch (best-model.pt)</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:48,277  - metric: "('micro avg', 'f1-score')"</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:48,278 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:48,278 Computation:</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:48,278  - compute on device: cpu</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:48,278  - embedding storage: cpu</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:48,278 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:48,278 Model training base path: "vignettes/inst/muller-campaign-communication"</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:48,278 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:48,278 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:52,195 epoch 1 - iter 9/91 - loss 1.15524849 - time (sec): 3.92 - samples/sec: 18.38 - lr: 0.020000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:50:56,077 epoch 1 - iter 18/91 - loss 1.08303712 - time (sec): 7.80 - samples/sec: 18.47 - lr: 0.020000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:51:00,032 epoch 1 - iter 27/91 - loss 0.97742616 - time (sec): 11.75 - samples/sec: 18.38 - lr: 0.020000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:51:04,102 epoch 1 - iter 36/91 - loss 0.89108824 - time (sec): 15.82 - samples/sec: 18.20 - lr: 0.020000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:51:08,186 epoch 1 - iter 45/91 - loss 0.82130664 - time (sec): 19.91 - samples/sec: 18.08 - lr: 0.020000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:51:11,751 epoch 1 - iter 54/91 - loss 0.79142332 - time (sec): 23.47 - samples/sec: 18.40 - lr: 0.020000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:51:15,968 epoch 1 - iter 63/91 - loss 0.76143430 - time (sec): 27.69 - samples/sec: 18.20 - lr: 0.020000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:51:19,678 epoch 1 - iter 72/91 - loss 0.74108296 - time (sec): 31.40 - samples/sec: 18.34 - lr: 0.020000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:51:23,538 epoch 1 - iter 81/91 - loss 0.70741312 - time (sec): 35.26 - samples/sec: 18.38 - lr: 0.020000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:51:27,670 epoch 1 - iter 90/91 - loss 0.70479750 - time (sec): 39.39 - samples/sec: 18.28 - lr: 0.020000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:51:27,903 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:51:27,903 EPOCH 1 done: loss 0.7061 - lr: 0.020000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:51:29,388 DEV : loss 0.4103182256221771 - f1-score (micro avg)  0.825</span></span>
<span><span class="co">#&gt; 2024-05-02 01:51:29,391  - 0 epochs without improvement</span></span>
<span><span class="co">#&gt; 2024-05-02 01:51:29,392 saving best model</span></span>
<span><span class="co">#&gt; 2024-05-02 01:51:30,448 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:51:30,448 Loading model from best epoch ...</span></span>
<span><span class="co">#&gt; 2024-05-02 01:51:33,620 </span></span>
<span><span class="co">#&gt; Results:</span></span>
<span><span class="co">#&gt; - F-score (micro) 0.8353</span></span>
<span><span class="co">#&gt; - F-score (macro) 0.8492</span></span>
<span><span class="co">#&gt; - Accuracy 0.8353</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; By class:</span></span>
<span><span class="co">#&gt;               precision    recall  f1-score   support</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;       Future     0.8571    0.8372    0.8471        43</span></span>
<span><span class="co">#&gt;      Present     0.7333    0.8148    0.7719        27</span></span>
<span><span class="co">#&gt;         Past     1.0000    0.8667    0.9286        15</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     accuracy                         0.8353        85</span></span>
<span><span class="co">#&gt;    macro avg     0.8635    0.8396    0.8492        85</span></span>
<span><span class="co">#&gt; weighted avg     0.8430    0.8353    0.8376        85</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; 2024-05-02 01:51:33,620 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; $test_score</span></span>
<span><span class="co">#&gt; [1] 0.8352941</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="continue-fine-tuning-with-new-dataset">Continue Fine-tuning with New Dataset<a class="anchor" aria-label="anchor" href="#continue-fine-tuning-with-new-dataset"></a>
</h3>
<p>Now, we can continue to fine-tune the already fine-tuned model with
an additional 2000 pieces of data. First, let’s say we have another 2000
entries called cc_muller_new. We can fine-tune the previous model with
these 2000 entries. The steps are the same as before. For this case, we
don’t need to split the dataset again. We can use the entire 2000
entries as the training set and use the old_test set to evaluate how
well our refined model performs.</p>
<div class="sourceCode" id="cb74"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://purrr.tidyverse.org/" class="external-link">purrr</a></span><span class="op">)</span></span>
<span><span class="va">cc_muller_new</span> <span class="op">&lt;-</span> <span class="va">cc_muller</span><span class="op">[</span><span class="fl">1001</span><span class="op">:</span><span class="fl">3000</span>,<span class="op">]</span></span>
<span><span class="va">new_text</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://purrr.tidyverse.org/reference/map.html" class="external-link">map</a></span><span class="op">(</span><span class="va">cc_muller_new</span><span class="op">$</span><span class="va">text</span>, <span class="va">Sentence</span><span class="op">)</span></span>
<span><span class="va">new_labels</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/character.html" class="external-link">as.character</a></span><span class="op">(</span><span class="va">cc_muller_new</span><span class="op">$</span><span class="va">class</span><span class="op">)</span></span>
<span></span>
<span><span class="va">new_text</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://purrr.tidyverse.org/reference/map2.html" class="external-link">map2</a></span><span class="op">(</span><span class="va">new_text</span>, <span class="va">new_labels</span>, <span class="op">~</span> <span class="op">{</span></span>
<span>  <span class="va">.x</span><span class="op">$</span><span class="fu">add_label</span><span class="op">(</span><span class="st">"classification"</span>, <span class="va">.y</span><span class="op">)</span></span>
<span>  <span class="va">.x</span></span>
<span><span class="op">}</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb75"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">new_corpus</span> <span class="op">&lt;-</span> <span class="fu">Corpus</span><span class="op">(</span>train<span class="op">=</span><span class="va">new_text</span>, test<span class="op">=</span><span class="va">old_test</span><span class="op">)</span></span>
<span><span class="co">#&gt; 2024-05-02 01:51:34,363 No dev split found. Using 0% (i.e. 200 samples) of the train split as dev data</span></span></code></pre></div>
<p>Load the model (<code>old_model</code>) we have already finetuned
from previous stage and let’s fine-tune it with the new data,
<code>new_corpus</code>.</p>
<div class="sourceCode" id="cb76"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">old_model</span> <span class="op">&lt;-</span> <span class="va">TextClassifier</span><span class="op">$</span><span class="fu">load</span><span class="op">(</span><span class="st">"vignettes/inst/muller-campaign-communication/best-model.pt"</span><span class="op">)</span></span>
<span><span class="va">new_trainer</span> <span class="op">&lt;-</span> <span class="fu">ModelTrainer</span><span class="op">(</span><span class="va">old_model</span>, <span class="va">new_corpus</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb77"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">new_trainer</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="st">"vignettes/inst/new-muller-campaign-communication"</span>,</span>
<span>                  learning_rate<span class="op">=</span><span class="fl">0.002</span>, </span>
<span>                  mini_batch_size<span class="op">=</span><span class="fl">8L</span>,  </span>
<span>                  max_epochs<span class="op">=</span><span class="fl">1L</span><span class="op">)</span>    </span>
<span><span class="co">#&gt; 2024-05-02 01:51:36,303 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:51:36,303 Model: "TextClassifier(</span></span>
<span><span class="co">#&gt;   (embeddings): TransformerDocumentEmbeddings(</span></span>
<span><span class="co">#&gt;     (model): DistilBertModel(</span></span>
<span><span class="co">#&gt;       (embeddings): Embeddings(</span></span>
<span><span class="co">#&gt;         (word_embeddings): Embedding(30523, 768, padding_idx=0)</span></span>
<span><span class="co">#&gt;         (position_embeddings): Embedding(512, 768)</span></span>
<span><span class="co">#&gt;         (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)</span></span>
<span><span class="co">#&gt;         (dropout): Dropout(p=0.1, inplace=False)</span></span>
<span><span class="co">#&gt;       )</span></span>
<span><span class="co">#&gt;       (transformer): Transformer(</span></span>
<span><span class="co">#&gt;         (layer): ModuleList(</span></span>
<span><span class="co">#&gt;           (0-5): 6 x TransformerBlock(</span></span>
<span><span class="co">#&gt;             (attention): MultiHeadSelfAttention(</span></span>
<span><span class="co">#&gt;               (dropout): Dropout(p=0.1, inplace=False)</span></span>
<span><span class="co">#&gt;               (q_lin): Linear(in_features=768, out_features=768, bias=True)</span></span>
<span><span class="co">#&gt;               (k_lin): Linear(in_features=768, out_features=768, bias=True)</span></span>
<span><span class="co">#&gt;               (v_lin): Linear(in_features=768, out_features=768, bias=True)</span></span>
<span><span class="co">#&gt;               (out_lin): Linear(in_features=768, out_features=768, bias=True)</span></span>
<span><span class="co">#&gt;             )</span></span>
<span><span class="co">#&gt;             (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)</span></span>
<span><span class="co">#&gt;             (ffn): FFN(</span></span>
<span><span class="co">#&gt;               (dropout): Dropout(p=0.1, inplace=False)</span></span>
<span><span class="co">#&gt;               (lin1): Linear(in_features=768, out_features=3072, bias=True)</span></span>
<span><span class="co">#&gt;               (lin2): Linear(in_features=3072, out_features=768, bias=True)</span></span>
<span><span class="co">#&gt;               (activation): GELUActivation()</span></span>
<span><span class="co">#&gt;             )</span></span>
<span><span class="co">#&gt;             (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)</span></span>
<span><span class="co">#&gt;           )</span></span>
<span><span class="co">#&gt;         )</span></span>
<span><span class="co">#&gt;       )</span></span>
<span><span class="co">#&gt;     )</span></span>
<span><span class="co">#&gt;   )</span></span>
<span><span class="co">#&gt;   (decoder): Linear(in_features=768, out_features=3, bias=True)</span></span>
<span><span class="co">#&gt;   (dropout): Dropout(p=0.0, inplace=False)</span></span>
<span><span class="co">#&gt;   (locked_dropout): LockedDropout(p=0.0)</span></span>
<span><span class="co">#&gt;   (word_dropout): WordDropout(p=0.0)</span></span>
<span><span class="co">#&gt;   (loss_function): CrossEntropyLoss()</span></span>
<span><span class="co">#&gt;   (weights): None</span></span>
<span><span class="co">#&gt;   (weight_tensor) None</span></span>
<span><span class="co">#&gt; )"</span></span>
<span><span class="co">#&gt; 2024-05-02 01:51:36,304 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:51:36,304 Corpus: 1800 train + 200 dev + 85 test sentences</span></span>
<span><span class="co">#&gt; 2024-05-02 01:51:36,304 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:51:36,304 Train:  1800 sentences</span></span>
<span><span class="co">#&gt; 2024-05-02 01:51:36,304         (train_with_dev=False, train_with_test=False)</span></span>
<span><span class="co">#&gt; 2024-05-02 01:51:36,304 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:51:36,304 Training Params:</span></span>
<span><span class="co">#&gt; 2024-05-02 01:51:36,304  - learning_rate: "0.002" </span></span>
<span><span class="co">#&gt; 2024-05-02 01:51:36,304  - mini_batch_size: "8"</span></span>
<span><span class="co">#&gt; 2024-05-02 01:51:36,304  - max_epochs: "1"</span></span>
<span><span class="co">#&gt; 2024-05-02 01:51:36,304  - shuffle: "True"</span></span>
<span><span class="co">#&gt; 2024-05-02 01:51:36,304 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:51:36,304 Plugins:</span></span>
<span><span class="co">#&gt; 2024-05-02 01:51:36,304  - AnnealOnPlateau | patience: '3', anneal_factor: '0.5', min_learning_rate: '0.0001'</span></span>
<span><span class="co">#&gt; 2024-05-02 01:51:36,304 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:51:36,304 Final evaluation on model from best epoch (best-model.pt)</span></span>
<span><span class="co">#&gt; 2024-05-02 01:51:36,304  - metric: "('micro avg', 'f1-score')"</span></span>
<span><span class="co">#&gt; 2024-05-02 01:51:36,304 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:51:36,305 Computation:</span></span>
<span><span class="co">#&gt; 2024-05-02 01:51:36,305  - compute on device: cpu</span></span>
<span><span class="co">#&gt; 2024-05-02 01:51:36,305  - embedding storage: cpu</span></span>
<span><span class="co">#&gt; 2024-05-02 01:51:36,305 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:51:36,305 Model training base path: "vignettes/inst/new-muller-campaign-communication"</span></span>
<span><span class="co">#&gt; 2024-05-02 01:51:36,305 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:51:36,305 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:51:45,842 epoch 1 - iter 22/225 - loss 0.47900023 - time (sec): 9.54 - samples/sec: 18.46 - lr: 0.002000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:51:56,716 epoch 1 - iter 44/225 - loss 0.41211617 - time (sec): 20.41 - samples/sec: 17.25 - lr: 0.002000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:52:06,466 epoch 1 - iter 66/225 - loss 0.40668128 - time (sec): 30.16 - samples/sec: 17.51 - lr: 0.002000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:52:16,274 epoch 1 - iter 88/225 - loss 0.41441286 - time (sec): 39.97 - samples/sec: 17.61 - lr: 0.002000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:52:27,433 epoch 1 - iter 110/225 - loss 0.41513873 - time (sec): 51.13 - samples/sec: 17.21 - lr: 0.002000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:52:37,728 epoch 1 - iter 132/225 - loss 0.40815942 - time (sec): 61.42 - samples/sec: 17.19 - lr: 0.002000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:52:49,121 epoch 1 - iter 154/225 - loss 0.39591000 - time (sec): 72.82 - samples/sec: 16.92 - lr: 0.002000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:52:58,490 epoch 1 - iter 176/225 - loss 0.38776927 - time (sec): 82.19 - samples/sec: 17.13 - lr: 0.002000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:53:09,089 epoch 1 - iter 198/225 - loss 0.38842803 - time (sec): 92.78 - samples/sec: 17.07 - lr: 0.002000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:53:19,214 epoch 1 - iter 220/225 - loss 0.38578013 - time (sec): 102.91 - samples/sec: 17.10 - lr: 0.002000 - momentum: 0.000000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:53:21,092 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:53:21,092 EPOCH 1 done: loss 0.3907 - lr: 0.002000</span></span>
<span><span class="co">#&gt; 2024-05-02 01:53:25,191 DEV : loss 0.41863852739334106 - f1-score (micro avg)  0.84</span></span>
<span><span class="co">#&gt; 2024-05-02 01:53:25,198  - 0 epochs without improvement</span></span>
<span><span class="co">#&gt; 2024-05-02 01:53:25,199 saving best model</span></span>
<span><span class="co">#&gt; 2024-05-02 01:53:26,261 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 2024-05-02 01:53:26,261 Loading model from best epoch ...</span></span>
<span><span class="co">#&gt; 2024-05-02 01:53:29,424 </span></span>
<span><span class="co">#&gt; Results:</span></span>
<span><span class="co">#&gt; - F-score (micro) 0.8471</span></span>
<span><span class="co">#&gt; - F-score (macro) 0.8509</span></span>
<span><span class="co">#&gt; - Accuracy 0.8471</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; By class:</span></span>
<span><span class="co">#&gt;               precision    recall  f1-score   support</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;       Future     0.8810    0.8605    0.8706        43</span></span>
<span><span class="co">#&gt;      Present     0.7419    0.8519    0.7931        27</span></span>
<span><span class="co">#&gt;         Past     1.0000    0.8000    0.8889        15</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     accuracy                         0.8471        85</span></span>
<span><span class="co">#&gt;    macro avg     0.8743    0.8374    0.8509        85</span></span>
<span><span class="co">#&gt; weighted avg     0.8578    0.8471    0.8492        85</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; 2024-05-02 01:53:29,424 ----------------------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; $test_score</span></span>
<span><span class="co">#&gt; [1] 0.8470588</span></span></code></pre></div>
<p>More R tutorial and documentation see <a href="https://github.com/davidycliao/flaiR" class="external-link">here</a>.</p>
</div>
</div>
<!-- # Finetune Models with HuggingFace via flaiR -->
<!-- <div style="text-align: justify"> -->
<!-- The following example offers a straightforward introduction on how to fully train your own model using the Flair framework and import a `BERT` model from [HuggingFace 🤗](https://github.com/huggingface). This example utilizes grandstanding score as training data from Julia Park's paper (_[When Do Politicians Grandstand? Measuring Message Politics in Committee Hearings](https://www.journals.uchicago.edu/doi/abs/10.1086/709147?journalCode=jop&mobileUi=0)_) and trains the model using Transformer-based models via flair NLP through `{flaiR}`. -->
<!-- <u>__Step 1__</u> Load Necessary Modules from Flair -->
<!-- ```{r} -->
<!-- # load training data: grandstanding score from Julia Park's paper -->
<!-- library(flaiR) -->
<!-- data(gs_score) -->
<!-- ``` -->
<!-- Load necessary classes from `flair` package. -->
<!-- ```{r} -->
<!-- # Sentence is a class for holding a text sentence -->
<!-- Sentence <- flair_data()$Sentence -->
<!-- # Corpus is a class for text corpora -->
<!-- Corpus <- flair_data()$Corpus -->
<!-- # TransformerDocumentEmbeddings is a class for loading transformer -->
<!-- TransformerDocumentEmbeddings <- flair_embeddings()$TransformerDocumentEmbeddings -->
<!-- # TextClassifier is a class for text classification -->
<!-- TextClassifier <- flair_models()$TextClassifier -->
<!-- # ModelTrainer is a class for training and evaluating models -->
<!-- ModelTrainer <- flair_trainers()$ModelTrainer -->
<!-- ``` -->
<!-- <u>__Step 2__</u> Split and Preprocess Data with Corpus Object -->
<!-- Split data into train and test sets using basic R functions. -->
<!-- ```{r} -->
<!-- library(purrr) -->
<!-- # split the data -->
<!-- text <- map(gs_score$speech, Sentence) -->
<!-- labels <- as.character(gs_score$rescaled_gs) -->
<!-- text <- map2(text, labels, ~ { -->
<!--   .x$add_label("classification", .y) -->
<!--   .x -->
<!-- }) -->
<!-- ``` -->
<!-- ```{r} -->
<!-- print(head(text, n =2)) -->
<!-- ``` -->
<!-- ```{r} -->
<!-- set.seed(2046) -->
<!-- sample <- sample(c(TRUE, FALSE), length(text), replace=TRUE, prob=c(0.8, 0.2)) -->
<!-- train  <- text[sample] -->
<!-- test   <- text[!sample] -->
<!-- ``` -->
<!-- If you do not provide a development split (dev split) while using Flair, it will automatically split the training data into training and development datasets. The test set is used for training the model and evaluating its final performance, whereas the development set (dev set) is used for adjusting model parameters and preventing overfitting, or in other words, for early stopping of the model. -->
<!-- ```{r} -->
<!-- corpus <- Corpus(train=train, test=test) -->
<!-- ``` -->
<!-- Alternatively, you can also create dev sets splitting test set. The following code splits the data into train, test, and dev sets with a ratio of 8:1:1. -->
<!-- ```{r} -->
<!-- set.seed(2046) -->
<!-- sample <- sample(c(TRUE, FALSE), length(text), replace=TRUE, prob=c(0.8, 0.2)) -->
<!-- train  <- text[sample] -->
<!-- test   <- text[!sample] -->
<!-- test_id <- sample(c(TRUE, FALSE), length(test), replace=TRUE, prob=c(0.5, 0.5)) -->
<!-- test   <- test[sample] -->
<!-- dev   <- test[!sample] -->
<!-- ``` -->
<!-- ```{r} -->
<!-- corpus <- Corpus(train=train, test=test, dev=dev) -->
<!-- ``` -->
<!-- <u>__Step 3__</u> Load Transformer Embeddings -->
<!-- ```{r} -->
<!-- document_embeddings <- TransformerDocumentEmbeddings('distilbert-base-uncased', fine_tune=TRUE) -->
<!-- ``` -->
<!-- First, `$make_label_dictionary` function is used to automatically create a label dictionary for the classification task. The label dictionary is a mapping from label to index, which is used to map the labels to a tensor of label indices. expcept classifcation task, flair also supports other label types for training custom model, such as `ner`, `pos` and `sentiment`. -->
<!-- ```{r} -->
<!-- label_dict <- corpus$make_label_dictionary(label_type="classification") -->
<!-- ``` -->
<!-- Besides, you can also create a label dictionary manually. The following code creates a label dictionary with two labels, `0` and `1`, and maps them to the indices `0` and `1` respectively. -->
<!-- ```{r} -->
<!-- # load Dictionary object from flair_data -->
<!-- Dictionary <- flair_data()$Dictionary -->
<!-- # manually create label_dict with two labels, 0 and 1 -->
<!-- label_dict <- Dictionary(add_unk=FALSE) -->
<!-- # you can specify the order of labels. Please note the label should be a list and character (string) type. -->
<!-- specific_order_labels <- list('0', '1') -->
<!-- for (label in seq_along(specific_order_labels)) { -->
<!--   label_dict$add_item(as.character(specific_order_labels [[label]])) -->
<!-- } -->
<!-- ``` -->
<!-- Then, we can use the `$item2idx` method to check the mapping from label to index. This is very important to make sure the labels are mapped correctly to the indices and tensors. -->
<!-- ```{r} -->
<!-- print(label_dict$idx2item) -->
<!-- ``` -->
<!-- ```{r} -->
<!-- print(label_dict$item2idx) -->
<!-- ``` -->
<!-- `TextClassifier` is used to create a text classifier. The classifier takes the document embeddings (importing from `'distilbert-base-uncased'` from HugginFace)  and the label dictionary as input. The label type is also specified as classification. -->
<!-- ```{r} -->
<!-- classifier <- TextClassifier(document_embeddings, -->
<!--                              label_dictionary=label_dict, -->
<!--                              label_type='classification') -->
<!-- ``` -->
<!-- <u>__Step 4__</u> Start Training -->
<!-- specific computation devices on your local machine. If you have a GPU, you can use `flair_gpu` to specify the GPU device. If you don't have a GPU, you can use `flaiR::flair_device` to specify the CPU device. -->
<!-- ```{r} -->
<!-- classifier$to(flair_device("cpu")) -->
<!-- ``` -->
<!-- `ModelTrainer` is used to train the model, which learns from the data based on the grandstanding score. -->
<!-- ```{r eval=TRUE} -->
<!-- trainer <- ModelTrainer(classifier, corpus) -->
<!-- ``` -->
<!-- ```{r eval=TRUE} -->
<!-- trainer$train('grand_standing_model',          # output directory -->
<!--               learning_rate=0.02,              # learning rate: if batch_growth_annealing activates,lr should starts a bit higher. -->
<!--               mini_batch_size=8L,              # batch size -->
<!--               anneal_with_restarts = TRUE, -->
<!--               save_final_model=TRUE, -->
<!--               max_epochs=10L)                  # Maximum number of epochs -->
<!-- ``` -->
<!-- <u>__Step 5__</u> Evaluate the Model -->
<!-- During and after the model training process, evaluating the performance of the trained model on the development set is straightforward and easy. -->
<!-- ```{r} -->
<!-- # import the performance metrics generated during the training process -->
<!-- performance_df <- read.table(file = "grand_standing/loss.tsv", header = TRUE, sep = "\t") -->
<!-- head(performance_df) -->
<!-- ``` -->
<!-- ```{r} -->
<!-- library(ggplot2) -->
<!-- ggplot(performance_df, aes(x = EPOCH)) + -->
<!--   geom_line(aes(y = TRAIN_LOSS, color = "Training Loss")) + -->
<!--   geom_line(aes(y = DEV_LOSS, color = "Development Loss")) + -->
<!--   geom_line(aes(y = DEV_RECALL, color = "Development Recall")) + -->
<!--   geom_line(aes(y = DEV_F1, color = "Development F1")) + -->
<!--   labs(title = "Training and Development Loss per Epoch", -->
<!--        x = "Epochs / Grandstanding Classifier", -->
<!--        y = "") + -->
<!--   scale_color_manual("", -->
<!--                      values = c("Training Loss" = "blue", -->
<!--                                 "Development Loss" = "red", -->
<!--                                 "Development F1" = "green"))+ -->
<!--   theme_minimal() -->
<!-- ``` -->
<!-- The overall performance of the model on the test set is also straightforward and easy to evaluate. You can find the performance metrics in the `model/training.log` file. -->
<!-- ``` -->
<!-- Results: -->
<!-- - F-score (micro) 0.7443 -->
<!-- - F-score (macro) 0.7438 -->
<!-- - Accuracy 0.7443 -->
<!-- By class: -->
<!--               precision    recall  f1-score   support -->
<!--            1     0.6781    0.8519    0.7551       324 -->
<!--            0     0.8362    0.6516    0.7324       376 -->
<!--     accuracy                         0.7443       700 -->
<!--    macro avg     0.7572    0.7517    0.7438       700 -->
<!-- weighted avg     0.7630    0.7443    0.7429       700 -->
<!-- ``` -->
<!-- <u>__Step 6__</u> Apply the Trained Model on Unseen Data for Prediction -->
<!-- We use the statement in the dataset as an example. -->
<!-- ```{r} -->
<!-- # load the trained model -->
<!-- data(statements) -->
<!-- Sentence <- flair_data()$Sentence -->
<!-- text <- statements[1, "Statement"] -->
<!-- sentence <- Sentence(text) -->
<!-- ``` -->
<!-- `lassifier$predict function is used to predict the label of the sentence. The function returns a sentence object with the predicted label.` -->
<!-- ```{r} -->
<!-- classifier$predict(sentence) -->
<!-- print(sentence) -->
<!-- ``` -->
<!-- `sentence$labels` is a list of labels, each of which has a value and a score. The value is the label itself, and the score is the probability of the label. The label with the highest score is the predicted label. -->
<!-- ```{r} -->
<!-- sentence$labels[[1]]$value -->
<!-- ``` -->
<!-- ```{r} -->
<!-- sentence$labels[[1]]$score -->
<!-- ``` -->
<!-- <u>__Step 7__</u> Reload the Model with the Best Performance -->
<!-- When you train the model with `save_final_model=TRUE`, the model with the best performance on the development set will be saved in the output directory. You can reload the model with the best performance using the `load` function. -->
<!-- ```{r} -->
<!-- Sentence <- flair_data()$Sentence -->
<!-- TextClassifier <- flair_models()$TextClassifier -->
<!-- classifier <- TextClassifier$load('grand_standing/best-model.pt') -->
<!-- ``` -->
<!-- We can create a function to classify the text using the specified Flair classifier. -->
<!-- ```{r} -->
<!-- classify_text <- function(text, classifier) { -->
<!--   # Classifies the given text using the specified Flair classifier. -->
<!--   # -->
<!--   # Args: -->
<!--   # text (str): The text to be classified. -->
<!--   # classifier (TextClassifier): The Flair classifier to use for prediction. -->
<!--   # -->
<!--   # Returns: -->
<!--   #   list: A list containing the predicted class label and score as strings. -->
<!--   sentence <- Sentence(text) -->
<!--   classifier$predict(sentence) -->
<!--   return(list (labels  = sentence$labels[[1]]$value, score  = as.character(sentence$labels[[1]]$score))) -->
<!--   } -->
<!-- ``` -->
<!-- Before performing classification task, let's quickly check the exmaple dataset. -->
<!-- ```{r} -->
<!-- data(statements) -->
<!-- print(statements) -->
<!-- ``` -->
<!-- Let's apply the function to the dataset. -->
<!-- ```{r} -->
<!-- for (i in seq_along(statements$Statement) ) { -->
<!--   out_come <- classify_text(statements$Statement[[i]], classifier) -->
<!--   statements[i, 'predicted_labels'] <- out_come[[1]] -->
<!--   statements[i, 'prop_score'] <- out_come[[2]] -->
<!-- } -->
<!-- ``` -->
<!-- ```{r} -->
<!-- statements[c("Type", "predicted_labels", "prop_score")] -->
<!-- ``` -->
</div>
</main>
</div>

  <aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



   
  <footer><div class="container">
  <div class="pkgdown-footer-left">
  <p>Developed by <a href="https://davidycliao.github.io" class="external-link">David Liao</a>, Paula Montano, <a href="https://samiradiebire.github.io" class="external-link">Samira Diebire</a>, Akbik Alan, Blythe Duncan, Vollgraf Roland.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.9.</p>
</div>

  </div></footer>
</body>
</html>
